File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\cli.py
"""
Command-line interface for the README generator.

This module provides the main CLI functionality for automatically generating
comprehensive README files for Python projects using LLM APIs,
optimized for Gemini 2.5 Pro with a 1M token window.
"""

import os
import sys
import argparse
import time
import json
import asyncio
from pathlib import Path
import logging

# Optional: Third-party LLM APIs
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    import openai
except ImportError:
    openai = None

try:
    import anthropic
except ImportError:
    anthropic = None

from config import Config, load_config, save_config
from parser.project_parser import parse_project
from utils.token_counter import estimate_tokens
from utils.file_utils import create_directory
from utils.json_serializer import serialize_project_data, save_json_to_file

logger = logging.getLogger(__name__)

class CLIError(Exception):
    pass

class LLMAPIError(Exception):
    pass

def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="readme-generator",
        description="Generate a README for a Python project using LLMs.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  readme-generator /path/to/project
  readme-generator . --output custom_readme.md
  readme-generator /path/to/project --model gemini_2_5_pro
  readme-generator /path/to/project --api-key YOUR_API_KEY
  readme-generator --init-config

Performance Target:
  Generates README for projects up to 25 files/5,000 lines in under 90 seconds
        """
    )
    parser.add_argument('--version', action='version', version='%(prog)s 1.0')
    parser.add_argument('--parse-only', action='store_true', help='Only parse project data to JSON, skip README generation')
    parser.add_argument('project_path', nargs='?', help='Path to the Python project root directory')
    parser.add_argument('--output', '-o', type=str, default='README.md', help='Output README file path')
    parser.add_argument('--json-output', type=str, help='Save parsed project data as JSON file')
    parser.add_argument('--model', choices=['gemini_2_5_pro', 'gemini_2_5_flash', 'gpt_4o', 'gpt_4o_mini', 'claude_sonnet'], default='gemini_2_5_pro', help='LLM model to use')
    parser.add_argument('--api-key', type=str, help='API key for LLM service')
    parser.add_argument('--max-tokens', type=int, default=1_000_000, help='Maximum token budget')
    parser.add_argument('--include-tests', action='store_true', help='Include test files in analysis')
    parser.add_argument('--include-private', action='store_true', help='Include private methods and classes')
    parser.add_argument('--config', type=str, help='Path to configuration file')
    parser.add_argument('--init-config', action='store_true', help='Initialize configuration file with default settings')
    parser.add_argument('--save-config', type=str, help='Save current settings to configuration file')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose output')
    parser.add_argument('--quiet', '-q', action='store_true', help='Suppress non-error output')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode with detailed logging')
    parser.add_argument('--timeout', type=int, default=90, help='Timeout in seconds')
    return parser

def validate_arguments(args: argparse.Namespace) -> None:
    if args.init_config:
        return
    if not args.project_path:
        raise CLIError("Project path is required. Use --help for usage information.")
    project_path = Path(args.project_path).resolve()
    if not project_path.exists():
        raise CLIError(f"Project path does not exist: {args.project_path}")
    if not project_path.is_dir():
        raise CLIError(f"Project path is not a directory: {args.project_path}")
    python_indicators = ['setup.py', 'pyproject.toml', 'requirements.txt', '__init__.py']
    has_python_files = any((project_path / indicator).exists() for indicator in python_indicators)
    has_py_files = any(project_path.glob('**/*.py'))
    if not has_python_files and not has_py_files:
        logger.warning(f"No Python project indicators found in {project_path}")
    if args.max_tokens < 1000:
        raise CLIError("Maximum tokens must be at least 1000")
    if args.timeout < 1:
        raise CLIError("Timeout must be at least 1 second")
    if args.timeout > 300:
        logger.warning(f"Timeout of {args.timeout}s exceeds recommended 90s performance target")

def validate_api_requirements(model_name: str, api_key: str) -> str:
    env_key_mapping = {
        'gemini_2_5_pro': 'GEMINI_API_KEY',
        'gemini_2_5_flash': 'GEMINI_API_KEY', 
        'gpt_4o': 'OPENAI_API_KEY',
        'gpt_4o_mini': 'OPENAI_API_KEY',
        'claude_sonnet': 'ANTHROPIC_API_KEY'
    }
    if not api_key:
        env_var = env_key_mapping.get(model_name)
        if env_var:
            api_key = os.getenv(env_var)
    if not api_key:
        env_var = env_key_mapping.get(model_name, 'API_KEY')
        raise CLIError(
            f"API key required for {model_name}. Provide via --api-key or {env_var}."
        )
    if model_name.startswith('gemini') and not genai:
        raise CLIError("google-generativeai package required for Gemini models.")
    elif model_name.startswith('gpt') and not openai:
        raise CLIError("openai package required for OpenAI models.")
    elif model_name.startswith('claude') and not anthropic:
        raise CLIError("anthropic package required for Claude models.")
    return api_key

def load_configuration(args: argparse.Namespace) -> Config:
    config = Config()
    if args.config:
        try:
            loaded_config = load_config(args.config)
            config.update(loaded_config)
            logger.info(f"Loaded configuration from {args.config}")
        except Exception as e:
            logger.warning(f"Failed to load configuration: {e}")
    config.model_name = args.model
    config.max_tokens = args.max_tokens
    config.include_tests = args.include_tests
    config.include_private = args.include_private
    config.verbose = args.verbose
    config.quiet = args.quiet
    config.debug = args.debug
    config.timeout = args.timeout
    # Fixed: Removed broken cache reference since --no-cache doesn't exist
    config.cache_enabled = True  # Default to enabled
    if args.api_key:
        config.api_key = args.api_key
    return config

async def generate_readme_with_llm(project_data: dict, config: Config, api_key: str) -> str:
    # MVP: project_data is a dict from the MVP parser
    serialized_data = serialize_project_data(project_data)
    token_count = estimate_tokens(json.dumps(serialized_data))
    logger.info(f"Sending ~{token_count:,} tokens to {config.model_name}")
    if token_count > config.max_tokens:
        logger.warning(f"Token count ({token_count:,}) exceeds limit ({config.max_tokens:,})")
    prompt = create_readme_prompt(serialized_data, serialized_data.get('project_metadata', {}).get('name', 'Project'))
    if config.model_name.startswith('gemini'):
        return await generate_with_gemini(prompt, api_key)
    elif config.model_name.startswith('gpt'):
        return await generate_with_openai(prompt, api_key)
    elif config.model_name.startswith('claude'):
        return await generate_with_claude(prompt, api_key)
    else:
        raise LLMAPIError(f"Unsupported model: {config.model_name}")

def create_readme_prompt(project_data: dict, project_name: str) -> str:
    return f"""You are an expert technical writer specializing in creating comprehensive README files for Python projects.
Generate a professional, well-structured README.md file for the project "{project_name}" based on the following parsed project information:
{json.dumps(project_data, indent=2)}

Requirements:
1. Create a complete README with these essential sections:
   - Project Description (clear, concise overview)
   - Installation Instructions
   - Usage Examples
   - Project Structure
   - Dependencies
   - API Documentation (key classes/functions)
2. Use proper Markdown formatting.
3. Be accurate to the actual project structure and dependencies.
Only output the README content in Markdown format."""

async def generate_with_gemini(prompt: str, api_key: str) -> str:
    if not genai:
        raise LLMAPIError("google-generativeai package not available")
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name="gemini-2.5-pro")
        logger.info(f"Generating README with Gemini 2.5 Pro")
        response = await model.generate_content_async(prompt)
        if not response.text:
            raise LLMAPIError("No content from Gemini API")
        return response.text
    except Exception as e:
        raise LLMAPIError(str(e))

async def generate_with_openai(prompt: str, api_key: str) -> str:
    if not openai:
        raise LLMAPIError("openai package not available")
    try:
        client = openai.AsyncOpenAI(api_key=api_key)
        response = await client.chat.completions.create(
            model='gpt-4o',
            messages=[{"role": "system", "content": "You are an expert technical writer."},{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4096
        )
        if not response.choices or not response.choices[0].message.content:
            raise LLMAPIError("No content from OpenAI API")
        return response.choices[0].message.content
    except Exception as e:
        raise LLMAPIError(str(e))

async def generate_with_claude(prompt: str, api_key: str) -> str:
    if not anthropic:
        raise LLMAPIError("anthropic package not available")
    try:
        client = anthropic.AsyncAnthropic(api_key=api_key)
        response = await client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4096,
            temperature=0.7,
            messages=[{"role": "user", "content": prompt}]
        )
        if not response.content or not response.content[0].text:
            raise LLMAPIError("No content from Claude API")
        return response.content[0].text
    except Exception as e:
        raise LLMAPIError(str(e))

def init_config_command(args: argparse.Namespace) -> None:
    config_path = "readme_generator_config.json"
    try:
        config = Config()
        config.model_name = "gemini_2_5_pro"
        config.max_tokens = 1_000_000
        config.timeout = 90
        save_config(config, config_path)
        print(f"✓ Configuration file created: {config_path}")
        print("Edit it to customize settings.")
    except Exception as e:
        raise CLIError(f"Failed to create configuration file: {e}")

async def parse_and_generate_command(args: argparse.Namespace, config: Config) -> None:
    start_time = time.time()
    try:
        project_path = Path(args.project_path).resolve()
        logger.info(f"Parsing project: {project_path}")
        parsing_start = time.time()
        
        # FIXED: Now properly passing include_tests and include_private arguments
        result = parse_project(
            str(project_path),
            include_tests=config.include_tests,
            include_private=config.include_private
        )
        parsing_time = time.time() - parsing_start

        if not config.quiet:
            print(f"✓ Project parsed in {parsing_time:.2f}s")
            # Show stats if available
            if 'stats' in result:
                stats = result['stats']
                print(f"  Files processed: {stats.get('files_processed', 0)}")
                print(f"  Examples found: {stats.get('examples_found', 0)}")

        # Save JSON output
        json_output_path = args.json_output or f"{project_path.name}_parsed_data.json"
        serialized_data = serialize_project_data(result)
        success = save_json_to_file(serialized_data, json_output_path)
        if success and not config.quiet:
            print(f"✓ JSON data saved to: {json_output_path}")

        if args.parse_only:
            if not config.quiet:
                print(f"✓ Parse-only mode completed in {time.time() - start_time:.2f}s")
            return

        api_key = validate_api_requirements(config.model_name, config.api_key)
        if not config.quiet:
            print(f"Generating README with {config.model_name}...")

        generation_start = time.time()
        readme_content = await generate_readme_with_llm(result, config, api_key)
        generation_time = time.time() - generation_start

        output_path = Path(args.output)
        if not output_path.is_absolute():
            output_path = project_path / output_path
        create_directory(str(output_path.parent))
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        if not config.quiet:
            print(f"✓ README generated in {generation_time:.2f}s")
            print(f"✓ README saved to: {output_path}")

    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"Command failed after {elapsed_time:.2f}s: {e}")
        raise

def main():
    try:
        parser = create_parser()
        args = parser.parse_args()
        if args.init_config:
            init_config_command(args)
            return
        validate_arguments(args)
        config = load_configuration(args)
        config.parse_only = getattr(args, 'parse_only', False)
        if config.debug:
            logging.getLogger().setLevel(logging.DEBUG)
        elif config.verbose:
            logging.getLogger().setLevel(logging.INFO)
        elif config.quiet:
            logging.getLogger().setLevel(logging.ERROR)
        if args.save_config:
            save_config(config, args.save_config)
            if not config.quiet:
                print(f"✓ Configuration saved to: {args.save_config}")
        asyncio.run(parse_and_generate_command(args, config))
    except CLIError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\config.py
"""
Minimal configuration management for the README generator.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, Union
import logging

logger = logging.getLogger(__name__)

class Config:
    """Minimal configuration for README generator."""
    
    def __init__(self):
        # Core settings used by CLI
        self.model_name: str = "gemini_2_5_pro"
        self.max_tokens: int = 1_000_000
        self.api_key: Optional[str] = None
        
        # Parsing options
        self.include_tests: bool = False
        self.include_private: bool = False
        
        # CLI options
        self.verbose: bool = False
        self.quiet: bool = False
        self.debug: bool = False
        self.timeout: int = 90
        self.cache_enabled: bool = True
        self.parse_only: bool = False
        
        # Try to get API key from environment
        self._load_api_key_from_env()
    
    def _load_api_key_from_env(self) -> None:
        """Load API key from environment variables."""
        env_keys = {
            "gemini_2_5_pro": "GEMINI_API_KEY",
            "gemini_2_5_flash": "GEMINI_API_KEY",
            "gpt_4o": "OPENAI_API_KEY",
            "gpt_4o_mini": "OPENAI_API_KEY",
            "claude_sonnet": "ANTHROPIC_API_KEY",
        }
        
        env_key = env_keys.get(self.model_name)
        if env_key and not self.api_key:
            self.api_key = os.getenv(env_key)
    
    def update(self, other: Union['Config', Dict[str, Any]]) -> None:
        """Update configuration with values from another config or dictionary."""
        if isinstance(other, Config):
            other_dict = self._to_dict(other)
        else:
            other_dict = other
        
        for key, value in other_dict.items():
            if hasattr(self, key):
                setattr(self, key, value)
    
    def _to_dict(self, config: 'Config') -> Dict[str, Any]:
        """Convert config object to dictionary."""
        return {
            attr: getattr(config, attr) 
            for attr in dir(config) 
            if not attr.startswith('_') and not callable(getattr(config, attr))
        }
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert this configuration to dictionary."""
        return self._to_dict(self)

def load_config(config_path: Union[str, Path]) -> Config:
    """Load configuration from JSON file."""
    config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        config = Config()
        config.update(data)
        
        # FIXED: Reload API key from environment after updating model_name from file
        config._load_api_key_from_env()
        
        logger.info(f"Loaded configuration from {config_path}")
        return config
        
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in configuration file: {e}")
    except Exception as e:
        raise ValueError(f"Failed to load configuration: {e}")


def save_config(config: Config, config_path: Union[str, Path]) -> None:
    """Save configuration to JSON file."""
    config_path = Path(config_path)
    
    # Create directory if it doesn't exist
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config.to_dict(), f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved configuration to {config_path}")
        
    except Exception as e:
        raise IOError(f"Failed to save configuration: {e}")


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\__init__.py
"""
README Generator - Automated README generation for Python projects.
"""

from .parser.project_parser import parse_project
from .utils.token_counter import estimate_tokens, count_tokens_in_dict
from .utils.content_prioritizer import filter_content_under_token_limit
from .utils.json_serializer import serialize_project_data
from .config import Config, load_config, save_config

__version__ = "0.1.0"
__author__ = "Your Name"
__email__ = "your.email@example.com"
__description__ = "Automated README generation for Python projects using LLM APIs"
__url__ = "https://github.com/yourusername/readme-generator"

# Main API exports
__all__ = [
    # Core parsing functions
    "parse_project",
    
    # Utility functions
    "estimate_tokens",
    "count_tokens_in_dict",
    "filter_content_under_token_limit",
    "serialize_project_data",
    
    # Configuration
    "Config",
    "load_config",
    "save_config",
    
    # Metadata
    "__version__",
    "__author__",
    "__email__",
    "__description__",
    "__url__",
]

# Package-level configuration
DEFAULT_CONFIG = {
    "model_name": "gemini_2_5_pro",
    "max_tokens": 1_000_000,
    "include_tests": False,
    "include_private": False,
    "output_format": "markdown",
    "verbose": False,
    "cache_enabled": True,
    "timeout": 90,
}

def get_version() -> str:
    """Get the current version of the package."""
    return __version__

def get_package_info() -> dict:
    """Get comprehensive package information."""
    return {
        "name": "readme-generator",
        "version": __version__,
        "author": __author__,
        "email": __email__,
        "description": __description__,
        "url": __url__,
        "python_requires": ">=3.8",
        "license": "MIT",
    }


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\__main__.py
"""
CLI entry point for the README generator package.

This module provides the main entry point when the package is run as a module
using 'python -m readme_generator'. It handles command-line argument parsing
and delegates to the appropriate CLI functions.
"""

import sys
import os
import logging
from pathlib import Path

# Add the package to Python path if running as script
if __name__ == "__main__":
    # Get the directory containing this file
    current_dir = Path(__file__).parent
    # Add parent directory to path so we can import the package
    sys.path.insert(0, str(current_dir.parent))

from cli import main


def setup_logging(verbose: bool = False):
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Set up console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(console_handler)
    
    # Reduce noise from third-party libraries
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('requests').setLevel(logging.WARNING)
    logging.getLogger('httpx').setLevel(logging.WARNING)


def check_python_version():
    """Check if Python version is compatible."""
    if sys.version_info < (3, 8):
        print("Error: Python 3.8 or higher is required.")
        print(f"You are using Python {sys.version}")
        sys.exit(1)


def handle_exceptions():
    """Set up global exception handling."""
    def exception_handler(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            print("\nOperation cancelled by user.")
            sys.exit(1)
        else:
            # Log the exception
            logging.error(
                "Uncaught exception", 
                exc_info=(exc_type, exc_value, exc_traceback)
            )
            print(f"An unexpected error occurred: {exc_value}")
            sys.exit(1)
    
    sys.excepthook = exception_handler


def entry_point():
    """Main entry point for the CLI application."""
    try:
        # Check Python version compatibility
        check_python_version()
        
        # Set up global exception handling
        handle_exceptions()
        
        # Parse arguments and determine verbosity early
        verbose = '--verbose' in sys.argv or '-v' in sys.argv
        
        # Set up logging
        setup_logging(verbose)
        
        # Log startup information
        logger = logging.getLogger(__name__)
        logger.info(f"Starting README Generator v{get_version()}")
        logger.debug(f"Python version: {sys.version}")
        logger.debug(f"Command line arguments: {sys.argv}")
        
        # Run the main CLI function
        main()
        
    except Exception as e:
        print(f"Failed to start README Generator: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # Import version info
    try:
        from readme_generator import get_version
    except ImportError:
        def get_version():
            return "0.1.0"
    
    entry_point()


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\code_parser.py
import ast
from pathlib import Path
from typing import Optional, Dict, Any, List

def parse_code_file(file_path: str, extract_full_code: bool = False) -> Optional[Dict[str, Any]]:
    """
    Parse a Python file and extract detailed information including full code for entry points.
    
    Args:
        file_path: Path to the Python file
        extract_full_code: Whether to include full source code (for entry points)
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            source = f.read()
        tree = ast.parse(source)
    except UnicodeDecodeError as e:
        logger.warning(f"Encoding error reading {file_path}: {e}")
        return None
    except SyntaxError as e:
        logger.warning(f"Syntax error parsing {file_path}: {e}")
        return None
    except Exception as e:
        logger.error(f"Failed to parse {file_path}: {e}")
        return None

    module_name = Path(file_path).stem
    module_info = {
        "name": module_name,
        "file": file_path,
        "docstring": ast.get_docstring(tree),
        "classes": [],
        "functions": [],
        "imports": [],
        "constants": []
    }
    
    # Include full source code for entry points
    if extract_full_code:
        module_info["source_code"] = source

    # Parse module-level nodes
    for node in tree.body:
        if isinstance(node, ast.ClassDef):
            class_info = {
                "name": node.name,
                "docstring": ast.get_docstring(node),
                "methods": [],
                "bases": [_get_name_from_node(base) for base in node.bases],
                "decorators": [_get_name_from_node(dec) for dec in node.decorator_list]
            }
            
            # Parse class methods
            for item in node.body:
                if isinstance(item, ast.FunctionDef):
                    method_info = {
                        "name": item.name,
                        "docstring": ast.get_docstring(item),
                        "decorators": [_get_name_from_node(dec) for dec in item.decorator_list],
                        "args": [arg.arg for arg in item.args.args]
                    }
                    class_info["methods"].append(method_info)
            
            module_info["classes"].append(class_info)
            
        elif isinstance(node, ast.FunctionDef):
            function_info = {
                "name": node.name,
                "docstring": ast.get_docstring(node),
                "decorators": [_get_name_from_node(dec) for dec in node.decorator_list],
                "args": [arg.arg for arg in node.args.args]
            }
            module_info["functions"].append(function_info)
            
        elif isinstance(node, ast.Import):
            for alias in node.names:
                module_info["imports"].append(alias.name)
                
        elif isinstance(node, ast.ImportFrom):
            module_name = node.module or ""
            for alias in node.names:
                import_str = f"from {module_name} import {alias.name}" if module_name else f"import {alias.name}"
                module_info["imports"].append(import_str)

    return module_info

def _get_name_from_node(node) -> str:
    """Helper to extract name from AST node."""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        return f"{_get_name_from_node(node.value)}.{node.attr}"
    elif isinstance(node, ast.Constant):
        return str(node.value)
    else:
        return str(node)


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\dependency_parser.py
import ast
import re
from pathlib import Path
from typing import List, Dict, Any

def parse_dependencies(project_path: str) -> List[str]:
    """
    Extract dependencies from multiple sources:
    - pyproject.toml (PEP 621 and Poetry)
    - setup.py (install_requires)
    - setup.cfg (options.install_requires)  
    - requirements.txt files (and variants)
    - Pipfile (pipenv)
    - environment.yml (conda)
    """
    project_path = Path(project_path).resolve()
    dependencies = []
    
    # Try each source in order of preference
    deps_sources = [
        _parse_pyproject_dependencies,
        _parse_setup_py_dependencies,
        _parse_setup_cfg_dependencies,
        _parse_requirements_files,
        _parse_pipfile_dependencies,
        _parse_conda_dependencies,
    ]
    
    for parse_func in deps_sources:
        deps = parse_func(project_path)
        if deps:
            dependencies.extend(deps)
    
    # Remove duplicates while preserving order
    seen = set()
    unique_deps = []
    for dep in dependencies:
        if dep not in seen:
            seen.add(dep)
            unique_deps.append(dep)
    
    return unique_deps

def _parse_pyproject_dependencies(project_path: Path) -> List[str]:
    """Parse dependencies from pyproject.toml (PEP 621 and Poetry)."""
    pyproject = project_path / "pyproject.toml"
    if not pyproject.exists():
        return []
    
    try:
        import tomllib
    except ImportError:
        try:
            import tomli as tomllib
        except ImportError:
            return []
    
    try:
        with open(pyproject, "rb") as f:
            data = tomllib.load(f)
        
        dependencies = []
        
        # PEP 621 project dependencies
        project = data.get("project", {})
        if "dependencies" in project:
            dependencies.extend(project["dependencies"])
        
        # Optional dependencies
        if "optional-dependencies" in project:
            for group_deps in project["optional-dependencies"].values():
                dependencies.extend(group_deps)
        
        # Poetry dependencies
        poetry = data.get("tool", {}).get("poetry", {})
        if "dependencies" in poetry:
            poetry_deps = poetry["dependencies"]
            for name, spec in poetry_deps.items():
                if name != "python":  # Skip Python version
                    if isinstance(spec, str):
                        dependencies.append(f"{name}{spec}")
                    elif isinstance(spec, dict) and "version" in spec:
                        dependencies.append(f"{name}{spec['version']}")
                    else:
                        dependencies.append(name)
        
        if "dev-dependencies" in poetry:
            dev_deps = poetry["dev-dependencies"]
            for name, spec in dev_deps.items():
                if isinstance(spec, str):
                    dependencies.append(f"{name}{spec}")
                elif isinstance(spec, dict) and "version" in spec:
                    dependencies.append(f"{name}{spec['version']}")
                else:
                    dependencies.append(name)
        
        return dependencies
        
    except Exception:
        return []

def _parse_setup_py_dependencies(project_path: Path) -> List[str]:
    """Parse dependencies from setup.py."""
    setup_py = project_path / "setup.py"
    if not setup_py.exists():
        return []
    
    try:
        with open(setup_py, "r", encoding="utf-8") as f:
            content = f.read()
        
        try:
            tree = ast.parse(content)
            return _extract_setup_dependencies_ast(tree)
        except:
            return _extract_setup_dependencies_regex(content)
            
    except Exception:
        return []

def _extract_setup_dependencies_ast(tree: ast.AST) -> List[str]:
    """Extract dependencies from setup() call using AST."""
    dependencies = []
    
    for node in ast.walk(tree):
        if (isinstance(node, ast.Call) 
            and getattr(node.func, "id", "") == "setup"):
            
            for kw in node.keywords:
                if kw.arg in ("install_requires", "requires"):
                    if isinstance(kw.value, (ast.List, ast.Tuple)):
                        for elt in kw.value.elts:
                            if isinstance(elt, ast.Constant):
                                dependencies.append(elt.value)
                            elif isinstance(elt, ast.Str):  # Python < 3.8
                                dependencies.append(elt.s)
    
    return dependencies

def _extract_setup_dependencies_regex(content: str) -> List[str]:
    """Extract dependencies using regex patterns."""
    dependencies = []
    
    # Look for install_requires
    install_requires_pattern = r'install_requires\s*=\s*\[(.*?)\]'
    match = re.search(install_requires_pattern, content, re.DOTALL)
    if match:
        deps_str = match.group(1)
        # Extract quoted strings
        dep_pattern = r'["\']([^"\']+)["\']'
        dependencies.extend(re.findall(dep_pattern, deps_str))
    
    return dependencies

def _parse_setup_cfg_dependencies(project_path: Path) -> List[str]:
    """Parse dependencies from setup.cfg."""
    setup_cfg = project_path / "setup.cfg"
    if not setup_cfg.exists():
        return []
    
    try:
        import configparser
        config = configparser.ConfigParser()
        config.read(setup_cfg)
        
        dependencies = []
        if 'options' in config and 'install_requires' in config['options']:
            deps_str = config['options']['install_requires']
            # Split by newlines and clean up
            deps = [dep.strip() for dep in deps_str.split('\n') if dep.strip()]
            dependencies.extend(deps)
        
        return dependencies
    except Exception:
        return []

def _parse_requirements_files(project_path: Path) -> List[str]:
    """Parse dependencies from requirements files."""
    dependencies = []
    
    # Common requirements file patterns
    req_patterns = [
        "requirements.txt",
        "requirements*.txt", 
        "reqs.txt",
        "deps.txt",
        "dependencies.txt",
        "dev-requirements.txt",
        "test-requirements.txt"
    ]
    
    req_files = []
    for pattern in req_patterns:
        req_files.extend(list(project_path.glob(pattern)))
        req_files.extend(list(project_path.glob(f"**/{pattern}")))
    
    for req_file in req_files:
        if req_file.is_file():
            try:
                with open(req_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # Skip empty lines and comments
                        if line and not line.startswith('#') and not line.startswith('-'):
                            # Remove inline comments
                            dep = line.split('#')[0].strip()
                            if dep:
                                dependencies.append(dep)
            except Exception:
                continue
    
    return dependencies

def _parse_pipfile_dependencies(project_path: Path) -> List[str]:
    """Parse dependencies from Pipfile."""
    pipfile = project_path / "Pipfile"
    if not pipfile.exists():
        return []
    
    try:
        import tomllib
    except ImportError:
        try:
            import tomli as tomllib
        except ImportError:
            return []
    
    try:
        with open(pipfile, "rb") as f:
            data = tomllib.load(f)
        
        dependencies = []
        
        # Regular packages
        if "packages" in data:
            for name, spec in data["packages"].items():
                if isinstance(spec, str):
                    dependencies.append(f"{name}{spec}")
                else:
                    dependencies.append(name)
        
        # Dev packages
        if "dev-packages" in data:
            for name, spec in data["dev-packages"].items():
                if isinstance(spec, str):
                    dependencies.append(f"{name}{spec}")
                else:
                    dependencies.append(name)
        
        return dependencies
        
    except Exception:
        return []

def _parse_conda_dependencies(project_path: Path) -> List[str]:
    """Parse dependencies from conda environment files."""
    dependencies = []
    
    env_files = list(project_path.glob("environment*.yml")) + list(project_path.glob("environment*.yaml"))
    
    for env_file in env_files:
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Simple parsing for dependencies section
            in_deps = False
            for line in content.split('\n'):
                line = line.strip()
                if line == "dependencies:":
                    in_deps = True
                    continue
                elif in_deps:
                    if line.startswith('- ') and not line.startswith('- pip:'):
                        dep = line[2:].strip()
                        if dep:
                            dependencies.append(dep)
                    elif not line.startswith(' ') and not line.startswith('-'):
                        in_deps = False
        except Exception:
            continue
    
    return dependencies


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\entry_point_parser.py
"""
Parser for identifying and extracting entry points and main scripts.
These are crucial for generating usage instructions in READMEs.
"""

import ast
import os
from pathlib import Path
from typing import List, Dict, Any

def parse_entry_points(project_path: str) -> Dict[str, Any]:
    """
    Identify and extract entry points including CLI scripts, main modules, and setup.py scripts.
    """
    project_path = Path(project_path).resolve()
    
    entry_points = {
        "main_modules": [],
        "cli_scripts": [],
        "setup_scripts": [],
        "package_entry_points": []
    }
    
    # Find main modules (__main__.py files)
    main_files = list(project_path.glob("**/__main__.py"))
    for main_file in main_files:
        entry_info = _extract_main_module_info(main_file)
        if entry_info:
            entry_points["main_modules"].append(entry_info)
    
    # Find CLI scripts (common patterns)
    cli_patterns = ["cli.py", "main.py", "*_cli.py", "run.py", "app.py"]
    for pattern in cli_patterns:
        cli_files = list(project_path.glob(f"**/{pattern}"))
        for cli_file in cli_files:
            # Skip if already found as __main__.py
            if cli_file.name == "__main__.py":
                continue
            entry_info = _extract_cli_script_info(cli_file)
            if entry_info:
                entry_points["cli_scripts"].append(entry_info)
    
    # Extract setup.py console scripts and entry points
    setup_py = project_path / "setup.py"
    if setup_py.exists():
        setup_entry_points = _extract_setup_entry_points(setup_py)
        entry_points["setup_scripts"].extend(setup_entry_points)
    
    # Extract pyproject.toml entry points
    pyproject_toml = project_path / "pyproject.toml"
    if pyproject_toml.exists():
        pyproject_entry_points = _extract_pyproject_entry_points(pyproject_toml)
        entry_points["package_entry_points"].extend(pyproject_entry_points)
    
    return entry_points

def _extract_main_module_info(main_file: Path) -> Dict[str, Any]:
    """Extract information from __main__.py files."""
    try:
        with open(main_file, 'r', encoding='utf-8') as f:
            source = f.read()
        
        return {
            "type": "main_module",
            "file": str(main_file),
            "usage": f"python -m {_get_package_name_from_main(main_file)}",
            "source_code": source,
            "docstring": _extract_module_docstring(source),
            "description": "Main module entry point"
        }
    except Exception:
        return None

def _extract_cli_script_info(cli_file: Path) -> Dict[str, Any]:
    """Extract information from CLI script files."""
    try:
        with open(cli_file, 'r', encoding='utf-8') as f:
            source = f.read()
        
        # Check if this looks like a CLI script
        if not _is_cli_script(source):
            return None
        
        return {
            "type": "cli_script",
            "file": str(cli_file),
            "usage": f"python {cli_file.name}",
            "source_code": source,
            "docstring": _extract_module_docstring(source),
            "description": "Command-line interface script",
            "argument_parser": _extract_argument_parser_info(source)
        }
    except Exception:
        return None

def _extract_setup_entry_points(setup_file: Path) -> List[Dict[str, Any]]:
    """Extract console scripts and entry points from setup.py."""
    entry_points = []
    
    try:
        with open(setup_file, 'r', encoding='utf-8') as f:
            source = f.read()
        
        tree = ast.parse(source)
        
        for node in ast.walk(tree):
            if (isinstance(node, ast.Call) and 
                getattr(node.func, "id", "") == "setup"):
                
                for kw in node.keywords:
                    if kw.arg == "entry_points":
                        # Extract entry points
                        entry_points.extend(_parse_entry_points_dict(kw.value))
                    elif kw.arg == "scripts":
                        # Extract script files
                        entry_points.extend(_parse_scripts_list(kw.value))
    except Exception:
        pass
    
    return entry_points

def _extract_pyproject_entry_points(pyproject_file: Path) -> List[Dict[str, Any]]:
    """Extract entry points from pyproject.toml."""
    entry_points = []
    
    try:
        import tomllib
    except ImportError:
        try:
            import tomli as tomllib
        except ImportError:
            return entry_points
    
    try:
        with open(pyproject_file, "rb") as f:
            data = tomllib.load(f)
        
        # Check project.scripts
        project = data.get("project", {})
        if "scripts" in project:
            for script_name, entry_point in project["scripts"].items():
                entry_points.append({
                    "type": "console_script",
                    "name": script_name,
                    "entry_point": entry_point,
                    "usage": script_name,
                    "description": f"Console script: {script_name}"
                })
        
        # Check project.entry-points
        if "entry-points" in project:
            for group, entries in project["entry-points"].items():
                for name, entry_point in entries.items():
                    entry_points.append({
                        "type": "entry_point",
                        "group": group,
                        "name": name,
                        "entry_point": entry_point,
                        "usage": name if group == "console_scripts" else f"{group}:{name}",
                        "description": f"Entry point: {group}.{name}"
                    })
    
    except Exception:
        pass
    
    return entry_points

def _get_package_name_from_main(main_file: Path) -> str:
    """Get package name from __main__.py file path."""
    parts = main_file.parts
    if "__main__.py" in parts:
        main_index = parts.index("__main__.py")
        if main_index > 0:
            return parts[main_index - 1]
    return main_file.parent.name

def _extract_module_docstring(source: str) -> str:
    """Extract module-level docstring."""
    try:
        tree = ast.parse(source)
        return ast.get_docstring(tree) or ""
    except Exception:
        return ""

def _is_cli_script(source: str) -> bool:
    """Check if source code looks like a CLI script."""
    cli_indicators = [
        "argparse",
        "ArgumentParser",
        "if __name__ == '__main__':",
        "sys.argv",
        "click",
        "@click.command",
        "typer"
    ]
    return any(indicator in source for indicator in cli_indicators)

def _extract_argument_parser_info(source: str) -> Dict[str, Any]:
    """Extract information about argument parser from CLI script."""
    try:
        tree = ast.parse(source)
        
        # Look for ArgumentParser creation and argument definitions
        parser_info = {
            "program_name": None,
            "description": None,
            "arguments": []
        }
        
        for node in ast.walk(tree):
            if (isinstance(node, ast.Call) and
                isinstance(node.func, ast.Attribute) and
                node.func.attr == "ArgumentParser"):
                
                # Extract ArgumentParser arguments
                for kw in node.keywords:
                    if kw.arg == "prog" and isinstance(kw.value, ast.Constant):
                        parser_info["program_name"] = kw.value.value
                    elif kw.arg == "description" and isinstance(kw.value, ast.Constant):
                        parser_info["description"] = kw.value.value
        
        return parser_info
    except Exception:
        return {}

def _parse_entry_points_dict(node) -> List[Dict[str, Any]]:
    """Parse entry_points dictionary from setup.py AST node."""
    # This would need more sophisticated AST parsing
    # For now, return empty list
    return []

def _parse_scripts_list(node) -> List[Dict[str, Any]]:
    """Parse scripts list from setup.py AST node."""
    scripts = []
    if isinstance(node, (ast.List, ast.Tuple)):
        for item in node.elts:
            if isinstance(item, ast.Constant):
                scripts.append({
                    "type": "script_file",
                    "file": item.value,
                    "usage": f"python {item.value}",
                    "description": f"Script file: {item.value}"
                })
    return scripts


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\example_parser.py
import ast
import re
from pathlib import Path
from typing import List, Dict, Any

def parse_examples(file_path: str) -> List[Dict[str, Any]]:
    """
    Extract code examples from multiple sources:
    - Docstring examples (doctest format)
    - Code blocks in docstrings
    - Main guard blocks
    - Function/class usage patterns
    - Comments with example code
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            source = f.read()
        tree = ast.parse(source)
    except Exception:
        return []
    
    examples = []
    
    # Extract from docstrings
    examples.extend(_extract_docstring_examples(tree, file_path, source))
    
    # Extract main guard examples
    examples.extend(_extract_main_guard_examples(tree, file_path, source))
    
    # Extract comment examples
    examples.extend(_extract_comment_examples(source, file_path))
    
    # Extract usage patterns
    examples.extend(_extract_usage_patterns(tree, file_path, source))
    
    return examples

def _extract_docstring_examples(tree: ast.AST, file_path: str, source: str) -> List[Dict[str, Any]]:
    """Extract examples from docstrings (doctests and code blocks)."""
    examples = []
    
    # Module docstring
    module_docstring = ast.get_docstring(tree)
    if module_docstring:
        examples.extend(_parse_docstring_for_examples(module_docstring, file_path, "module"))
    
    # Walk through all nodes
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            class_docstring = ast.get_docstring(node)
            if class_docstring:
                examples.extend(_parse_docstring_for_examples(
                    class_docstring, file_path, f"class {node.name}"
                ))
        elif isinstance(node, ast.FunctionDef):
            func_docstring = ast.get_docstring(node)
            if func_docstring:
                examples.extend(_parse_docstring_for_examples(
                    func_docstring, file_path, f"function {node.name}"
                ))
    
    return examples

def _parse_docstring_for_examples(docstring: str, file_path: str, context: str) -> List[Dict[str, Any]]:
    """Parse a docstring for various types of examples."""
    examples = []
    
    if not docstring:
        return examples
    
    # 1. Doctest examples (>>> format)
    doctest_examples = _extract_doctest_examples(docstring)
    for example in doctest_examples:
        examples.append({
            "file": file_path,
            "context": context,
            "type": "doctest",
            "code": example,
            "language": "python"
        })
    
    # 2. Code blocks (markdown style)
    code_blocks = _extract_code_blocks(docstring)
    for block in code_blocks:
        examples.append({
            "file": file_path,
            "context": context,
            "type": "code_block",
            "code": block["code"],
            "language": block.get("language", "python")
        })
    
    # 3. Example sections
    example_sections = _extract_example_sections(docstring)
    for section in example_sections:
        examples.append({
            "file": file_path,
            "context": context,
            "type": "example_section",
            "code": section,
            "language": "python"
        })
    
    return examples

def _extract_doctest_examples(docstring: str) -> List[str]:
    """Extract doctest-style examples (>>> format)."""
    examples = []
    current_example = []
    
    for line in docstring.splitlines():
        stripped = line.strip()
        if stripped.startswith(">>>"):
            current_example.append(stripped)
        elif stripped.startswith("...") and current_example:
            current_example.append(stripped)
        elif current_example and not stripped:
            # Empty line, might be end of example
            continue
        elif current_example:
            # Non-continuation line, end current example
            examples.append('\n'.join(current_example))
            current_example = []
    
    # Don't forget the last example
    if current_example:
        examples.append('\n'.join(current_example))
    
    return examples

def _extract_code_blocks(docstring: str) -> List[Dict[str, Any]]:
    """Extract code blocks from markdown-style formatting."""
    blocks = []
    
    # Pattern for fenced code blocks
    fenced_pattern = r'``````'
    matches = re.findall(fenced_pattern, docstring, re.DOTALL)
    
    for language, code in matches:
        blocks.append({
            "code": code.strip(),
            "language": language or "python"
        })
    
    # Pattern for indented code blocks (following "Example:" or similar)
    example_pattern = r'(?:Example|Usage|Code):\s*\n((?:    .*\n?)+)'
    matches = re.findall(example_pattern, docstring, re.MULTILINE)
    
    for match in matches:
        # Remove common indentation
        lines = match.split('\n')
        if lines:
            # Find minimum indentation
            min_indent = min(len(line) - len(line.lstrip()) 
                           for line in lines if line.strip())
            # Remove common indentation
            code_lines = [line[min_indent:] if len(line) >= min_indent else line 
                         for line in lines]
            code = '\n'.join(code_lines).strip()
            if code:
                blocks.append({
                    "code": code,
                    "language": "python"
                })
    
    return blocks

def _extract_example_sections(docstring: str) -> List[str]:
    """Extract dedicated example sections."""
    examples = []
    
    # Look for sections starting with "Examples:", "Example:", etc.
    sections = re.split(r'\n\s*(Examples?|Usage|Sample Code):\s*\n', docstring, flags=re.IGNORECASE)
    
    for i in range(1, len(sections), 2):  # Every other section starting from 1
        if i + 1 < len(sections):
            example_content = sections[i + 1].split('\n\n')[0]  # Take first paragraph
            if example_content.strip():
                examples.append(example_content.strip())
    
    return examples

def _extract_main_guard_examples(tree: ast.AST, file_path: str, source: str) -> List[Dict[str, Any]]:
    """Extract examples from if __name__ == '__main__': blocks."""
    examples = []
    
    for node in ast.walk(tree):
        if (isinstance(node, ast.If) and 
            isinstance(node.test, ast.Compare) and
            isinstance(node.test.left, ast.Name) and
            node.test.left.id == '__name__'):
            
            # Extract the code from the main guard
            start_line = node.lineno - 1
            end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 10
            
            source_lines = source.split('\n')
            if start_line < len(source_lines):
                # Find the actual end of the if block
                main_guard_lines = []
                indent_level = None
                
                for i in range(start_line, min(end_line, len(source_lines))):
                    line = source_lines[i]
                    if indent_level is None and line.strip():
                        indent_level = len(line) - len(line.lstrip())
                    
                    if line.strip():  # Non-empty line
                        current_indent = len(line) - len(line.lstrip())
                        if current_indent >= indent_level:
                            main_guard_lines.append(line)
                        else:
                            break
                    else:
                        main_guard_lines.append(line)
                
                if main_guard_lines:
                    code = '\n'.join(main_guard_lines)
                    examples.append({
                        "file": file_path,
                        "context": "main guard",
                        "type": "main_example",
                        "code": code.strip(),
                        "language": "python"
                    })
    
    return examples

def _extract_comment_examples(source: str, file_path: str) -> List[Dict[str, Any]]:
    """Extract examples from comments."""
    examples = []
    
    # Look for comment blocks that contain example code
    comment_blocks = []
    current_block = []
    
    for line in source.split('\n'):
        stripped = line.strip()
        if stripped.startswith('#') and len(stripped) > 1:
            comment_text = stripped[1:].strip()
            if comment_text:
                current_block.append(comment_text)
        elif current_block:
            comment_blocks.append('\n'.join(current_block))
            current_block = []
    
    if current_block:
        comment_blocks.append('\n'.join(current_block))
    
    # Look for example patterns in comment blocks
    for block in comment_blocks:
        if any(keyword in block.lower() for keyword in ['example', 'usage', 'sample', 'demo']):
            # Try to extract code-like content
            code_lines = []
            for line in block.split('\n'):
                # Look for lines that look like code
                if (any(char in line for char in ['=', '(', ')', '.', 'import', 'from', 'def', 'class']) 
                    and not line.lower().startswith(('example', 'usage', 'sample', 'demo'))):
                    code_lines.append(line)
            
            if code_lines:
                examples.append({
                    "file": file_path,
                    "context": "comment",
                    "type": "comment_example",
                    "code": '\n'.join(code_lines),
                    "language": "python"
                })
    
    return examples

def _extract_usage_patterns(tree: ast.AST, file_path: str, source: str) -> List[Dict[str, Any]]:
    """Extract common usage patterns from the code itself."""
    examples = []
    
    # Look for class instantiation patterns
    for node in ast.walk(tree):
        if isinstance(node, ast.Assign):
            # Look for assignments that might be examples
            if (isinstance(node.value, ast.Call) and 
                isinstance(node.value.func, ast.Name)):
                
                # Get the source code for this assignment
                if hasattr(node, 'lineno'):
                    line_no = node.lineno - 1
                    source_lines = source.split('\n')
                    if line_no < len(source_lines):
                        line = source_lines[line_no].strip()
                        if line and not line.startswith(('_', 'self.')):
                            examples.append({
                                "file": file_path,
                                "context": "usage pattern",
                                "type": "instantiation",
                                "code": line,
                                "language": "python"
                            })
    
    return examples


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\metadata_parser.py
import ast
import os
import re
from pathlib import Path
from typing import Dict, Any, Optional

def parse_metadata(project_path: str) -> Dict[str, Any]:
    """
    Extract project metadata from multiple sources:
    - pyproject.toml
    - setup.py  
    - setup.cfg
    - __init__.py files
    - README files
    - Project directory structure
    """
    project_path = Path(project_path).resolve()
    metadata = {}
    
    # Try pyproject.toml first (most modern)
    metadata.update(_parse_pyproject_toml(project_path))
    
    # Try setup.py (traditional)
    if not metadata:
        metadata.update(_parse_setup_py(project_path))
    
    # Try setup.cfg (setuptools)
    if not metadata:
        metadata.update(_parse_setup_cfg(project_path))
    
    # Extract from __init__.py files
    init_metadata = _parse_init_files(project_path)
    for key, value in init_metadata.items():
        if not metadata.get(key):
            metadata[key] = value
    
    # Extract from README
    readme_metadata = _parse_readme(project_path)
    for key, value in readme_metadata.items():
        if not metadata.get(key):
            metadata[key] = value
    
    # Infer from directory structure if still missing
    if not metadata.get('name'):
        metadata['name'] = project_path.name
    
    return metadata

def _parse_pyproject_toml(project_path: Path) -> Dict[str, Any]:
    """Parse pyproject.toml for project metadata."""
    pyproject = project_path / "pyproject.toml"
    if not pyproject.exists():
        return {}
    
    try:
        import tomllib
    except ImportError:
        try:
            import tomli as tomllib
        except ImportError:
            return {}
    
    try:
        with open(pyproject, "rb") as f:
            data = tomllib.load(f)
        
        project = data.get("project", {})
        metadata = {}
        
        # Standard PEP 621 fields
        for field in ["name", "version", "description", "readme", "license"]:
            if field in project:
                metadata[field] = project[field]
        
        # Authors
        if "authors" in project:
            authors = project["authors"]
            if isinstance(authors, list) and authors:
                metadata["author"] = authors[0].get("name", "")
                metadata["author_email"] = authors[0].get("email", "")
        
        # URLs
        if "urls" in project:
            urls = project["urls"]
            metadata["homepage"] = urls.get("homepage", urls.get("Home", ""))
            metadata["repository"] = urls.get("repository", urls.get("Repository", ""))
        
        return metadata
        
    except Exception:
        return {}

def _parse_setup_py(project_path: Path) -> Dict[str, Any]:
    """Parse setup.py for project metadata."""
    setup_py = project_path / "setup.py"
    if not setup_py.exists():
        return {}
    
    try:
        with open(setup_py, "r", encoding="utf-8") as f:
            content = f.read()
        
        # Try to parse as AST first
        try:
            tree = ast.parse(content)
            return _extract_setup_call_metadata(tree)
        except:
            # Fallback to regex parsing
            return _extract_setup_regex_metadata(content)
            
    except Exception:
        return {}

def _extract_setup_call_metadata(tree: ast.AST) -> Dict[str, Any]:
    """Extract metadata from setup() call in AST."""
    metadata = {}
    
    for node in ast.walk(tree):
        if (isinstance(node, ast.Call) 
            and getattr(node.func, "id", "") == "setup"):
            
            for kw in node.keywords:
                if kw.arg in ("name", "version", "description", "author", 
                             "author_email", "url", "license"):
                    if isinstance(kw.value, ast.Constant):
                        metadata[kw.arg] = kw.value.value
                    elif isinstance(kw.value, ast.Str):  # Python < 3.8
                        metadata[kw.arg] = kw.value.s
    
    return metadata

def _extract_setup_regex_metadata(content: str) -> Dict[str, Any]:
    """Extract metadata using regex patterns."""
    metadata = {}
    
    patterns = {
        'name': r'name\s*=\s*["\']([^"\']+)["\']',
        'version': r'version\s*=\s*["\']([^"\']+)["\']',
        'description': r'description\s*=\s*["\']([^"\']+)["\']',
        'author': r'author\s*=\s*["\']([^"\']+)["\']',
        'author_email': r'author_email\s*=\s*["\']([^"\']+)["\']',
        'url': r'url\s*=\s*["\']([^"\']+)["\']',
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, content, re.MULTILINE)
        if match:
            metadata[key] = match.group(1)
    
    return metadata

def _parse_setup_cfg(project_path: Path) -> Dict[str, Any]:
    """Parse setup.cfg for project metadata."""
    setup_cfg = project_path / "setup.cfg"
    if not setup_cfg.exists():
        return {}
    
    try:
        import configparser
        config = configparser.ConfigParser()
        config.read(setup_cfg)
        
        metadata = {}
        if 'metadata' in config:
            section = config['metadata']
            for key in ('name', 'version', 'description', 'author', 'author_email', 'url'):
                if key in section:
                    metadata[key] = section[key]
        
        return metadata
    except Exception:
        return {}

def _parse_init_files(project_path: Path) -> Dict[str, Any]:
    """Extract metadata from __init__.py files."""
    metadata = {}
    
    # Look for __init__.py files
    init_files = list(project_path.glob("**/__init__.py"))
    
    for init_file in init_files:
        try:
            with open(init_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Look for common metadata variables
            patterns = {
                'version': r'__version__\s*=\s*["\']([^"\']+)["\']',
                'author': r'__author__\s*=\s*["\']([^"\']+)["\']',
                'description': r'__description__\s*=\s*["\']([^"\']+)["\']',
                'email': r'__email__\s*=\s*["\']([^"\']+)["\']',
            }
            
            for key, pattern in patterns.items():
                if not metadata.get(key):
                    match = re.search(pattern, content)
                    if match:
                        metadata[key] = match.group(1)
        except Exception:
            continue
    
    return metadata

def _parse_readme(project_path: Path) -> Dict[str, Any]:
    """Extract metadata from README files."""
    metadata = {}
    
    # Look for README files
    readme_patterns = ["README*", "readme*", "Readme*"]
    readme_files = []
    
    for pattern in readme_patterns:
        readme_files.extend(list(project_path.glob(pattern)))
    
    for readme_file in readme_files:
        if readme_file.is_file():
            try:
                with open(readme_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract title from first heading
                if not metadata.get('description'):
                    title_match = re.search(r'^#\s*(.+)$', content, re.MULTILINE)
                    if title_match:
                        metadata['description'] = title_match.group(1).strip()
                
                break
            except Exception:
                continue
    
    return metadata


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\project_parser.py
from pathlib import Path
from typing import Any, Dict, List
from .metadata_parser import parse_metadata
from .dependency_parser import parse_dependencies
from .structure_parser import parse_structure
from .example_parser import parse_examples
from .code_parser import parse_code_file
from .entry_point_parser import parse_entry_points

def parse_project(
    project_path: str,
    include_tests: bool = False,
    include_private: bool = False
) -> Dict[str, Any]:
    """
    Orchestrate comprehensive parsing of Python project including entry points.
    """
    project_path = Path(project_path).resolve()

    # Parse project-level information
    metadata = parse_metadata(str(project_path))
    dependencies = parse_dependencies(str(project_path))
    entry_points = parse_entry_points(str(project_path))
    
    # Get basic file structure
    structure = parse_structure(str(project_path))

    # Filter test files if requested
    if not include_tests:
        structure = [
            mod for mod in structure
            if not _is_test_file(mod["file"])
        ]

    def _is_test_file(file_path: str) -> bool:
        """Check if a file is a test file based on naming conventions."""
        path = Path(file_path)
        
        # Check if file is in a test directory
        if any(part.lower() in ('test', 'tests') for part in path.parts[:-1]):
            return True
        
        # Check if filename suggests it's a test
        name = path.stem.lower()
        if name.startswith('test_') or name.endswith('_test') or name == 'test':
            return True
        
        return False


    # Parse detailed code information for each module
    detailed_modules = []
    all_examples = []
    
    # Determine which files are entry points (should include full source)
    entry_point_files = set()
    for ep_category in entry_points.values():
        if isinstance(ep_category, list):
            for ep in ep_category:
                if 'file' in ep:
                    entry_point_files.add(ep['file'])
    
    for module_basic in structure:
        file_path = module_basic["file"]
        
        # Extract full code for entry points
        is_entry_point = file_path in entry_point_files or _is_likely_entry_point(file_path)
        
        # Get detailed code information
        code_details = parse_code_file(file_path, extract_full_code=is_entry_point)
        
        if code_details:
            detailed_modules.append(code_details)
        else:
            detailed_modules.append(module_basic)
        
        # Extract examples from this file
        file_examples = parse_examples(file_path)
        all_examples.extend(file_examples)

    # Calculate comprehensive stats
    total_classes = sum(len(mod.get("classes", [])) for mod in detailed_modules)
    total_functions = sum(len(mod.get("functions", [])) for mod in detailed_modules)
    total_methods = sum(
        sum(len(cls.get("methods", [])) for cls in mod.get("classes", []))
        for mod in detailed_modules
    )

    return {
        "success": True,
        "project_metadata": metadata,
        "dependencies": dependencies,
        "entry_points": entry_points,  # New: Entry point information
        "modules": detailed_modules,
        "examples": all_examples,
        "stats": {
            "files_processed": len(detailed_modules),
            "examples_found": len(all_examples),
            "classes_found": total_classes,
            "functions_found": total_functions,
            "methods_found": total_methods,
            "entry_points_found": sum(len(eps) if isinstance(eps, list) else 0 for eps in entry_points.values()),
        }
    }

def _is_likely_entry_point(file_path: str) -> bool:
    """Check if a file is likely an entry point based on naming patterns."""
    file_name = Path(file_path).name
    entry_patterns = [
        "main.py", "cli.py", "__main__.py", "run.py", "app.py", 
        "start.py", "launch.py", "execute.py"
    ]
    return any(file_name == pattern or file_name.endswith(f"_{pattern}") for pattern in entry_patterns)

# Test harness
if __name__ == "__main__":
    import sys
    import json
    project_root = sys.argv[1] if len(sys.argv) > 1 else "."
    result = parse_project(project_root)
    print(json.dumps(result, indent=2))


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\structure_parser.py
import os
from pathlib import Path

def parse_structure(project_path):
    """
    Traverse project directory and find all Python (.py) files,
    returning a list of dicts with keys: 'file' (full path) and 'name' (module name).
    Ignores folders named __pycache__, venv, build, dist, tests, docs.
    """
    project_path = Path(project_path).resolve()
    ignored_dirs = {"__pycache__", "venv", "build", "dist", "tests", "docs"}

    modules = []

    for root, dirs, files in os.walk(project_path):
        # Filter ignored dirs to avoid descending into them
        dirs[:] = [d for d in dirs if d not in ignored_dirs]

        for file in files:
            if file.endswith(".py"):
                full_path = Path(root) / file
                modules.append({
                    "file": str(full_path.resolve()),
                    "name": full_path.stem
                })

    return modules


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\parser\__init__.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\conftest.py
import sys
import os
from pathlib import Path

# Get the directory containing this file (tests directory)
tests_dir = Path(__file__).parent
# Get project root (parent of tests directory)  
project_root = tests_dir.parent
# Get src directory
src_dir = project_root / "src"

# Add both project root and src directory to Python path
for path in [str(project_root), str(src_dir)]:
    if path not in sys.path:
        sys.path.insert(0, path)

# Verify paths were added
print(f"Added to sys.path: {project_root}, {src_dir}")


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\test_cli.py
import pytest
from unittest.mock import patch, MagicMock, AsyncMock
from argparse import Namespace
import tempfile
from pathlib import Path

from cli import (
    create_parser, 
    validate_arguments, 
    load_configuration,
    validate_api_requirements,
    init_config_command,
    CLIError,
    LLMAPIError
)
from config import Config

class TestCLIParser:
    """Test cases for CLI argument parsing."""
    
    def test_create_parser_defaults(self):
        """Test parser creation with default values."""
        parser = create_parser()
        args = parser.parse_args(["/test/path"])
        
        assert args.project_path == "/test/path"
        assert args.model == "gemini_2_5_pro"
        assert args.max_tokens == 1_000_000
        assert args.include_tests is False
        assert args.include_private is False
        assert args.verbose is False
        assert args.quiet is False
        assert args.debug is False
        assert args.timeout == 90
        assert args.output == "README.md"
        assert args.parse_only is False
    
    def test_parse_args_with_all_options(self):
        """Test parsing arguments with various options."""
        parser = create_parser()
        args = parser.parse_args([
            "/test/path",
            "--model", "gpt_4o",
            "--max-tokens", "500000",
            "--include-tests",
            "--include-private",
            "--verbose",
            "--debug",
            "--output", "custom_readme.md",
            "--json-output", "data.json",
            "--api-key", "test-key",
            "--timeout", "120",
            "--parse-only",
            "--config", "config.json",
            "--save-config", "save.json"
        ])
        
        assert args.project_path == "/test/path"
        assert args.model == "gpt_4o"
        assert args.max_tokens == 500000
        assert args.include_tests is True
        assert args.include_private is True
        assert args.verbose is True
        assert args.debug is True
        assert args.output == "custom_readme.md"
        assert args.json_output == "data.json"
        assert args.api_key == "test-key"
        assert args.timeout == 120
        assert args.parse_only is True
        assert args.config == "config.json"
        assert args.save_config == "save.json"
    
    def test_model_choices(self):
        """Test that only valid model choices are accepted."""
        parser = create_parser()
        
        # Valid models should work
        for model in ['gemini_2_5_pro', 'gemini_2_5_flash', 'gpt_4o', 'gpt_4o_mini', 'claude_sonnet']:
            args = parser.parse_args(["/test/path", "--model", model])
            assert args.model == model
        
        # Invalid model should raise SystemExit (argparse error)
        with pytest.raises(SystemExit):
            parser.parse_args(["/test/path", "--model", "invalid_model"])

class TestArgumentValidation:
    """Test cases for argument validation."""
    
    def test_validate_arguments_init_config(self):
        """Test that init-config bypasses other validation."""
        args = Namespace(init_config=True, project_path=None)
        # Should not raise an exception
        validate_arguments(args)
    
    def test_validate_arguments_success(self, tmp_path):
        """Test successful argument validation."""
        test_dir = tmp_path
        (test_dir / "__init__.py").write_text("")
        
        args = Namespace(
            project_path=str(test_dir),
            init_config=False,
            max_tokens=10000,
            timeout=60
        )
        # Should not raise an exception
        validate_arguments(args)
    
    def test_validate_arguments_missing_path(self):
        """Test validation fails when project path is missing."""
        args = Namespace(project_path=None, init_config=False)
        
        with pytest.raises(CLIError) as exc_info:
            validate_arguments(args)
        assert "Project path is required" in str(exc_info.value)
    
    def test_validate_arguments_nonexistent_path(self):
        """Test validation fails for nonexistent path."""
        args = Namespace(
            project_path="/nonexistent/path",
            init_config=False,
            max_tokens=10000,
            timeout=60
        )
        
        with pytest.raises(CLIError) as exc_info:
            validate_arguments(args)
        assert "does not exist" in str(exc_info.value)
    
    def test_validate_arguments_not_directory(self, tmp_path):
        """Test validation fails when path is not a directory."""
        test_file = tmp_path / "not_a_dir.txt"
        test_file.write_text("content")
        
        args = Namespace(
            project_path=str(test_file),
            init_config=False,
            max_tokens=10000,
            timeout=60
        )
        
        with pytest.raises(CLIError) as exc_info:
            validate_arguments(args)
        assert "not a directory" in str(exc_info.value)
    
    def test_validate_arguments_token_limit_too_low(self, tmp_path):
        """Test validation fails when max_tokens is too low."""
        args = Namespace(
            project_path=str(tmp_path),
            init_config=False,
            max_tokens=500,  # Below minimum of 1000
            timeout=60
        )
        
        with pytest.raises(CLIError) as exc_info:
            validate_arguments(args)
        assert "at least 1000" in str(exc_info.value)
    
    def test_validate_arguments_timeout_too_low(self, tmp_path):
        """Test validation fails when timeout is too low."""
        args = Namespace(
            project_path=str(tmp_path),
            init_config=False,
            max_tokens=10000,
            timeout=0  # Below minimum of 1
        )
        
        with pytest.raises(CLIError) as exc_info:
            validate_arguments(args)
        assert "at least 1 second" in str(exc_info.value)

class TestConfigurationLoading:
    """Test cases for configuration loading."""
    
    def test_load_configuration_defaults(self):
        """Test loading configuration with default command-line args."""
        args = Namespace(
            model="gpt_4o",
            max_tokens=500000,
            include_tests=True,
            include_private=False,
            verbose=True,
            quiet=False,
            debug=False,
            timeout=120,
            config=None,
            api_key="test-key"
        )
        
        config = load_configuration(args)
        
        assert isinstance(config, Config)
        assert config.model_name == "gpt_4o"
        assert config.max_tokens == 500000
        assert config.include_tests is True
        assert config.include_private is False
        assert config.verbose is True
        assert config.quiet is False
        assert config.debug is False
        assert config.timeout == 120
        assert config.api_key == "test-key"
        assert config.cache_enabled is True
    
    @patch('cli.load_config')  # FIXED: Changed from 'src.cli.load_config'
    def test_load_configuration_from_file(self, mock_load_config):
        """Test loading configuration from file."""
        # Mock the loaded config
        mock_config = Config()
        mock_config.model_name = "claude_sonnet"
        mock_config.max_tokens = 750000
        mock_load_config.return_value = mock_config
        
        args = Namespace(
            model="gpt_4o",  # This should override the file config
            max_tokens=500000,  # This should override the file config
            include_tests=False,
            include_private=False,
            verbose=False,
            quiet=False,
            debug=False,
            timeout=90,
            config="test_config.json",
            api_key=None
        )
        
        config = load_configuration(args)
        
        mock_load_config.assert_called_once_with("test_config.json")
        # Command line args should override file config
        assert config.model_name == "gpt_4o"
        assert config.max_tokens == 500000
    
    @patch('cli.load_config')  # FIXED: Changed from 'src.cli.load_config'
    def test_load_configuration_file_error(self, mock_load_config, caplog):
        """Test handling of configuration file loading errors."""
        mock_load_config.side_effect = Exception("File error")
        
        args = Namespace(
            model="gemini_2_5_pro",
            max_tokens=1000000,
            include_tests=False,
            include_private=False,
            verbose=False,
            quiet=False,
            debug=False,
            timeout=90,
            config="bad_config.json",
            api_key=None
        )
        
        # Should not raise, but should log warning
        config = load_configuration(args)
        assert isinstance(config, Config)
        assert "Failed to load configuration" in caplog.text

class TestAPIValidation:
    """Test cases for API requirements validation."""
    
    def test_validate_api_requirements_with_key(self):
        """Test API validation when key is provided."""
        api_key = validate_api_requirements("gemini_2_5_pro", "test-key")
        assert api_key == "test-key"
    
    @patch.dict('os.environ', {'GEMINI_API_KEY': 'env-key'})
    def test_validate_api_requirements_from_env(self):
        """Test API validation gets key from environment."""
        api_key = validate_api_requirements("gemini_2_5_pro", None)
        assert api_key == "env-key"
    
    def test_validate_api_requirements_missing_key(self):
        """Test API validation fails when no key is available."""
        with pytest.raises(CLIError) as exc_info:
            validate_api_requirements("gemini_2_5_pro", None)
        assert "API key required" in str(exc_info.value)
    
    @patch('cli.genai', None)  # FIXED: Changed from 'src.cli.genai'
    def test_validate_api_requirements_missing_package(self):
        """Test API validation fails when required package is missing."""
        with pytest.raises(CLIError) as exc_info:
            validate_api_requirements("gemini_2_5_pro", "test-key")
        assert "google-generativeai package required" in str(exc_info.value)

class TestInitConfig:
    """Test cases for init-config command."""
    
    @patch('cli.save_config')  # FIXED: Changed from 'src.cli.save_config'
    def test_init_config_command_success(self, mock_save_config, capsys):
        """Test successful config initialization."""
        args = Namespace()
        
        init_config_command(args)
        
        # Check that save_config was called
        mock_save_config.assert_called_once()
        call_args = mock_save_config.call_args
        config_arg = call_args[0][0]
        
        assert isinstance(config_arg, Config)
        assert config_arg.model_name == "gemini_2_5_pro"
        assert config_arg.max_tokens == 1_000_000
        assert config_arg.timeout == 90
        
        # Check output
        captured = capsys.readouterr()
        assert "Configuration file created" in captured.out
    
    @patch('cli.save_config')  # FIXED: Changed from 'src.cli.save_config'
    def test_init_config_command_failure(self, mock_save_config):
        """Test config initialization failure."""
        mock_save_config.side_effect = Exception("Save failed")
        args = Namespace()
        
        with pytest.raises(CLIError) as exc_info:
            init_config_command(args)
        assert "Failed to create configuration file" in str(exc_info.value)

class TestIntegration:
    """Integration test cases."""
    
    def test_full_argument_flow(self, tmp_path):
        """Test complete argument parsing and validation flow."""
        # Create a mock Python project
        project_dir = tmp_path / "test_project"
        project_dir.mkdir()
        (project_dir / "__init__.py").write_text("")
        (project_dir / "main.py").write_text("print('hello')")
        
        # Parse arguments
        parser = create_parser()
        args = parser.parse_args([
            str(project_dir),
            "--model", "gpt_4o_mini",
            "--include-tests",
            "--verbose"
        ])
        
        # Validate arguments
        validate_arguments(args)  # Should not raise
        
        # Load configuration
        config = load_configuration(args)
        
        assert config.model_name == "gpt_4o_mini"
        assert config.include_tests is True
        assert config.verbose is True


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\test_config.py
import pytest
import json
import tempfile
from pathlib import Path
from unittest.mock import patch

from config import Config, load_config, save_config

class TestConfigDefaults:
    """Test cases for Config default values."""
    
    def test_config_initialization_defaults(self):
        """Test that Config initializes with correct default values."""
        config = Config()
        
        assert config.model_name == "gemini_2_5_pro"
        assert config.max_tokens == 1_000_000
        assert config.api_key is None
        assert config.include_tests is False
        assert config.include_private is False
        assert config.verbose is False
        assert config.quiet is False
        assert config.debug is False
        assert config.timeout == 90
        assert config.cache_enabled is True
        assert config.parse_only is False
    
    @patch.dict('os.environ', {'GEMINI_API_KEY': 'test-env-key'})
    def test_config_loads_api_key_from_env(self):
        """Test that Config loads API key from environment variables."""
        config = Config()
        assert config.api_key == 'test-env-key'
    
    @patch.dict('os.environ', {'OPENAI_API_KEY': 'openai-key'})
    def test_config_loads_different_api_key_for_different_model(self):
        """Test API key loading for different models."""
        config = Config()
        config.model_name = "gpt_4o"
        config._load_api_key_from_env()
        assert config.api_key == 'openai-key'

class TestConfigUpdate:
    """Test cases for Config update functionality."""
    
    def test_update_with_dict(self):
        """Test updating config with dictionary values."""
        config = Config()
        update_data = {
            "model_name": "gpt_4o",
            "max_tokens": 500000,
            "verbose": True,
            "include_tests": True
        }
        
        config.update(update_data)
        
        assert config.model_name == "gpt_4o"
        assert config.max_tokens == 500000
        assert config.verbose is True
        assert config.include_tests is True
        # Other values should remain unchanged
        assert config.timeout == 90
        assert config.debug is False
    
    def test_update_with_another_config(self):
        """Test updating config with another Config object."""
        config1 = Config()
        config2 = Config()
        
        config2.model_name = "claude_sonnet"
        config2.verbose = True
        config2.max_tokens = 750000
        
        config1.update(config2)
        
        assert config1.model_name == "claude_sonnet"
        assert config1.verbose is True
        assert config1.max_tokens == 750000
    
    def test_update_ignores_invalid_attributes(self):
        """Test that update ignores attributes that don't exist on Config."""
        config = Config()
        original_model = config.model_name
        
        update_data = {
            "model_name": "gpt_4o",
            "nonexistent_attr": "should_be_ignored",
            "another_invalid": 12345
        }
        
        config.update(update_data)
        
        assert config.model_name == "gpt_4o"
        assert not hasattr(config, "nonexistent_attr")
        assert not hasattr(config, "another_invalid")

class TestConfigSerialization:
    """Test cases for Config serialization."""
    
    def test_to_dict(self):
        """Test converting Config to dictionary."""
        config = Config()
        config.model_name = "test_model"
        config.verbose = True
        
        config_dict = config.to_dict()
        
        assert isinstance(config_dict, dict)
        assert config_dict["model_name"] == "test_model"
        assert config_dict["verbose"] is True
        assert config_dict["max_tokens"] == 1_000_000
        
        # Should not include private methods or callable attributes
        assert "_load_api_key_from_env" not in config_dict
        assert "update" not in config_dict
        assert "to_dict" not in config_dict

class TestConfigFileOperations:
    """Test cases for Config file loading and saving."""
    
    def test_save_and_load_config(self, tmp_path):
        """Test saving and loading configuration to/from file."""
        config = Config()
        config.model_name = "claude_sonnet"
        config.max_tokens = 500000
        config.verbose = True
        config.include_tests = True
        
        config_file = tmp_path / "test_config.json"
        
        # Save configuration
        save_config(config, str(config_file))
        
        # Check file was created
        assert config_file.exists()
        
        # Load configuration
        loaded_config = load_config(str(config_file))
        
        assert isinstance(loaded_config, Config)
        assert loaded_config.model_name == "claude_sonnet"
        assert loaded_config.max_tokens == 500000
        assert loaded_config.verbose is True
        assert loaded_config.include_tests is True
    
    def test_save_config_creates_directory(self, tmp_path):
        """Test that save_config creates directories if they don't exist."""
        config = Config()
        nested_dir = tmp_path / "nested" / "dir"
        config_file = nested_dir / "config.json"
        
        save_config(config, str(config_file))
        
        assert config_file.exists()
        
        # Verify content is valid JSON
        with open(config_file, 'r') as f:
            data = json.load(f)
        assert data["model_name"] == "gemini_2_5_pro"
    
    def test_load_config_file_not_found(self):
        """Test loading non-existent configuration file."""
        with pytest.raises(FileNotFoundError) as exc_info:
            load_config("/nonexistent/config.json")
        assert "Configuration file not found" in str(exc_info.value)
    
    def test_load_config_invalid_json(self, tmp_path):
        """Test loading configuration file with invalid JSON."""
        config_file = tmp_path / "invalid.json"
        config_file.write_text("{ invalid json content")
        
        with pytest.raises(ValueError) as exc_info:
            load_config(str(config_file))
        assert "Invalid JSON" in str(exc_info.value)
    
    def test_load_config_partial_data(self, tmp_path):
        """Test loading configuration with only partial data."""
        config_file = tmp_path / "partial_config.json"
        partial_data = {
            "model_name": "gpt_4o_mini",
            "verbose": True
            # Missing other fields
        }
        
        with open(config_file, 'w') as f:
            json.dump(partial_data, f)
        
        loaded_config = load_config(str(config_file))
        
        # Should have loaded values
        assert loaded_config.model_name == "gpt_4o_mini"
        assert loaded_config.verbose is True
        
        # Should have defaults for missing values
        assert loaded_config.max_tokens == 1_000_000
        assert loaded_config.include_tests is False
    
    def test_save_config_io_error(self, tmp_path):
        """Test save_config handles IO errors gracefully."""
        config = Config()
        
        # FIXED: Mock the open function instead of mkdir to trigger the right error path
        with patch('builtins.open', side_effect=PermissionError("Access denied")):
            with pytest.raises(IOError) as exc_info:
                save_config(config, str(tmp_path / "config.json"))
            assert "Failed to save configuration" in str(exc_info.value)

class TestConfigIntegration:
    """Integration test cases for Config."""
    
    def test_full_config_workflow(self, tmp_path):
        """Test complete configuration workflow."""
        # Create initial config
        config1 = Config()
        config1.model_name = "claude_sonnet"
        config1.max_tokens = 750000
        config1.verbose = True
        config1.api_key = "test-key"
        
        config_file = tmp_path / "workflow_config.json"
        
        # Save config
        save_config(config1, str(config_file))
        
        # Load config
        config2 = load_config(str(config_file))
        
        # Update loaded config
        config2.model_name = "gpt_4o"
        config2.include_tests = True
        
        # Save updated config
        save_config(config2, str(config_file))
        
        # Load final config
        config3 = load_config(str(config_file))
        
        # Verify all changes persisted
        assert config3.model_name == "gpt_4o"
        assert config3.max_tokens == 750000
        assert config3.verbose is True
        assert config3.include_tests is True
        assert config3.api_key == "test-key"
    
    def test_config_with_environment_override(self, tmp_path):
        """Test config behavior with environment variables."""
        config_file = tmp_path / "env_config.json"
        
        # Save config without API key
        config = Config()
        config.model_name = "gpt_4o"  # FIXED: Changed from "openai_model" to valid model name
        save_config(config, str(config_file))
        
        # Load config with environment variable set
        with patch.dict('os.environ', {'OPENAI_API_KEY': 'env-override-key'}):
            loaded_config = load_config(str(config_file))
            # API key should come from environment during initialization
            assert loaded_config.api_key == 'env-override-key'


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\__init__.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\setup.py
#!/usr/bin/env python3
"""
Setup configuration for sample-project.

This file provides backward compatibility for systems that don't support
PEP 517/518 build systems. Modern installations should prefer pyproject.toml.
"""

from setuptools import setup, find_packages
from pathlib import Path
import re

# Read version from package
def get_version():
    """Extract version from package __init__.py"""
    init_file = Path(__file__).parent / "sample_project" / "__init__.py"
    if init_file.exists():
        with open(init_file, encoding='utf-8') as f:
            content = f.read()
            match = re.search(r'^__version__ = ["\']([^"\']+)["\']', content, re.M)
            if match:
                return match.group(1)
    return "1.2.3"  # fallback version

# Read long description from README
def get_long_description():
    """Read long description from README file"""
    readme_file = Path(__file__).parent / "README.md"
    if readme_file.exists():
        with open(readme_file, encoding='utf-8') as f:
            return f.read()
    return "A comprehensive sample Python project for testing README generation"

# Read requirements from files
def get_requirements(filename):
    """Read requirements from requirements file"""
    req_file = Path(__file__).parent / filename
    if req_file.exists():
        with open(req_file, encoding='utf-8') as f:
            return [
                line.strip() 
                for line in f 
                if line.strip() and not line.startswith('#')
            ]
    return []

setup(
    # Basic package information
    name="sample-project",
    version=get_version(),
    description="A comprehensive sample Python project for testing README generation",
    long_description=get_long_description(),
    long_description_content_type="text/markdown",
    
    # Author information
    author="John Developer",
    author_email="john.dev@example.com",
    maintainer="John Developer",
    maintainer_email="john.dev@example.com",
    
    # URLs
    url="https://github.com/example/sample-project",
    project_urls={
        "Documentation": "https://sample-project.readthedocs.io/",
        "Source Code": "https://github.com/example/sample-project",
        "Bug Reports": "https://github.com/example/sample-project/issues",
        "Funding": "https://github.com/sponsors/example",
        "Changelog": "https://github.com/example/sample-project/blob/main/CHANGELOG.md",
    },
    
    # Package discovery
    packages=find_packages(exclude=["tests*", "docs*", "scripts*"]),
    package_dir={"": "."},
    
    # Package data
    package_data={
        "sample_project": [
            "data/*.json",
            "templates/*.txt", 
            "config/*.yaml",
            "static/*.css",
            "static/*.js"
        ],
    },
    include_package_data=True,
    
    # Dependencies
    install_requires=[
        "requests>=2.25.0",
        "click>=8.0.0", 
        "pydantic>=1.8.0,<3.0.0",
        "rich>=10.0.0",
        "typer>=0.4.0",
        "pyyaml>=6.0",
        "configparser>=5.0.0"
    ],
    
    # Optional dependencies
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-cov>=4.0.0",
            "pytest-mock>=3.10.0",
            "black>=22.0.0",
            "flake8>=4.0.0",
            "mypy>=1.0.0",
            "pre-commit>=2.20.0"
        ],
        "test": [
            "pytest>=7.0.0",
            "pytest-cov>=4.0.0", 
            "pytest-mock>=3.10.0",
            "coverage>=6.0.0"
        ],
        "docs": [
            "sphinx>=4.0.0",
            "sphinx-rtd-theme>=1.0.0",
            "myst-parser>=0.18.0"
        ],
        "async": [
            "aiohttp>=3.8.0",
            "asyncio-mqtt>=0.11.0"
        ],
        "all": [
            # Combines all optional dependencies
            "pytest>=7.0.0", "pytest-cov>=4.0.0", "pytest-mock>=3.10.0",
            "black>=22.0.0", "flake8>=4.0.0", "mypy>=1.0.0", "pre-commit>=2.20.0",
            "coverage>=6.0.0", "sphinx>=4.0.0", "sphinx-rtd-theme>=1.0.0",
            "myst-parser>=0.18.0", "aiohttp>=3.8.0", "asyncio-mqtt>=0.11.0"
        ]
    },
    
    # Entry points
    entry_points={
        "console_scripts": [
            "sample-cli=sample_project.cli:main",
            "sample-tool=sample_project.main:run_tool",
            "sample-project=sample_project.cli:main",
        ],
        "sample_project.plugins": [
            "default=sample_project.plugins:DefaultPlugin",
            "advanced=sample_project.plugins:AdvancedPlugin",
        ],
    },
    
    # Metadata
    keywords=["sample", "example", "demo", "cli", "utility"],
    license="MIT",
    
    # Classifiers
    classifiers=[
        # Development Status
        "Development Status :: 4 - Beta",
        
        # Intended Audience
        "Intended Audience :: Developers",
        "Intended Audience :: End Users/Desktop",
        "Intended Audience :: System Administrators",
        
        # License
        "License :: OSI Approved :: MIT License",
        
        # Operating Systems
        "Operating System :: OS Independent",
        "Operating System :: POSIX",
        "Operating System :: Microsoft :: Windows",
        "Operating System :: MacOS",
        
        # Programming Languages
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Programming Language :: Python :: 3 :: Only",
        
        # Topics
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Software Development :: Tools",
        "Topic :: Utilities",
        "Topic :: System :: Systems Administration",
        "Topic :: Internet :: WWW/HTTP :: Dynamic Content",
        
        # Natural Language
        "Natural Language :: English",
        
        # Environment
        "Environment :: Console",
        "Environment :: Web Environment",
    ],
    
    # Python version requirement
    python_requires=">=3.8",
    
    # Additional options
    zip_safe=False,
    platforms=["any"],
    
    # Test suite
    test_suite="tests",
    tests_require=get_requirements("requirements-test.txt"),
    
    # Command class customizations (if needed)
    # cmdclass={
    #     'test': CustomTestCommand,
    #     'build_ext': CustomBuildExtCommand,
    # },
)


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\docs\examples.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\sample_project\cli.py
"""
Command-line interface for the Sample Project.

This module provides a comprehensive CLI for interacting with the sample project,
including data processing, configuration management, and various utility commands.
"""

import argparse
import asyncio
import json
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import click
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import track

from .main import SampleProcessor, DataProcessor, create_processor, ProcessingError
from .config import Config, load_config, save_config
from .models import DataModel, ValidationError
from .utils import format_output, validate_input

# Initialize rich console for beautiful output
console = Console()

# Typer app for modern CLI
app = typer.Typer(
    name="sample-project",
    help="A comprehensive sample project CLI for data processing and management.",
    add_completion=False,
    rich_markup_mode="rich"
)

# Version information
__version__ = "1.2.3"


class CLIError(Exception):
    """Custom exception for CLI-related errors."""
    pass


@app.command()
def version():
    """Show version information."""
    console.print(f"Sample Project CLI v{__version__}", style="bold green")
    console.print(f"Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}")


@app.command()
def process(
    input_data: str = typer.Argument(..., help="Input data to process"),
    processor_type: str = typer.Option("sample", help="Type of processor to use"),
    config_file: Optional[str] = typer.Option(None, "--config", "-c", help="Configuration file path"),
    output_format: str = typer.Option("json", help="Output format (json, yaml, table)"),
    debug: bool = typer.Option(False, "--debug", help="Enable debug mode"),
    async_mode: bool = typer.Option(False, "--async", help="Use async processing"),
    save_result: Optional[str] = typer.Option(None, "--save", help="Save result to file"),
):
    """
    Process input data using the specified processor.
    
    Examples:
        sample-project process "Hello World" --debug
        sample-project process '{"name": "test", "value": 42}' --processor-type data
        sample-project process "batch data" --save results.json
    """
    try:
        # Load configuration
        if config_file:
            config = load_config(config_file)
            console.print(f"Loaded config from: {config_file}", style="dim")
        else:
            config = Config()
        
        # Override config with CLI options
        if debug:
            config.debug = debug
        
        # Create processor
        processor = create_processor(processor_type, config)
        
        # Process data
        start_time = time.time()
        
        if async_mode:
            result = asyncio.run(processor.process_async(input_data))
        else:
            result = processor.process(input_data)
        
        processing_time = time.time() - start_time
        
        # Format and display output
        if output_format == "table":
            _display_result_table(result, processing_time)
        elif output_format == "yaml":
            import yaml
            output = yaml.dump(result.to_dict(), default_flow_style=False)
            console.print(output)
        else:  # json
            output = json.dumps(result.to_dict(), indent=2)
            console.print_json(output)
        
        # Save result if requested
        if save_result:
            with open(save_result, 'w') as f:
                json.dump(result.to_dict(), f, indent=2)
            console.print(f"Result saved to: {save_result}", style="green")
        
        # Show processing stats if debug
        if debug:
            stats = processor.get_statistics()
            console.print(f"\nProcessing Stats: {stats}", style="dim")
        
    except (ProcessingError, ValidationError) as e:
        console.print(f"Processing Error: {e}", style="bold red")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"Unexpected Error: {e}", style="bold red")
        if debug:
            console.print_exception()
        raise typer.Exit(1)


@app.command()
def batch(
    input_file: str = typer.Argument(..., help="Input file with data to process"),
    processor_type: str = typer.Option("sample", help="Type of processor to use"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Output file for results"),
    config_file: Optional[str] = typer.Option(None, "--config", "-c", help="Configuration file"),
    max_items: int = typer.Option(100, help="Maximum items to process"),
    show_progress: bool = typer.Option(True, help="Show progress bar"),
):
    """
    Process multiple items from a file in batch mode.
    
    Input file should contain JSON lines or a JSON array.
    
    Examples:
        sample-project batch data.jsonl --output results.json
        sample-project batch data.json --max-items 50 --no-show-progress
    """
    try:
        # Load input data
        with open(input_file, 'r') as f:
            if input_file.endswith('.jsonl'):
                data_items = [json.loads(line.strip()) for line in f if line.strip()]
            else:
                content = json.load(f)
                data_items = content if isinstance(content, list) else [content]
        
        console.print(f"Loaded {len(data_items)} items from {input_file}")
        
        # Load configuration
        config = load_config(config_file) if config_file else Config()
        config.max_items = max_items
        
        # Create processor
        processor = create_processor(processor_type, config)
        
        # Process batch
        start_time = time.time()
        
        if show_progress:
            results = []
            for item in track(data_items[:max_items], description="Processing items..."):
                try:
                    result = processor.process(item)
                    results.append(result)
                except Exception as e:
                    console.print(f"Failed to process item: {e}", style="yellow")
                    results.append({"success": False, "error": str(e)})
        else:
            results = processor.process_batch(data_items[:max_items])
        
        processing_time = time.time() - start_time
        
        # Prepare output
        output_data = {
            "batch_info": {
                "total_items": len(results),
                "processing_time": processing_time,
                "processor_type": processor_type,
                "config": config.to_dict()
            },
            "results": [r.to_dict() if hasattr(r, 'to_dict') else r for r in results]
        }
        
        # Save or display results
        if output_file:
            with open(output_file, 'w') as f:
                json.dump(output_data, f, indent=2)
            console.print(f"Batch results saved to: {output_file}", style="green")
        else:
            console.print_json(json.dumps(output_data, indent=2))
        
        # Show summary
        successful = sum(1 for r in results if getattr(r, 'success', True))
        console.print(f"\nBatch Summary: {successful}/{len(results)} successful", style="bold")
        
    except FileNotFoundError:
        console.print(f"Input file not found: {input_file}", style="bold red")
        raise typer.Exit(1)
    except json.JSONDecodeError as e:
        console.print(f"Invalid JSON in input file: {e}", style="bold red")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"Batch processing failed: {e}", style="bold red")
        raise typer.Exit(1)


@app.command()
def config(
    action: str = typer.Argument(..., help="Action: show, create, validate"),
    config_file: Optional[str] = typer.Option("sample_config.json", "--file", "-f", help="Configuration file path"),
    debug: bool = typer.Option(False, help="Debug mode"),
    max_items: int = typer.Option(100, help="Maximum items to process"),
    cache_enabled: bool = typer.Option(True, help="Enable caching"),
):
    """
    Manage configuration files.
    
    Actions:
        show: Display current configuration
        create: Create a new configuration file
        validate: Validate an existing configuration file
    
    Examples:
        sample-project config show --file my_config.json
        sample-project config create --debug --max-items 200
        sample-project config validate --file production.json
    """
    try:
        if action == "show":
            if Path(config_file).exists():
                config = load_config(config_file)
                console.print(f"Configuration from {config_file}:")
                console.print_json(json.dumps(config.to_dict(), indent=2))
            else:
                console.print(f"Configuration file not found: {config_file}", style="yellow")
                console.print("Using default configuration:")
                config = Config()
                console.print_json(json.dumps(config.to_dict(), indent=2))
        
        elif action == "create":
            config = Config(
                debug=debug,
                max_items=max_items,
                cache_enabled=cache_enabled
            )
            save_config(config, config_file)
            console.print(f"Configuration created: {config_file}", style="green")
            console.print_json(json.dumps(config.to_dict(), indent=2))
        
        elif action == "validate":
            if not Path(config_file).exists():
                console.print(f"Configuration file not found: {config_file}", style="bold red")
                raise typer.Exit(1)
            
            try:
                config = load_config(config_file)
                console.print(f"✓ Configuration is valid: {config_file}", style="green")
                
                # Additional validation checks
                issues = []
                if config.max_items <= 0:
                    issues.append("max_items must be positive")
                if config.max_size <= 0:
                    issues.append("max_size must be positive")
                
                if issues:
                    console.print("⚠ Configuration issues found:", style="yellow")
                    for issue in issues:
                        console.print(f"  - {issue}")
                else:
                    console.print("✓ All validation checks passed", style="green")
                    
            except Exception as e:
                console.print(f"✗ Configuration is invalid: {e}", style="bold red")
                raise typer.Exit(1)
        
        else:
            console.print(f"Unknown action: {action}", style="bold red")
            console.print("Available actions: show, create, validate")
            raise typer.Exit(1)
    
    except Exception as e:
        console.print(f"Configuration command failed: {e}", style="bold red")
        raise typer.Exit(1)


@app.command()
def stats(
    processor_type: str = typer.Option("sample", help="Type of processor"),
    config_file: Optional[str] = typer.Option(None, "--config", "-c", help="Configuration file"),
    reset: bool = typer.Option(False, "--reset", help="Reset statistics"),
):
    """
    Show or reset processor statistics.
    
    Examples:
        sample-project stats --processor-type sample
        sample-project stats --reset
    """
    try:
        config = load_config(config_file) if config_file else Config()
        processor = create_processor(processor_type, config)
        
        if reset:
            processor.reset_counters()
            processor.clear_cache()
            console.print("Statistics reset", style="green")
        else:
            stats = processor.get_statistics()
            
            table = Table(title=f"{processor_type.title()} Processor Statistics")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", style="green")
            
            for key, value in stats.items():
                table.add_row(key.replace('_', ' ').title(), str(value))
            
            console.print(table)
    
    except Exception as e:
        console.print(f"Stats command failed: {e}", style="bold red")
        raise typer.Exit(1)


@app.command()
def interactive():
    """
    Start an interactive session for data processing.
    
    This provides a REPL-like interface for experimenting with the processors.
    """
    console.print("Welcome to Sample Project Interactive Mode!", style="bold green")
    console.print("Type 'help' for available commands, 'exit' to quit.\n")
    
    config = Config(debug=True)
    processor = create_processor("sample", config)
    
    while True:
        try:
            user_input = console.input("[bold blue]sample-project>[/bold blue] ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['exit', 'quit']:
                console.print("Goodbye!", style="green")
                break
            
            if user_input.lower() == 'help':
                _show_interactive_help()
                continue
            
            if user_input.startswith('config '):
                config_cmd = user_input[7:].strip()
                if config_cmd == 'show':
                    console.print_json(json.dumps(config.to_dict(), indent=2))
                continue
            
            if user_input.startswith('stats'):
                stats = processor.get_statistics()
                console.print(f"Statistics: {stats}")
                continue
            
            # Process the input
            try:
                result = processor.process(user_input)
                console.print("Result:", style="bold")
                console.print_json(json.dumps(result.to_dict(), indent=2))
            except Exception as e:
                console.print(f"Processing Error: {e}", style="red")
        
        except KeyboardInterrupt:
            console.print("\nGoodbye!", style="green")
            break
        except EOFError:
            console.print("\nGoodbye!", style="green")
            break


def _show_interactive_help():
    """Show help for interactive mode."""
    help_table = Table(title="Interactive Mode Commands")
    help_table.add_column("Command", style="cyan")
    help_table.add_column("Description", style="white")
    
    commands = [
        ("help", "Show this help message"),
        ("config show", "Show current configuration"),
        ("stats", "Show processor statistics"),
        ("exit/quit", "Exit interactive mode"),
        ("<data>", "Process any data input"),
    ]
    
    for command, description in commands:
        help_table.add_row(command, description)
    
    console.print(help_table)


def _display_result_table(result, processing_time):
    """Display processing result in table format."""
    table = Table(title="Processing Result")
    table.add_column("Field", style="cyan")
    table.add_column("Value", style="green")
    
    table.add_row("Success", str(result.success))
    table.add_row("Processing Time", f"{processing_time:.3f}s")
    
    if result.content:
        content_str = str(result.content)[:100] + "..." if len(str(result.content)) > 100 else str(result.content)
        table.add_row("Content", content_str)
    
    if result.error:
        table.add_row("Error", result.error)
    
    if result.metadata:
        for key, value in result.metadata.items():
            table.add_row(f"Meta: {key}", str(value))
    
    console.print(table)


# Click-based alternative commands (for demonstration)
@click.group()
@click.version_option(version=__version__)
def cli():
    """Alternative Click-based CLI interface."""
    pass


@cli.command()
@click.argument('data')
@click.option('--debug/--no-debug', default=False, help='Enable debug mode')
@click.option('--format', type=click.Choice(['json', 'yaml', 'table']), default='json')
def click_process(data, debug, format):
    """Process data using Click interface."""
    config = Config(debug=debug)
    processor = SampleProcessor(config)
    
    try:
        result = processor.process(data)
        
        if format == 'json':
            click.echo(json.dumps(result.to_dict(), indent=2))
        elif format == 'table':
            click.echo(f"Success: {result.success}")
            click.echo(f"Content: {result.content}")
        
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)


# Entry point for console scripts
def main():
    """Main entry point for the CLI application."""
    try:
        app()
    except KeyboardInterrupt:
        console.print("\nOperation cancelled by user.", style="yellow")
        sys.exit(1)
    except Exception as e:
        console.print(f"Unexpected error: {e}", style="bold red")
        sys.exit(1)


# Alternative entry point for Click
def click_main():
    """Entry point for Click-based CLI."""
    cli()


if __name__ == "__main__":
    main()


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\sample_project\config.py
"""
Configuration management for the Sample Project.

This module provides comprehensive configuration handling including
loading from files, environment variables, and runtime settings.
"""

import json
import os
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, Optional, Union, List
import logging
from enum import Enum

logger = logging.getLogger(__name__)


class LogLevel(Enum):
    """Enumeration of supported log levels."""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class ConfigError(Exception):
    """Raised when configuration operations fail."""
    pass


@dataclass
class Config:
    """
    Main configuration class for the Sample Project.
    
    This class manages all configuration settings including
    processing parameters, system settings, and feature flags.
    
    Attributes:
        debug: Enable debug mode
        max_items: Maximum number of items to process
        max_size: Maximum size limit for data processing
        timeout: Timeout in seconds for operations
        cache_enabled: Whether to enable result caching
        log_level: Logging level
        output_format: Default output format
        
        # Processing options
        uppercase: Convert strings to uppercase during processing
        multiply_factor: Multiplication factor for numeric processing
        
        # Advanced settings
        parallel_processing: Enable parallel processing
        max_workers: Maximum number of worker threads
        retry_attempts: Number of retry attempts for failed operations
        retry_delay: Delay between retries in seconds
        
        # Feature flags
        experimental_features: Enable experimental features
        strict_validation: Enable strict input validation
        
        # File and directory settings
        data_directory: Directory for data files
        output_directory: Directory for output files
        temp_directory: Temporary directory for processing
        
        # External service settings
        api_endpoints: Dictionary of external API endpoints
        api_timeouts: Timeout settings for external APIs
        api_keys: API keys for external services
        
    Example:
        >>> config = Config(debug=True, max_items=50)
        >>> config.debug
        True
        >>> config.is_debug_mode()
        True
    """
    
    # Core settings
    debug: bool = False
    max_items: int = 100
    max_size: int = 10000
    timeout: int = 30
    cache_enabled: bool = True
    log_level: LogLevel = LogLevel.INFO
    output_format: str = "json"
    
    # Processing options
    uppercase: bool = False
    multiply_factor: Optional[float] = None
    
    # Advanced settings
    parallel_processing: bool = False
    max_workers: int = 4
    retry_attempts: int = 3
    retry_delay: float = 1.0
    
    # Feature flags
    experimental_features: bool = False
    strict_validation: bool = True
    
    # File and directory settings
    data_directory: str = "data"
    output_directory: str = "output"
    temp_directory: str = "temp"
    
    # External service settings
    api_endpoints: Dict[str, str] = field(default_factory=dict)
    api_timeouts: Dict[str, int] = field(default_factory=dict)
    api_keys: Dict[str, str] = field(default_factory=dict)
    
    # Custom settings for extensibility
    custom_settings: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Initialize configuration after instance creation."""
        # Load from environment variables
        self._load_from_environment()
        
        # Set up default API endpoints
        if not self.api_endpoints:
            self.api_endpoints = {
                "test_api": "https://api.example.com/v1",
                "backup_api": "https://backup.example.com/v1",
            }
        
        # Set up default API timeouts
        if not self.api_timeouts:
            self.api_timeouts = {
                "test_api": 30,
                "backup_api": 60,
            }
        
        # Validate configuration
        self.validate()
    
    def validate(self) -> None:
        """
        Validate configuration settings.
        
        Raises:
            ConfigError: If configuration is invalid
        """
        if self.max_items <= 0:
            raise ConfigError("max_items must be positive")
        
        if self.max_size <= 0:
            raise ConfigError("max_size must be positive")
        
        if self.timeout <= 0:
            raise ConfigError("timeout must be positive")
        
        if self.max_workers <= 0:
            raise ConfigError("max_workers must be positive")
        
        if self.retry_attempts < 0:
            raise ConfigError("retry_attempts cannot be negative")
        
        if self.retry_delay < 0:
            raise ConfigError("retry_delay cannot be negative")
        
        # Validate directories exist or can be created
        for dir_attr in ['data_directory', 'output_directory', 'temp_directory']:
            dir_path = Path(getattr(self, dir_attr))
            try:
                dir_path.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.warning(f"Cannot create directory {dir_path}: {e}")
    
    def _load_from_environment(self) -> None:
        """Load configuration values from environment variables."""
        env_mappings = {
            'SAMPLE_DEBUG': ('debug', lambda x: x.lower() == 'true'),
            'SAMPLE_MAX_ITEMS': ('max_items', int),
            'SAMPLE_MAX_SIZE': ('max_size', int),
            'SAMPLE_TIMEOUT': ('timeout', int),
            'SAMPLE_CACHE_ENABLED': ('cache_enabled', lambda x: x.lower() == 'true'),
            'SAMPLE_LOG_LEVEL': ('log_level', lambda x: LogLevel(x.upper())),
            'SAMPLE_OUTPUT_FORMAT': ('output_format', str),
            'SAMPLE_UPPERCASE': ('uppercase', lambda x: x.lower() == 'true'),
            'SAMPLE_MULTIPLY_FACTOR': ('multiply_factor', float),
            'SAMPLE_PARALLEL_PROCESSING': ('parallel_processing', lambda x: x.lower() == 'true'),
            'SAMPLE_MAX_WORKERS': ('max_workers', int),
            'SAMPLE_RETRY_ATTEMPTS': ('retry_attempts', int),
            'SAMPLE_RETRY_DELAY': ('retry_delay', float),
            'SAMPLE_EXPERIMENTAL': ('experimental_features', lambda x: x.lower() == 'true'),
            'SAMPLE_STRICT_VALIDATION': ('strict_validation', lambda x: x.lower() == 'true'),
            'SAMPLE_DATA_DIR': ('data_directory', str),
            'SAMPLE_OUTPUT_DIR': ('output_directory', str),
            'SAMPLE_TEMP_DIR': ('temp_directory', str),
        }
        
        for env_var, (attr_name, converter) in env_mappings.items():
            env_value = os.getenv(env_var)
            if env_value is not None:
                try:
                    converted_value = converter(env_value)
                    setattr(self, attr_name, converted_value)
                    logger.debug(f"Loaded {attr_name} from environment: {converted_value}")
                except (ValueError, TypeError) as e:
                    logger.warning(f"Failed to convert environment variable {env_var}: {e}")
        
        # Load API keys from environment
        api_key_prefixes = ['SAMPLE_API_KEY_', 'API_KEY_']
        for prefix in api_key_prefixes:
            for key, value in os.environ.items():
                if key.startswith(prefix):
                    service_name = key[len(prefix):].lower()
                    self.api_keys[service_name] = value
                    logger.debug(f"Loaded API key for service: {service_name}")
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert configuration to dictionary representation.
        
        Returns:
            Dictionary containing all configuration values
        """
        config_dict = asdict(self)
        
        # Convert LogLevel enum to string
        if isinstance(config_dict.get('log_level'), LogLevel):
            config_dict['log_level'] = self.log_level.value
        
        return config_dict


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\sample_project\main.py
"""
Core business logic and main processing classes.

This module contains the primary functionality of the sample project,
including data processors, business logic, and core algorithms.
"""

import asyncio
import logging
import time
from typing import Any, Dict, List, Optional, Union, Callable
from pathlib import Path
from dataclasses import dataclass
from abc import ABC, abstractmethod

from .config import Config, DEFAULT_CONFIG
from .models import DataModel, ResultModel, ValidationError
from .utils import validate_input, timing_decorator, format_output

logger = logging.getLogger(__name__)


class ProcessingError(Exception):
    """Raised when data processing fails."""
    
    def __init__(self, message: str, error_code: int = 500, details: Optional[Dict] = None):
        super().__init__(message)
        self.error_code = error_code
        self.details = details or {}
        self.timestamp = time.time()


class BaseProcessor(ABC):
    """
    Abstract base class for all data processors.
    
    This class defines the interface that all processors must implement
    and provides common functionality for error handling and logging.
    """
    
    def __init__(self, config: Optional[Config] = None):
        """
        Initialize the processor with configuration.
        
        Args:
            config: Configuration object, uses defaults if None
        """
        self.config = config or Config()
        self.logger = logging.getLogger(f"{self.__class__.__module__}.{self.__class__.__name__}")
        self._setup_processor()
    
    def _setup_processor(self) -> None:
        """Set up processor-specific configuration."""
        if self.config.debug:
            self.logger.setLevel(logging.DEBUG)
            self.logger.debug(f"Initialized {self.__class__.__name__} in debug mode")
    
    @abstractmethod
    def process(self, data: Any) -> Any:
        """
        Process input data and return results.
        
        Args:
            data: Input data to process
            
        Returns:
            Processed data
            
        Raises:
            ProcessingError: If processing fails
        """
        pass
    
    @abstractmethod
    def validate_input(self, data: Any) -> bool:
        """
        Validate input data format and content.
        
        Args:
            data: Data to validate
            
        Returns:
            True if valid, False otherwise
        """
        pass
    
    def get_status(self) -> Dict[str, Any]:
        """Get current processor status."""
        return {
            "class": self.__class__.__name__,
            "config": self.config.to_dict(),
            "debug": self.config.debug,
            "timestamp": time.time()
        }


class SampleProcessor(BaseProcessor):
    """
    Main processor class for handling various data processing tasks.
    
    This processor can handle different types of input data and apply
    various transformations based on configuration settings.
    
    Attributes:
        processed_count: Number of items processed
        error_count: Number of processing errors
        
    Example:
        >>> config = Config(debug=True, max_items=100)
        >>> processor = SampleProcessor(config)
        >>> result = processor.process("Hello World")
        >>> print(result.content)
        Processed: Hello World
    """
    
    def __init__(self, config: Optional[Config] = None):
        super().__init__(config)
        self.processed_count = 0
        self.error_count = 0
        self._cache = {}
        self._processors = self._initialize_processors()
    
    def _initialize_processors(self) -> Dict[str, Callable]:
        """Initialize specialized processors for different data types."""
        return {
            'string': self._process_string,
            'number': self._process_number,
            'list': self._process_list,
            'dict': self._process_dict,
            'model': self._process_data_model,
        }
    
    def process(self, data: Any) -> ResultModel:
        """
        Process input data and return a ResultModel.
        
        Args:
            data: Input data of any supported type
            
        Returns:
            ResultModel containing processed data and metadata
            
        Raises:
            ProcessingError: If processing fails
            ValidationError: If input validation fails
        """
        start_time = time.time()
        
        try:
            # Validate input
            if not self.validate_input(data):
                raise ValidationError(f"Invalid input data: {type(data)}")
            
            # Check cache if enabled
            if self.config.cache_enabled:
                cache_key = self._generate_cache_key(data)
                if cache_key in self._cache:
                    self.logger.debug(f"Returning cached result for key: {cache_key}")
                    return self._cache[cache_key]
            
            # Determine data type and process
            data_type = self._determine_data_type(data)
            processor_func = self._processors.get(data_type, self._process_generic)
            
            processed_data = processor_func(data)
            
            # Create result model
            result = ResultModel(
                success=True,
                content=processed_data,
                metadata={
                    'input_type': data_type,
                    'processor': self.__class__.__name__,
                    'processing_time': time.time() - start_time,
                    'config': self.config.to_dict()
                }
            )
            
            # Cache result if enabled
            if self.config.cache_enabled:
                self._cache[cache_key] = result
            
            self.processed_count += 1
            return result
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Processing failed: {e}")
            raise ProcessingError(f"Failed to process data: {e}")
    
    def process_data_model(self, model: DataModel) -> ResultModel:
        """
        Process a DataModel instance with specialized handling.
        
        Args:
            model: DataModel instance to process
            
        Returns:
            ResultModel with processed model data
        """
        return self.process(model)
    
    async def process_async(self, data: Any) -> ResultModel:
        """
        Asynchronously process data.
        
        Args:
            data: Input data to process
            
        Returns:
            ResultModel containing processed data
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.process, data)
    
    def process_batch(self, data_list: List[Any]) -> List[ResultModel]:
        """
        Process multiple items in batch.
        
        Args:
            data_list: List of data items to process
            
        Returns:
            List of ResultModel instances
        """
        results = []
        max_items = self.config.max_items
        
        for i, data in enumerate(data_list[:max_items]):
            try:
                result = self.process(data)
                results.append(result)
            except Exception as e:
                self.logger.warning(f"Failed to process item {i}: {e}")
                # Add failed result
                results.append(ResultModel(
                    success=False,
                    error=str(e),
                    metadata={'item_index': i}
                ))
        
        return results
    
    def validate_input(self, data: Any) -> bool:
        """Validate input data."""
        if data is None:
            return False
        
        # Check size limits for collections
        if isinstance(data, (list, dict, str)):
            if len(data) > self.config.max_size:
                return False
        
        return True
    
    def _determine_data_type(self, data: Any) -> str:
        """Determine the type category of input data."""
        if isinstance(data, str):
            return 'string'
        elif isinstance(data, (int, float)):
            return 'number'
        elif isinstance(data, list):
            return 'list'
        elif isinstance(data, dict):
            return 'dict'
        elif isinstance(data, DataModel):
            return 'model'
        else:
            return 'generic'
    
    def _process_string(self, data: str) -> str:
        """Process string data."""
        if self.config.uppercase:
            data = data.upper()
        return f"Processed: {data}"
    
    def _process_number(self, data: Union[int, float]) -> Union[int, float]:
        """Process numeric data."""
        if self.config.multiply_factor:
            data *= self.config.multiply_factor
        return data
    
    def _process_list(self, data: List[Any]) -> List[Any]:
        """Process list data."""
        processed = []
        for item in data[:self.config.max_items]:
            if isinstance(item, str):
                processed.append(self._process_string(item))
            else:
                processed.append(item)
        return processed
    
    def _process_dict(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process dictionary data."""
        processed = {}
        for key, value in data.items():
            if isinstance(value, str):
                processed[key] = self._process_string(value)
            else:
                processed[key] = value
        return processed
    
    def _process_data_model(self, data: DataModel) -> Dict[str, Any]:
        """Process DataModel instances."""
        return {
            'name': self._process_string(data.name),
            'value': self._process_number(data.value),
            'metadata': data.metadata,
            'processed_timestamp': time.time()
        }
    
    def _process_generic(self, data: Any) -> str:
        """Generic processor for unknown data types."""
        return f"Generic processing: {str(data)}"
    
    def _generate_cache_key(self, data: Any) -> str:
        """Generate a cache key for input data."""
        return f"{type(data).__name__}_{hash(str(data))}"
    
    @timing_decorator
    def get_statistics(self) -> Dict[str, Any]:
        """Get processing statistics."""
        return {
            'processed_count': self.processed_count,
            'error_count': self.error_count,
            'cache_size': len(self._cache),
            'success_rate': (
                self.processed_count / (self.processed_count + self.error_count)
                if (self.processed_count + self.error_count) > 0 else 0.0
            )
        }
    
    def clear_cache(self) -> None:
        """Clear the processing cache."""
        self._cache.clear()
        self.logger.info("Processing cache cleared")
    
    def reset_counters(self) -> None:
        """Reset processing counters."""
        self.processed_count = 0
        self.error_count = 0
        self.logger.info("Processing counters reset")


class DataProcessor(BaseProcessor):
    """
    Specialized processor for structured data operations.
    
    This processor focuses on transforming structured data formats
    and provides utilities for data validation and conversion.
    """
    
    def __init__(self, config: Optional[Config] = None):
        super().__init__(config)
        self.supported_formats = ['json', 'yaml', 'xml', 'csv']
    
    def process(self, data: Any) -> Dict[str, Any]:
        """Process structured data."""
        if not self.validate_input(data):
            raise ValidationError("Invalid structured data")
        
        return {
            'original': data,
            'processed': self._transform_data(data),
            'format': self._detect_format(data),
            'timestamp': time.time()
        }
    
    def validate_input(self, data: Any) -> bool:
        """Validate structured data input."""
        return isinstance(data, (dict, list)) and len(str(data)) <= self.config.max_size
    
    def _transform_data(self, data: Any) -> Any:
        """Apply data transformations."""
        if isinstance(data, dict):
            return {k.upper() if isinstance(k, str) else k: v for k, v in data.items()}
        elif isinstance(data, list):
            return [item.upper() if isinstance(item, str) else item for item in data]
        return data
    
    def _detect_format(self, data: Any) -> str:
        """Detect the format of structured data."""
        if isinstance(data, dict):
            return 'json-like'
        elif isinstance(data, list):
            return 'array-like'
        return 'unknown'


# Factory function for creating processors
def create_processor(processor_type: str = "sample", config: Optional[Config] = None) -> BaseProcessor:
    """
    Factory function to create different types of processors.
    
    Args:
        processor_type: Type of processor to create ("sample", "data")
        config: Configuration for the processor
        
    Returns:
        Processor instance
        
    Raises:
        ValueError: If processor_type is unknown
        
    Example:
        >>> processor = create_processor("sample", Config(debug=True))
        >>> isinstance(processor, SampleProcessor)
        True
    """
    processors = {
        'sample': SampleProcessor,
        'data': DataProcessor,
    }
    
    if processor_type not in processors:
        raise ValueError(f"Unknown processor type: {processor_type}")
    
    return processors[processor_type](config)


# Convenience function for quick processing
def quick_process(data: Any, processor_type: str = "sample", **config_kwargs) -> ResultModel:
    """
    Quickly process data with default configuration.
    
    Args:
        data: Data to process
        processor_type: Type of processor to use
        **config_kwargs: Configuration options
        
    Returns:
        Processing result
    """
    config = Config(**config_kwargs)
    processor = create_processor(processor_type, config)
    return processor.process(data)


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\sample_project\models.py
"""
Data models and schemas for the Sample Project.

This module defines the core data structures, validation logic,
and serialization methods used throughout the application.
"""

import json
import time
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Optional, Union, Type
from enum import Enum, auto
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class ValidationError(Exception):
    """Raised when data validation fails."""
    
    def __init__(self, message: str, field: Optional[str] = None, value: Any = None):
        super().__init__(message)
        self.field = field
        self.value = value
        self.timestamp = time.time()


class ProcessingStatus(Enum):
    """Enumeration of processing status values."""
    PENDING = auto()
    IN_PROGRESS = auto()
    COMPLETED = auto()
    FAILED = auto()
    CANCELLED = auto()


class Priority(Enum):
    """Task priority levels."""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class BaseModel:
    """
    Base class for all data models with common functionality.
    
    Provides validation, serialization, and utility methods
    that are inherited by all specific model classes.
    """
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    
    def validate(self) -> bool:
        """
        Validate the model instance.
        
        Returns:
            True if validation passes
            
        Raises:
            ValidationError: If validation fails
        """
        # Update the updated_at timestamp
        self.updated_at = datetime.now()
        return True
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert model to dictionary representation.
        
        Returns:
            Dictionary representation of the model
        """
        return asdict(self)
    
    def to_json(self, indent: int = 2) -> str:
        """
        Convert model to JSON string.
        
        Args:
            indent: JSON indentation level
            
        Returns:
            JSON string representation
        """
        return json.dumps(self.to_dict(), indent=indent, default=str)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'BaseModel':
        """
        Create model instance from dictionary.
        
        Args:
            data: Dictionary containing model data
            
        Returns:
            New model instance
        """
        # Filter only fields that exist in the dataclass
        valid_fields = {f.name for f in cls.__dataclass_fields__}
        filtered_data = {k: v for k, v in data.items() if k in valid_fields}
        return cls(**filtered_data)
    
    @classmethod
    def from_json(cls, json_str: str) -> 'BaseModel':
        """
        Create model instance from JSON string.
        
        Args:
            json_str: JSON string containing model data
            
        Returns:
            New model instance
        """
        try:
            data = json.loads(json_str)
            return cls.from_dict(data)
        except json.JSONDecodeError as e:
            raise ValidationError(f"Invalid JSON: {e}")
    
    def update(self, **kwargs) -> None:
        """
        Update model fields with new values.
        
        Args:
            **kwargs: Field values to update
        """
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
        self.updated_at = datetime.now()
    
    def copy(self) -> 'BaseModel':
        """Create a copy of the model instance."""
        return self.__class__.from_dict(self.to_dict())


@dataclass
class DataModel(BaseModel):
    """
    Primary data model for representing business entities.
    
    This model represents the core data structure used throughout
    the application for processing and storage.
    
    Attributes:
        name: Entity name (required)
        value: Numeric value associated with the entity
        description: Optional description text
        tags: List of tags for categorization
        metadata: Additional metadata as key-value pairs
        is_active: Whether the entity is currently active
        priority: Priority level for processing
        
    Example:
        >>> model = DataModel(
        ...     name="test_entity",
        ...     value=42,
        ...     description="A test entity",
        ...     tags=["test", "example"]
        ... )
        >>> model.validate()
        True
        >>> model.name
        'test_entity'
    """
    name: str
    value: Union[int, float] = 0
    description: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    is_active: bool = True
    priority: Priority = Priority.NORMAL
    
    def validate(self) -> bool:
        """
        Validate DataModel instance.
        
        Returns:
            True if validation passes
            
        Raises:
            ValidationError: If validation fails
        """
        super().validate()
        
        # Name validation
        if not self.name or not isinstance(self.name, str):
            raise ValidationError("Name is required and must be a string", "name", self.name)
        
        if len(self.name.strip()) == 0:
            raise ValidationError("Name cannot be empty", "name", self.name)
        
        if len(self.name) > 100:
            raise ValidationError("Name must be 100 characters or less", "name", self.name)
        
        # Value validation
        if not isinstance(self.value, (int, float)):
            raise ValidationError("Value must be numeric", "value", self.value)
        
        # Tags validation
        if not isinstance(self.tags, list):
            raise ValidationError("Tags must be a list", "tags", self.tags)
        
        for tag in self.tags:
            if not isinstance(tag, str):
                raise ValidationError("All tags must be strings", "tags", tag)
        
        # Metadata validation
        if not isinstance(self.metadata, dict):
            raise ValidationError("Metadata must be a dictionary", "metadata", self.metadata)
        
        return True
    
    def add_tag(self, tag: str) -> None:
        """Add a tag to the model."""
        if isinstance(tag, str) and tag not in self.tags:
            self.tags.append(tag)
            self.update()
    
    def remove_tag(self, tag: str) -> None:
        """Remove a tag from the model."""
        if tag in self.tags:
            self.tags.remove(tag)
            self.update()
    
    def has_tag(self, tag: str) -> bool:
        """Check if model has a specific tag."""
        return tag in self.tags
    
    def set_metadata(self, key: str, value: Any) -> None:
        """Set a metadata key-value pair."""
        self.metadata[key] = value
        self.update()
    
    def get_metadata(self, key: str, default: Any = None) -> Any:
        """Get a metadata value by key."""
        return self.metadata.get(key, default)
    
    def calculate_score(self) -> float:
        """
        Calculate a composite score based on model attributes.
        
        Returns:
            Calculated score as float
        """
        base_score = float(self.value)
        
        # Priority multiplier
        priority_multiplier = {
            Priority.LOW: 0.8,
            Priority.NORMAL: 1.0,
            Priority.HIGH: 1.2,
            Priority.CRITICAL: 1.5
        }
        
        score = base_score * priority_multiplier.get(self.priority, 1.0)
        
        # Tag bonus
        tag_bonus = len(self.tags) * 0.1
        score += tag_bonus
        
        # Active bonus
        if self.is_active:
            score *= 1.1
        
        return round(score, 2)


@dataclass
class ResultModel(BaseModel):
    """
    Model for representing processing results.
    
    This model encapsulates the results of data processing operations,
    including success status, content, and metadata.
    
    Attributes:
        success: Whether the operation was successful
        content: The processed content/data
        error: Error message if operation failed
        metadata: Additional metadata about the operation
        status: Processing status
        execution_time: Time taken for processing (seconds)
        
    Example:
        >>> result = ResultModel(
        ...     success=True,
        ...     content="Processed data",
        ...     metadata={"processor": "SampleProcessor"}
        ... )
        >>> result.success
        True
    """
    success: bool
    content: Any = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    status: ProcessingStatus = ProcessingStatus.COMPLETED
    execution_time: Optional[float] = None
    
    def validate(self) -> bool:
        """Validate ResultModel instance."""
        super().validate()
        
        # If not successful, should have error message
        if not self.success and not self.error:
            raise ValidationError("Failed results must include an error message")
        
        # If successful, should have content (unless explicitly None)
        if self.success and self.content is None and self.status == ProcessingStatus.COMPLETED:
            logger.warning("Successful result has None content")
        
        return True
    
    def is_success(self) -> bool:
        """Check if the result represents a successful operation."""
        return self.success and self.status != ProcessingStatus.FAILED
    
    def get_content_summary(self, max_length: int = 100) -> str:
        """
        Get a summary of the content.
        
        Args:
            max_length: Maximum length of summary
            
        Returns:
            Content summary string
        """
        if self.content is None:
            return "No content"
        
        content_str = str(self.content)
        if len(content_str) <= max_length:
            return content_str
        
        return content_str[:max_length] + "..."
    
    def add_metadata(self, key: str, value: Any) -> None:
        """Add metadata key-value pair."""
        self.metadata[key] = value
        self.update()


@dataclass
class ConfigModel(BaseModel):
    """
    Model for configuration settings.
    
    This model represents configuration options that can be
    persisted and loaded for application settings.
    
    Attributes:
        debug: Debug mode flag
        max_items: Maximum number of items to process
        max_size: Maximum size limit for data
        timeout: Timeout in seconds
        cache_enabled: Whether caching is enabled
        log_level: Logging level
        custom_settings: Additional custom settings
        
    Example:
        >>> config = ConfigModel(debug=True, max_items=50)
        >>> config.debug
        True
        >>> config.get_log_level()
        'INFO'
    """
    debug: bool = False
    max_items: int = 100
    max_size: int = 10000
    timeout: int = 30
    cache_enabled: bool = True
    log_level: str = "INFO"
    custom_settings: Dict[str, Any] = field(default_factory=dict)
    
    def validate(self) -> bool:
        """Validate ConfigModel instance."""
        super().validate()
        
        if self.max_items <= 0:
            raise ValidationError("max_items must be positive", "max_items", self.max_items)
        
        if self.max_size <= 0:
            raise ValidationError("max_size must be positive", "max_size", self.max_size)
        
        if self.timeout <= 0:
            raise ValidationError("timeout must be positive", "timeout", self.timeout)
        
        valid_log_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        if self.log_level.upper() not in valid_log_levels:
            raise ValidationError(
                f"log_level must be one of {valid_log_levels}", 
                "log_level", 
                self.log_level
            )
        
        return True
    
    def get_log_level(self) -> str:
        """Get the log level in uppercase."""
        return self.log_level.upper()
    
    def is_debug_mode(self) -> bool:
        """Check if debug mode is enabled."""
        return self.debug or self.get_log_level() == "DEBUG"
    
    def set_custom_setting(self, key: str, value: Any) -> None:
        """Set a custom configuration setting."""
        self.custom_settings[key] = value
        self.update()
    
    def get_custom_setting(self, key: str, default: Any = None) -> Any:
        """Get a custom configuration setting."""
        return self.custom_settings.get(key, default)


@dataclass
class TaskModel(BaseModel):
    """
    Model for representing processing tasks.
    
    This model represents individual tasks that can be queued,
    processed, and tracked through their lifecycle.
    
    Attributes:
        id: Unique task identifier
        name: Task name/description
        data: Task input data
        status: Current task status
        priority: Task priority level
        result: Task processing result
        error_count: Number of processing errors
        max_retries: Maximum retry attempts
        
    Example:
        >>> task = TaskModel(
        ...     id="task_123",
        ...     name="Process data",
        ...     data={"key": "value"}
        ... )
        >>> task.can_retry()
        True
    """
    id: str
    name: str
    data: Any = None
    status: ProcessingStatus = ProcessingStatus.PENDING
    priority: Priority = Priority.NORMAL
    result: Optional[ResultModel] = None
    error_count: int = 0
    max_retries: int = 3
    
    def validate(self) -> bool:
        """Validate TaskModel instance."""
        super().validate()
        
        if not self.id or not isinstance(self.id, str):
            raise ValidationError("Task ID is required", "id", self.id)
        
        if not self.name or not isinstance(self.name, str):
            raise ValidationError("Task name is required", "name", self.name)
        
        if self.error_count < 0:
            raise ValidationError("Error count cannot be negative", "error_count", self.error_count)
        
        if self.max_retries < 0:
            raise ValidationError("Max retries cannot be negative", "max_retries", self.max_retries)
        
        return True
    
    def can_retry(self) -> bool:
        """Check if the task can be retried."""
        return (
            self.status == ProcessingStatus.FAILED and 
            self.error_count < self.max_retries
        )
    
    def mark_in_progress(self) -> None:
        """Mark task as in progress."""
        self.status = ProcessingStatus.IN_PROGRESS
        self.update()
    
    def mark_completed(self, result: ResultModel) -> None:
        """Mark task as completed with result."""
        self.status = ProcessingStatus.COMPLETED
        self.result = result
        self.update()
    
    def mark_failed(self, error_message: str) -> None:
        """Mark task as failed with error."""
        self.status = ProcessingStatus.FAILED
        self.error_count += 1
        if not self.result:
            self.result = ResultModel(success=False, error=error_message)
        else:
            self.result.error = error_message
        self.update()
    
    def get_priority_score(self) -> int:
        """Get numeric priority score for sorting."""
        return self.priority.value


# Model registry for dynamic model creation
MODEL_REGISTRY: Dict[str, Type[BaseModel]] = {
    'data': DataModel,
    'result': ResultModel,
    'config': ConfigModel,
    'task': TaskModel,
}


def create_model(model_type: str, **kwargs) -> BaseModel:
    """
    Factory function to create model instances by type.
    
    Args:
        model_type: Type of model to create
        **kwargs: Model initialization parameters
        
    Returns:
        New model instance
        
    Raises:
        ValueError: If model_type is unknown
        
    Example:
        >>> model = create_model('data', name='test', value=42)
        >>> isinstance(model, DataModel)
        True
    """
    if model_type not in MODEL_REGISTRY:
        raise ValueError(f"Unknown model type: {model_type}")
    
    model_class = MODEL_REGISTRY[model_type]
    return model_class(**kwargs)


def validate_models(models: List[BaseModel]) -> List[ValidationError]:
    """
    Validate multiple models and return any validation errors.
    
    Args:
        models: List of models to validate
        
    Returns:
        List of validation errors (empty if all valid)
    """
    errors = []
    
    for i, model in enumerate(models):
        try:
            model.validate()
        except ValidationError as e:
            e.model_index = i
            e.model_type = type(model).__name__
            errors.append(e)
    
    return errors


def serialize_models(models: List[BaseModel]) -> List[Dict[str, Any]]:
    """
    Serialize a list of models to dictionaries.
    
    Args:
        models: List of models to serialize
        
    Returns:
        List of model dictionaries
    """
    return [model.to_dict() for model in models]


# Utility functions for model manipulation
def filter_models_by_status(models: List[TaskModel], status: ProcessingStatus) -> List[TaskModel]:
    """Filter task models by status."""
    return [model for model in models if model.status == status]


def sort_models_by_priority(models: List[TaskModel], descending: bool = True) -> List[TaskModel]:
    """Sort task models by priority."""
    return sorted(models, key=lambda x: x.get_priority_score(), reverse=descending)


def find_model_by_id(models: List[TaskModel], task_id: str) -> Optional[TaskModel]:
    """Find a task model by ID."""
    return next((model for model in models if model.id == task_id), None)


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\sample_project\utils.py
"""
Utility functions and decorators for the Sample Project.

This module provides common utilities, helper functions, decorators,
and validation tools used throughout the project.
"""

import functools
import json
import re
import time
from typing import Any, Callable, Dict, List, Optional, Union
from pathlib import Path
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# Constants
EMAIL_PATTERN = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')
URL_PATTERN = re.compile(r'^https?://(?:[-\w.])+(?:[:\d]+)?(?:/(?:[\w/_.])*)?(?:\?(?:[\w&=%.])*)?(?:#(?:\w*))?$')
PHONE_PATTERN = re.compile(r'^\+?1?-?\s?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})$')

# Type aliases
ValidatorFunction = Callable[[Any], bool]
FormatterFunction = Callable[[Any], str]


def timing_decorator(func: Callable) -> Callable:
    """
    Decorator to measure and log function execution time.
    
    Args:
        func: Function to be timed
        
    Returns:
        Wrapped function that logs execution time
        
    Example:
        >>> @timing_decorator
        ... def slow_function():
        ...     time.sleep(0.1)
        ...     return "done"
        >>> result = slow_function()  # Logs execution time
        >>> result
        'done'
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            logger.debug(f"{func.__name__} executed in {execution_time:.4f} seconds")
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"{func.__name__} failed after {execution_time:.4f} seconds: {e}")
            raise
    return wrapper


def retry_decorator(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """
    Decorator for retrying function calls with exponential backoff.
    
    Args:
        max_attempts: Maximum number of retry attempts
        delay: Initial delay between retries in seconds
        backoff: Multiplier for delay after each attempt
        
    Returns:
        Decorator function
        
    Example:
        >>> @retry_decorator(max_attempts=3, delay=0.1)
        ... def flaky_function():
        ...     import random
        ...     if random.random() < 0.7:
        ...         raise ValueError("Random failure")
        ...     return "success"
        >>> result = flaky_function()  # Will retry up to 3 times
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            current_delay = delay
            last_exception = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt == max_attempts - 1:  # Last attempt
                        break
                    
                    logger.warning(f"{func.__name__} attempt {attempt + 1} failed: {e}. Retrying in {current_delay}s")
                    time.sleep(current_delay)
                    current_delay *= backoff
            
            logger.error(f"{func.__name__} failed after {max_attempts} attempts")
            raise last_exception
        return wrapper
    return decorator


def cache_decorator(ttl_seconds: int = 300):
    """
    Simple caching decorator with TTL (Time To Live).
    
    Args:
        ttl_seconds: Cache TTL in seconds
        
    Returns:
        Decorator function
        
    Example:
        >>> @cache_decorator(ttl_seconds=60)
        ... def expensive_function(x):
        ...     time.sleep(1)  # Simulate expensive operation
        ...     return x * 2
        >>> result1 = expensive_function(5)  # Takes ~1 second
        >>> result2 = expensive_function(5)  # Returns immediately from cache
    """
    def decorator(func: Callable) -> Callable:
        cache = {}
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key
            key = str(args) + str(sorted(kwargs.items()))
            current_time = time.time()
            
            # Check if cached result is still valid
            if key in cache:
                result, timestamp = cache[key]
                if current_time - timestamp < ttl_seconds:
                    logger.debug(f"Cache hit for {func.__name__}")
                    return result
                else:
                    del cache[key]  # Remove expired entry
            
            # Call function and cache result
            result = func(*args, **kwargs)
            cache[key] = (result, current_time)
            logger.debug(f"Cache miss for {func.__name__}, result cached")
            return result
        
        # Add cache management methods
        wrapper.clear_cache = lambda: cache.clear()
        wrapper.cache_info = lambda: {"size": len(cache), "ttl": ttl_seconds}
        return wrapper
    return decorator


def validate_input(data: Any, validators: Optional[List[ValidatorFunction]] = None) -> bool:
    """
    Validate input data using a list of validator functions.
    
    Args:
        data: Data to validate
        validators: List of validator functions
        
    Returns:
        True if all validations pass, False otherwise
        
    Example:
        >>> def is_string(x): return isinstance(x, str)
        >>> def min_length(x): return len(x) >= 3
        >>> validate_input("hello", [is_string, min_length])
        True
        >>> validate_input("hi", [is_string, min_length])
        False
    """
    if validators is None:
        return True
    
    for validator in validators:
        try:
            if not validator(data):
                return False
        except Exception as e:
            logger.warning(f"Validator {validator.__name__} raised exception: {e}")
            return False
    
    return True


def validate_email(email: str) -> bool:
    """
    Validate email address format.
    
    Args:
        email: Email address string
        
    Returns:
        True if valid email format
        
    Example:
        >>> validate_email("user@example.com")
        True
        >>> validate_email("invalid-email")
        False
    """
    return bool(EMAIL_PATTERN.match(email)) if isinstance(email, str) else False


def validate_url(url: str) -> bool:
    """
    Validate URL format.
    
    Args:
        url: URL string
        
    Returns:
        True if valid URL format
        
    Example:
        >>> validate_url("https://example.com")
        True
        >>> validate_url("not-a-url")
        False
    """
    return bool(URL_PATTERN.match(url)) if isinstance(url, str) else False


def validate_phone(phone: str) -> bool:
    """
    Validate US phone number format.
    
    Args:
        phone: Phone number string
        
    Returns:
        True if valid phone format
        
    Example:
        >>> validate_phone("(555) 123-4567")
        True
        >>> validate_phone("555-1234")
        False
    """
    return bool(PHONE_PATTERN.match(phone)) if isinstance(phone, str) else False


def format_output(data: Any, format_type: str = "json", indent: int = 2) -> str:
    """
    Format data for output in various formats.
    
    Args:
        data: Data to format
        format_type: Output format ("json", "yaml", "pretty", "csv")
        indent: Indentation level for formatted output
        
    Returns:
        Formatted string
        
    Example:
        >>> data = {"name": "test", "value": 42}
        >>> formatted = format_output(data, "json")
        >>> "name" in formatted and "value" in formatted
        True
    """
    try:
        if format_type == "json":
            return json.dumps(data, indent=indent, ensure_ascii=False, default=str)
        
        elif format_type == "yaml":
            try:
                import yaml
                return yaml.dump(data, default_flow_style=False, indent=indent)
            except ImportError:
                logger.warning("PyYAML not available, falling back to JSON")
                return json.dumps(data, indent=indent)
        
        elif format_type == "pretty":
            return _pretty_format(data, indent)
        
        elif format_type == "csv" and isinstance(data, (list, tuple)):
            return _format_as_csv(data)
        
        else:
            return str(data)
    
    except Exception as e:
        logger.error(f"Failed to format output: {e}")
        return str(data)


def _pretty_format(data: Any, indent: int = 2) -> str:
    """Format data in a pretty, human-readable format."""
    def _format_recursive(obj, level=0):
        prefix = " " * (level * indent)
        
        if isinstance(obj, dict):
            if not obj:
                return "{}"
            lines = ["{"]
            for key, value in obj.items():
                formatted_value = _format_recursive(value, level + 1)
                lines.append(f"{prefix}  {key}: {formatted_value}")
            lines.append(f"{prefix}}}")
            return "\n".join(lines)
        
        elif isinstance(obj, (list, tuple)):
            if not obj:
                return "[]"
            lines = ["["]
            for item in obj:
                formatted_item = _format_recursive(item, level + 1)
                lines.append(f"{prefix}  {formatted_item}")
            lines.append(f"{prefix}]")
            return "\n".join(lines)
        
        elif isinstance(obj, str):
            return f'"{obj}"'
        
        else:
            return str(obj)
    
    return _format_recursive(data)


def _format_as_csv(data: List[Any]) -> str:
    """Format list data as CSV."""
    import csv
    import io
    
    if not data:
        return ""
    
    output = io.StringIO()
    
    # Handle list of dicts
    if isinstance(data[0], dict):
        fieldnames = data[0].keys()
        writer = csv.DictWriter(output, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
    else:
        # Handle list of values
        writer = csv.writer(output)
        for item in data:
            if isinstance(item, (list, tuple)):
                writer.writerow(item)
            else:
                writer.writerow([item])
    
    return output.getvalue()


def sanitize_string(text: str, max_length: int = 1000, remove_html: bool = True) -> str:
    """
    Sanitize string input by removing dangerous characters and limiting length.
    
    Args:
        text: Input text to sanitize
        max_length: Maximum allowed length
        remove_html: Whether to remove HTML tags
        
    Returns:
        Sanitized string
        
    Example:
        >>> sanitize_string("<script>alert('xss')</script>Hello", remove_html=True)
        'Hello'
    """
    if not isinstance(text, str):
        text = str(text)
    
    # Remove HTML tags if requested
    if remove_html:
        text = re.sub(r'<[^>]+>', '', text)
    
    # Remove potentially dangerous characters
    text = re.sub(r'[<>\"\'&]', '', text)
    
    # Limit length
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    return text.strip()


def deep_merge_dicts(dict1: Dict, dict2: Dict) -> Dict:
    """
    Deep merge two dictionaries.
    
    Args:
        dict1: First dictionary
        dict2: Second dictionary (takes precedence)
        
    Returns:
        Merged dictionary
        
    Example:
        >>> d1 = {"a": {"x": 1}, "b": 2}
        >>> d2 = {"a": {"y": 2}, "c": 3}
        >>> result = deep_merge_dicts(d1, d2)
        >>> result["a"]["x"] == 1 and result["a"]["y"] == 2
        True
    """
    result = dict1.copy()
    
    for key, value in dict2.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge_dicts(result[key], value)
        else:
            result[key] = value
    
    return result


def flatten_dict(d: Dict, parent_key: str = '', sep: str = '.') -> Dict:
    """
    Flatten a nested dictionary.
    
    Args:
        d: Dictionary to flatten
        parent_key: Parent key for recursion
        sep: Separator for nested keys
        
    Returns:
        Flattened dictionary
        
    Example:
        >>> nested = {"a": {"b": {"c": 1}}, "d": 2}
        >>> flat = flatten_dict(nested)
        >>> flat["a.b.c"] == 1
        True
    """
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)


@timing_decorator
def safe_file_operation(filepath: Union[str, Path], operation: str, data: Any = None) -> Any:
    """
    Safely perform file operations with proper error handling.
    
    Args:
        filepath: Path to the file
        operation: Operation type ("read", "write", "append")
        data: Data to write (for write operations)
        
    Returns:
        File content for read operations, None for write operations
        
    Example:
        >>> safe_file_operation("test.txt", "write", "Hello World")
        >>> content = safe_file_operation("test.txt", "read")
        >>> content.strip()
        'Hello World'
    """
    filepath = Path(filepath)
    
    try:
        if operation == "read":
            if not filepath.exists():
                raise FileNotFoundError(f"File not found: {filepath}")
            return filepath.read_text(encoding='utf-8')
        
        elif operation == "write":
            filepath.parent.mkdir(parents=True, exist_ok=True)
            filepath.write_text(str(data), encoding='utf-8')
            logger.info(f"Successfully wrote to {filepath}")
        
        elif operation == "append":
            filepath.parent.mkdir(parents=True, exist_ok=True)
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(str(data))
            logger.info(f"Successfully appended to {filepath}")
        
        else:
            raise ValueError(f"Unknown operation: {operation}")
    
    except Exception as e:
        logger.error(f"File operation failed: {e}")
        raise


def calculate_file_hash(filepath: Union[str, Path], algorithm: str = "sha256") -> str:
    """
    Calculate hash of a file.
    
    Args:
        filepath: Path to the file
        algorithm: Hash algorithm ("md5", "sha1", "sha256")
        
    Returns:
        Hex digest of the file hash
        
    Example:
        >>> hash_value = calculate_file_hash("test.txt")
        >>> len(hash_value) == 64  # SHA256 produces 64 character hex string
        True
    """
    import hashlib
    
    filepath = Path(filepath)
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")
    
    hash_obj = getattr(hashlib, algorithm)()
    
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_obj.update(chunk)
    
    return hash_obj.hexdigest()


class PerformanceMonitor:
    """
    Context manager for monitoring performance metrics.
    
    Example:
        >>> with PerformanceMonitor("test_operation") as monitor:
        ...     time.sleep(0.1)  # Simulate work
        ...     monitor.add_metric("items_processed", 42)
        >>> monitor.get_results()["duration"] > 0
        True
    """
    
    def __init__(self, operation_name: str):
        self.operation_name = operation_name
        self.start_time = None
        self.end_time = None
        self.metrics = {}
    
    def __enter__(self):
        self.start_time = time.time()
        logger.debug(f"Starting performance monitoring for: {self.operation_name}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        duration = self.end_time - self.start_time
        
        if exc_type is None:
            logger.info(f"{self.operation_name} completed in {duration:.4f} seconds")
        else:
            logger.error(f"{self.operation_name} failed after {duration:.4f} seconds")
        
        self.metrics['duration'] = duration
        self.metrics['success'] = exc_type is None
        self.metrics['timestamp'] = self.end_time
    
    def add_metric(self, name: str, value: Any) -> None:
        """Add a custom metric."""
        self.metrics[name] = value
    
    def get_results(self) -> Dict[str, Any]:
        """Get all performance metrics."""
        return {
            'operation': self.operation_name,
            **self.metrics
        }


# Factory for creating common validators
def create_validators(
    min_length: Optional[int] = None,
    max_length: Optional[int] = None,
    pattern: Optional[str] = None,
    allowed_types: Optional[List[type]] = None
) -> List[ValidatorFunction]:
    """
    Create a list of common validators.
    
    Args:
        min_length: Minimum length requirement
        max_length: Maximum length requirement  
        pattern: Regex pattern to match
        allowed_types: List of allowed types
        
    Returns:
        List of validator functions
        
    Example:
        >>> validators = create_validators(min_length=3, max_length=10, allowed_types=[str])
        >>> validate_input("hello", validators)
        True
        >>> validate_input("hi", validators)
        False
    """
    validators = []
    
    if allowed_types:
        validators.append(lambda x: type(x) in allowed_types)
    
    if min_length is not None:
        validators.append(lambda x: len(str(x)) >= min_length)
    
    if max_length is not None:
        validators.append(lambda x: len(str(x)) <= max_length)
    
    if pattern:
        compiled_pattern = re.compile(pattern)
        validators.append(lambda x: bool(compiled_pattern.match(str(x))))
    
    return validators


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\fixtures\sample_project\sample_project\__init__.py
"""
Sample Project - A comprehensive Python project for testing README generation.

This package demonstrates various Python patterns and structures including:
- Configuration management
- CLI interfaces  
- Data models
- Utility functions
- Core business logic
- Entry points and scripts

Features:
- Configurable data processing pipeline
- Command-line interface with multiple commands
- Extensible plugin architecture
- Comprehensive error handling
- Type hints and documentation

Usage:
    >>> from sample_project import SampleProcessor
    >>> processor = SampleProcessor()
    >>> result = processor.process("Hello World")
    >>> print(result)
    Processed: Hello World

Example:
    Basic usage of the package:

    ```
    from sample_project import SampleProcessor, Config
    from sample_project.models import DataModel

    # Initialize with custom config
    config = Config(debug=True, max_items=100)
    processor = SampleProcessor(config)

    # Process some data
    data = DataModel(name="test", value=42)
    result = processor.process_data_model(data)
    ```
"""

from .main import SampleProcessor, DataProcessor, ProcessingError
from .config import Config, load_config, save_config, DEFAULT_CONFIG
from .models import DataModel, ResultModel, ValidationError
from .utils import format_output, validate_input, timing_decorator

# Package metadata
__version__ = "1.2.3"
__author__ = "John Developer"
__email__ = "john.dev@example.com"
__description__ = "A comprehensive sample Python project for testing README generation"
__url__ = "https://github.com/example/sample-project"
__license__ = "MIT"

# Version info tuple
__version_info__ = tuple(map(int, __version__.split('.')))

# Package-level constants
DEFAULT_TIMEOUT = 30
MAX_RETRIES = 3
SUPPORTED_FORMATS = ["json", "yaml", "xml", "csv"]

# Main API exports
__all__ = [
    # Core classes
    "SampleProcessor",
    "DataProcessor", 
    
    # Configuration
    "Config",
    "load_config",
    "save_config",
    "DEFAULT_CONFIG",
    
    # Data models
    "DataModel",
    "ResultModel",
    
    # Utilities
    "format_output",
    "validate_input", 
    "timing_decorator",
    
    # Exceptions
    "ProcessingError",
    "ValidationError",
    
    # Metadata
    "__version__",
    "__author__",
    "__email__",
    "__description__",
    "__url__",
    "__license__",
    "__version_info__",
    
    # Constants
    "DEFAULT_TIMEOUT",
    "MAX_RETRIES", 
    "SUPPORTED_FORMATS",
]

# Package initialization
def get_version() -> str:
    """Get the current package version."""
    return __version__

def get_package_info() -> dict:
    """Get comprehensive package information."""
    return {
        "name": "sample-project",
        "version": __version__,
        "author": __author__,
        "email": __email__,
        "description": __description__,
        "url": __url__,
        "license": __license__,
        "python_requires": ">=3.8",
        "supported_formats": SUPPORTED_FORMATS,
    }

# Convenience function for quick setup
def quick_setup(debug: bool = False, **kwargs) -> SampleProcessor:
    """
    Quickly set up a SampleProcessor with common defaults.
    
    Args:
        debug: Enable debug mode
        **kwargs: Additional configuration options
        
    Returns:
        Configured SampleProcessor instance
        
    Example:
        >>> processor = quick_setup(debug=True, max_items=50)
        >>> isinstance(processor, SampleProcessor)
        True
    """
    config = Config(debug=debug, **kwargs)
    return SampleProcessor(config)

# Module-level logger setup
import logging

def setup_logging(level: str = "INFO") -> None:
    """Set up package-level logging configuration."""
    numeric_level = getattr(logging, level.upper(), logging.INFO)
    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logging.getLogger(__name__).info(f"Sample Project v{__version__} initialized")

# Auto-setup logging when package is imported
setup_logging()


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_code_parser.py
import pytest
import tempfile
from pathlib import Path
from textwrap import dedent

from parser.code_parser import parse_code_file, _get_name_from_node
import ast

class TestParseCodeFile:
    """Test cases for parse_code_file function."""
    
    def test_parse_simple_python_file(self, tmp_path):
        """Test parsing a simple Python file with basic structure."""
        test_file = tmp_path / "simple.py"
        test_content = dedent('''
        """This is a simple module."""
        
        def hello_world():
            """Say hello to the world."""
            return "Hello, World!"
        
        class SimpleClass:
            """A simple class example."""
            
            def __init__(self):
                self.value = 42
                
            def get_value(self):
                """Get the stored value."""
                return self.value
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert result["name"] == "simple"
        assert result["file"] == str(test_file)
        assert result["docstring"] == "This is a simple module."
        assert len(result["classes"]) == 1
        assert len(result["functions"]) == 1
        assert result["classes"][0]["name"] == "SimpleClass"
        assert result["functions"][0]["name"] == "hello_world"
    
    def test_parse_file_with_full_code_extraction(self, tmp_path):
        """Test parsing with full source code extraction enabled."""
        test_file = tmp_path / "entry_point.py"
        test_content = dedent('''
        #!/usr/bin/env python3
        """Entry point module."""
        
        def main():
            print("Hello from main!")
            
        if __name__ == "__main__":
            main()
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file), extract_full_code=True)
        
        assert result is not None
        assert "source_code" in result
        assert result["source_code"] == test_content
        assert result["functions"][0]["name"] == "main"
    
    def test_parse_file_without_full_code_extraction(self, tmp_path):
        """Test parsing without full source code extraction."""
        test_file = tmp_path / "regular.py"
        test_content = '''def test(): pass'''
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file), extract_full_code=False)
        
        assert result is not None
        assert "source_code" not in result
        assert result["functions"][0]["name"] == "test"
    
    def test_parse_file_with_classes_and_methods(self, tmp_path):
        """Test parsing file with classes containing methods."""
        test_file = tmp_path / "class_example.py"
        test_content = dedent('''
        class Calculator:
            """A calculator class."""
            
            def __init__(self, initial_value=0):
                """Initialize calculator with optional initial value."""
                self.value = initial_value
            
            @property
            def current_value(self):
                """Get current calculator value."""
                return self.value
                
            @staticmethod
            def add_numbers(a, b):
                """Add two numbers."""
                return a + b
            
            @classmethod    
            def from_string(cls, value_str):
                """Create calculator from string value."""
                return cls(int(value_str))
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert len(result["classes"]) == 1
        
        calc_class = result["classes"][0]
        assert calc_class["name"] == "Calculator"
        assert calc_class["docstring"] == "A calculator class."
        assert len(calc_class["methods"]) == 4
        
        # Check method details
        method_names = [m["name"] for m in calc_class["methods"]]
        assert "__init__" in method_names
        assert "current_value" in method_names
        assert "add_numbers" in method_names
        assert "from_string" in method_names
        
        # Check method with decorators
        static_method = next(m for m in calc_class["methods"] if m["name"] == "add_numbers")
        assert "staticmethod" in static_method["decorators"]
        
        class_method = next(m for m in calc_class["methods"] if m["name"] == "from_string")
        assert "classmethod" in class_method["decorators"]
    
    def test_parse_file_with_inheritance(self, tmp_path):
        """Test parsing file with class inheritance."""
        test_file = tmp_path / "inheritance.py"
        test_content = dedent('''
        class Animal:
            """Base animal class."""
            pass
            
        class Dog(Animal):
            """Dog class inheriting from Animal."""
            
            def bark(self):
                return "Woof!"
                
        class GermanShepherd(Dog, object):
            """German Shepherd inheriting from Dog and object."""
            
            def guard(self):
                return "Protecting!"
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert len(result["classes"]) == 3
        
        # Check base classes
        dog_class = next(c for c in result["classes"] if c["name"] == "Dog")
        assert dog_class["bases"] == ["Animal"]
        
        german_shepherd = next(c for c in result["classes"] if c["name"] == "GermanShepherd") 
        assert "Dog" in german_shepherd["bases"]
        assert "object" in german_shepherd["bases"]
    
    def test_parse_file_with_imports(self, tmp_path):
        """Test parsing file with various import statements."""
        test_file = tmp_path / "imports.py"
        test_content = dedent('''
        import os
        import sys
        from pathlib import Path
        from typing import List, Dict
        from . import relative_module
        from ..parent import parent_module
        
        def use_imports():
            pass
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert len(result["imports"]) >= 6
        
        imports = result["imports"]
        assert "os" in imports
        assert "sys" in imports
        assert "from pathlib import Path" in imports
        assert "from typing import List" in imports
        assert "from typing import Dict" in imports
    
    def test_parse_file_with_decorators(self, tmp_path):
        """Test parsing file with decorated functions and classes."""
        test_file = tmp_path / "decorators.py"
        test_content = dedent('''
        from functools import wraps
        
        def my_decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                return func(*args, **kwargs)
            return wrapper
        
        @my_decorator
        @staticmethod
        def decorated_function():
            """A decorated function."""
            return "decorated"
            
        @property
        class DecoratedClass:
            """A decorated class."""
            pass
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        
        # Check function decorators
        decorated_func = next(f for f in result["functions"] if f["name"] == "decorated_function")
        assert "my_decorator" in decorated_func["decorators"]
        assert "staticmethod" in decorated_func["decorators"]
        
        # Check class decorators  
        decorated_class = next(c for c in result["classes"] if c["name"] == "DecoratedClass")
        assert "property" in decorated_class["decorators"]
    
    def test_parse_empty_file(self, tmp_path):
        """Test parsing an empty Python file."""
        test_file = tmp_path / "empty.py"
        test_file.write_text("")
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert result["name"] == "empty"
        assert result["docstring"] is None
        assert len(result["classes"]) == 0
        assert len(result["functions"]) == 0
        assert len(result["imports"]) == 0
    
    def test_parse_file_with_module_docstring_only(self, tmp_path):
        """Test parsing file with only module docstring."""
        test_file = tmp_path / "docstring_only.py"
        test_content = '''"""This module only has a docstring."""'''
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert result["docstring"] == "This module only has a docstring."
        assert len(result["classes"]) == 0
        assert len(result["functions"]) == 0
    
    def test_parse_nonexistent_file(self):
        """Test parsing a file that doesn't exist."""
        result = parse_code_file("/nonexistent/file.py")
        assert result is None
    
    def test_parse_invalid_python_syntax(self, tmp_path):
        """Test parsing file with invalid Python syntax."""
        test_file = tmp_path / "invalid.py"
        test_content = '''
        def broken_function(
            # Missing closing parenthesis and colon
        '''
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        assert result is None
    
    def test_parse_binary_file(self, tmp_path):
        """Test parsing a binary file."""
        test_file = tmp_path / "binary.py"
        # Write some binary data
        test_file.write_bytes(b'\x00\x01\x02\x03')
        
        result = parse_code_file(str(test_file))
        assert result is None
    
    def test_parse_file_with_complex_expressions(self, tmp_path):
        """Test parsing file with complex expressions and edge cases."""
        test_file = tmp_path / "complex.py"
        test_content = dedent('''
        """Module with complex expressions."""
        
        # Constants at module level
        VERSION = "1.0.0"
        DEBUG = True
        
        class ComplexClass(dict, metaclass=type):
            """Class with complex inheritance and metaclass."""
            
            @property
            @lru_cache(maxsize=128)
            def complex_property(self):
                """Property with multiple decorators."""
                return self._value
            
            def method_with_complex_args(self, pos_arg, *args, **kwargs):
                """Method with complex argument signature."""
                return locals()
        
        def function_with_defaults(a=1, b="default", c=[]):
            """Function with default arguments."""
            return a, b, c
        
        async def async_function():
            """An async function."""
            await something()
        
        def generator_function():
            """A generator function."""
            yield 1
            yield 2
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file))
        
        assert result is not None
        assert result["docstring"] == "Module with complex expressions."
        
        # Check complex class
        complex_class = result["classes"][0]
        assert complex_class["name"] == "ComplexClass"
        assert "dict" in complex_class["bases"]
        
        # Check function names
        func_names = [f["name"] for f in result["functions"]]
        assert "function_with_defaults" in func_names
        assert "async_function" in func_names
        assert "generator_function" in func_names


class TestGetNameFromNode:
    """Test cases for _get_name_from_node helper function."""
    
    def test_name_node(self):
        """Test extracting name from Name node."""
        node = ast.Name(id="test_name")
        result = _get_name_from_node(node)
        assert result == "test_name"
    
    def test_attribute_node(self):
        """Test extracting name from Attribute node."""
        # Create ast.Attribute for "module.attribute"
        value_node = ast.Name(id="module")
        node = ast.Attribute(value=value_node, attr="attribute")
        result = _get_name_from_node(node)
        assert result == "module.attribute"
    
    def test_nested_attribute_node(self):
        """Test extracting name from nested Attribute node."""
        # Create ast.Attribute for "package.module.attribute"
        package_node = ast.Name(id="package")
        module_node = ast.Attribute(value=package_node, attr="module")
        attribute_node = ast.Attribute(value=module_node, attr="attribute")
        
        result = _get_name_from_node(attribute_node)
        assert result == "package.module.attribute"
    
    def test_constant_node(self):
        """Test extracting name from Constant node."""
        node = ast.Constant(value="string_constant")
        result = _get_name_from_node(node)
        assert result == "string_constant"
        
        node = ast.Constant(value=42)
        result = _get_name_from_node(node)
        assert result == "42"
    
    def test_unknown_node_type(self):
        """Test extracting name from unknown node type."""
        # Use a List node which isn't handled specifically
        node = ast.List(elts=[], ctx=ast.Load())
        result = _get_name_from_node(node)
        # Should return string representation
        assert isinstance(result, str)
        assert "List" in result


class TestCodeParserIntegration:
    """Integration tests for the code parser."""
    
    def test_parse_real_world_module(self, tmp_path):
        """Test parsing a realistic Python module."""
        test_file = tmp_path / "real_world.py"
        test_content = dedent('''
        #!/usr/bin/env python3
        """
        A real-world Python module example.
        
        This module demonstrates common Python patterns and structures
        that the parser should handle correctly.
        """
        
        import os
        import sys
        from typing import Optional, List, Dict, Any
        from pathlib import Path
        from dataclasses import dataclass
        
        __version__ = "1.0.0"
        __author__ = "Test Author"
        
        # Module-level constants
        DEFAULT_CONFIG = {
            "debug": False,
            "max_retries": 3
        }
        
        @dataclass
        class Configuration:
            """Configuration data class."""
            debug: bool = False
            max_retries: int = 3
            output_path: Optional[Path] = None
            
            def validate(self) -> bool:
                """Validate configuration settings."""
                return self.max_retries > 0
        
        class BaseProcessor:
            """Base class for all processors."""
            
            def __init__(self, config: Configuration):
                """Initialize with configuration."""
                self.config = config
                self._state = "initialized"
            
            @property
            def is_ready(self) -> bool:
                """Check if processor is ready."""
                return self._state == "ready"
            
            @abstractmethod
            def process(self, data: Any) -> Any:
                """Process data - must be implemented by subclasses."""
                raise NotImplementedError
            
            @classmethod
            def from_dict(cls, config_dict: Dict[str, Any]) -> 'BaseProcessor':
                """Create processor from dictionary."""
                config = Configuration(**config_dict)
                return cls(config)
        
        class FileProcessor(BaseProcessor):
            """Processor for file operations."""
            
            def __init__(self, config: Configuration, file_patterns: List[str] = None):
                """Initialize file processor."""
                super().__init__(config)
                self.file_patterns = file_patterns or ["*.py"]
            
            def process(self, file_path: Path) -> Dict[str, Any]:
                """Process a single file."""
                if not file_path.exists():
                    raise FileNotFoundError(f"File not found: {file_path}")
                
                return {
                    "path": str(file_path),
                    "size": file_path.stat().st_size,
                    "processed": True
                }
            
            async def process_async(self, file_path: Path) -> Dict[str, Any]:
                """Asynchronously process a file."""
                return await self._async_process_impl(file_path)
            
            def _async_process_impl(self, file_path: Path) -> Dict[str, Any]:
                """Implementation detail for async processing."""
                return self.process(file_path)
        
        def main(args: Optional[List[str]] = None) -> int:
            """Main entry point."""
            if args is None:
                args = sys.argv[1:]
            
            config = Configuration(debug="--debug" in args)
            processor = FileProcessor(config)
            
            try:
                # Process files
                for arg in args:
                    if not arg.startswith("--"):
                        result = processor.process(Path(arg))
                        print(f"Processed: {result}")
                return 0
            except Exception as e:
                print(f"Error: {e}")
                return 1
        
        if __name__ == "__main__":
            sys.exit(main())
        ''').strip()
        
        test_file.write_text(test_content)
        
        result = parse_code_file(str(test_file), extract_full_code=True)
        
        # Comprehensive validation
        assert result is not None
        assert result["name"] == "real_world"
        assert "A real-world Python module example." in result["docstring"]
        assert "source_code" in result
        
        # Check imports
        imports = result["imports"]
        assert "os" in imports
        assert "sys" in imports
        assert any("from typing import" in imp for imp in imports)
        
        # Check classes
        class_names = [c["name"] for c in result["classes"]]
        assert "Configuration" in class_names
        assert "BaseProcessor" in class_names
        assert "FileProcessor" in class_names
        
        # Check inheritance
        file_processor = next(c for c in result["classes"] if c["name"] == "FileProcessor")
        assert "BaseProcessor" in file_processor["bases"]
        
        # Check decorators
        config_class = next(c for c in result["classes"] if c["name"] == "Configuration")
        assert "dataclass" in config_class["decorators"]
        
        # Check functions
        func_names = [f["name"] for f in result["functions"]]
        assert "main" in func_names
        
        # Verify methods are captured
        base_processor = next(c for c in result["classes"] if c["name"] == "BaseProcessor")
        method_names = [m["name"] for m in base_processor["methods"]]
        assert "__init__" in method_names
        assert "is_ready" in method_names
        assert "process" in method_names
        assert "from_dict" in method_names


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_dependency_parser.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_entry_point_parser.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_example_parser.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_metadata_parser.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_project_parser.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\test_structure_parser.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\tests\parser\__init__.py


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\utils\content_prioritizer.py
from .token_counter import estimate_tokens

def filter_content_under_token_limit(items, max_tokens):
    """Yield as many items as fit under max_tokens (approx)."""
    total = 0
    for item in items:
        tc = estimate_tokens(str(item))
        if total + tc > max_tokens:
            break
        total += tc
        yield item


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\utils\file_utils.py
"""
File system utilities for safe and efficient file operations.

This module provides functions for reading files, detecting encodings,
and basic directory traversal for Python projects.
"""

import os
import mimetypes
from pathlib import Path
from typing import Optional, Generator
import logging

logger = logging.getLogger(__name__)

IGNORE_FOLDERS = {'.git', 'venv', 'env', '__pycache__', 'build', 'dist', '.tox'}
PYTHON_EXTENSIONS = {'.py', '.pyw', '.pyx', '.pyi'}
TEXT_EXTENSIONS = {
    '.txt', '.md', '.rst', '.yaml', '.yml', '.json', '.xml', '.cfg',
    '.ini', '.toml', '.conf', '.env'
}

def read_file_safely(file_path: str, max_size: int = 10 * 1024 * 1024) -> Optional[str]:
    """Reads file contents if file size is below max_size."""
    try:
        p = Path(file_path)
        if not p.is_file():
            logger.debug(f"Not a file: {file_path}")
            return None
        if p.stat().st_size > max_size:
            logger.warning(f"File too large to read: {file_path}")
            return None
        return p.read_text(encoding='utf-8', errors='ignore')
    except Exception as e:
        logger.error(f"Failed to read file {file_path}: {e}")
        return None

def create_directory(path: str) -> bool:
    """Create directory if it doesn't exist."""
    try:
        Path(path).mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        logger.error(f"Failed to create directory {path}: {e}")
        return False

def find_python_files(project_path: str, include_tests: bool = False) -> Generator[str, None, None]:
    """Yield all Python file paths in project, optionally skipping test folders."""
    root_path = Path(project_path)
    if not root_path.exists():
        logger.warning(f"Project path not found: {project_path}")
        return
    for root, dirs, files in os.walk(project_path):
        # Filter out ignored folders
        dirs[:] = [d for d in dirs if d not in IGNORE_FOLDERS]
        if not include_tests:
            # Optionally skip any test folder
            dirs[:] = [d for d in dirs if 'test' not in d.lower()]
        for file in files:
            if file.endswith('.py'):
                yield str(Path(root) / file)


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\utils\json_serializer.py
"""
JSON serialization utilities for project data output.

This module provides functions to serialize Python dict/list project data
to JSON files with proper formatting and error handling.
"""

import json
from typing import Any, Dict, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

def serialize_project_data(data: Any) -> Any:
    """
    Serialize project data; mono pass-through for MVP.
    This can be extended if richer processing is required.
    """
    return data

def format_json_output(data: Any, indent: int = 2) -> str:
    """
    Format data as pretty JSON string.
    """
    try:
        return json.dumps(data, indent=indent, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Failed to format JSON output: {e}")
        return "{}"

def save_json_to_file(data: Any, file_path: str, indent: int = 2) -> bool:
    """
    Save data as JSON to the given filepath.
    Returns True on success, False on failure.
    """
    try:
        output_path = Path(file_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        json_text = format_json_output(data, indent=indent)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(json_text)
        logger.info(f"Saved JSON data to {file_path}")
        return True
    except Exception as e:
        logger.error(f"Failed to save JSON to {file_path}: {e}")
        return False

def load_json_from_file(file_path: str) -> Optional[Dict]:
    """
    Load and parse JSON file to dict.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Failed to load JSON from {file_path}: {e}")
        return None


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\utils\token_counter.py
"""
Token counting and estimation utilities for LLM context budgets.

This module provides rough token estimates for strings,
appropriate for planning LLM prompt sizes.
"""

import json
import logging

logger = logging.getLogger(__name__)

def estimate_tokens(text: str) -> int:
    """
    Rough token estimate based on character count (~4 chars per token).
    """
    if not text:
        return 0
    # Basic heuristic: one token per 4 characters roughly
    return max(1, len(text) // 4)

def count_tokens_in_dict(data: dict) -> int:
    """
    Estimate tokens in a dict by JSON serialization length.
    """
    try:
        text = json.dumps(data, ensure_ascii=False)
        return estimate_tokens(text)
    except Exception as e:
        logger.error(f"Error estimating tokens in dict: {e}")
        return 0


File: C:/Users/lwhitaker/personal\SYSC4918\SYSC4918/src\utils\__init__.py
from .file_utils import (
    read_file_safely,
    create_directory,
    find_python_files
)

from .json_serializer import (
    serialize_project_data,
    save_json_to_file,
    load_json_from_file,
    format_json_output
)

from .token_counter import (
    estimate_tokens,
    count_tokens_in_dict,
)

__all__ = [
    "read_file_safely",
    "create_directory",
    "find_python_files",
    "serialize_project_data",
    "save_json_to_file",
    "load_json_from_file",
    "format_json_output",
    "estimate_tokens",
    "count_tokens_in_dict",
]


