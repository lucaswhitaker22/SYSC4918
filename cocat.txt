File: /workspaces/SYSC4918/src/cli.py
"""
Command-line interface for the README generator.

This module provides the main CLI functionality for automatically generating
comprehensive README files for Python projects using LLM APIs, specifically
optimized for Gemini 2.5 Pro with 1M token context window.
"""

import os
import sys
import argparse
import time
import json
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List
import logging

# Third-party imports
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    import openai
except ImportError:
    openai = None

try:
    import anthropic
except ImportError:
    anthropic = None

from config import Config, load_config, save_config
from parser.project_parser import parse_project
from utils.token_counter import TokenCounter, get_token_budget_allocation
from utils.file_utils import create_directory, read_file_safely
from utils.json_serializer import serialize_project_data, save_json_to_file

logger = logging.getLogger(__name__)


class CLIError(Exception):
    """Custom exception for CLI errors."""
    pass


class LLMAPIError(Exception):
    """Custom exception for LLM API errors."""
    pass


def create_parser() -> argparse.ArgumentParser:
    """Create and configure the argument parser."""
    parser = argparse.ArgumentParser(
        prog="readme-generator",
        description="description",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  readme-generator /path/to/project
  readme-generator . --output custom_readme.md
  readme-generator /path/to/project --model gemini_2_5_pro
  readme-generator /path/to/project --api-key YOUR_API_KEY
  readme-generator --init-config
  
Performance Target:
  Generates README for projects up to 25 files/5,000 lines in under 90 seconds
        """
    )
    
    # Version
    parser.add_argument(
        '--version',
        action='version',
        version=f'%(prog)s 1.0',
    )
    
    parser.add_argument(
        '--parse-only',
        action='store_true',
        help='Only parse project data to JSON, skip README generation'
    )

    # Main argument
    parser.add_argument(
        'project_path',
        nargs='?',
        help='Path to the Python project root directory'
    )
    
    # Output options
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='README.md',
        help='Output README file path (default: README.md in project root)'
    )
    
    parser.add_argument(
        '--json-output',
        type=str,
        help='Save parsed project data as JSON file'
    )
    
    # Model configuration (based on research recommendations)
    parser.add_argument(
        '--model',
        choices=['gemini_2_5_pro', 'gemini_2_5_flash', 'gpt_4o', 'gpt_4o_mini', 'claude_sonnet'],
        default='gemini_2_5_pro',
        help='LLM model to use (default: gemini_2_5_pro - recommended for balance of cost/context/accuracy)'
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        help='API key for LLM service (or set via environment variables)'
    )
    
    parser.add_argument(
        '--max-tokens',
        type=int,
        default=1_000_000,
        help='Maximum token budget (default: 1000000 for Gemini 2.5 Pro)'
    )
    
    # Parsing options
    parser.add_argument(
        '--include-tests',
        action='store_true',
        help='Include test files in analysis'
    )
    
    parser.add_argument(
        '--include-private',
        action='store_true',
        help='Include private methods and classes'
    )
    
    # Configuration
    parser.add_argument(
        '--config',
        type=str,
        help='Path to configuration file'
    )
    
    parser.add_argument(
        '--init-config',
        action='store_true',
        help='Initialize configuration file with default settings'
    )
    
    parser.add_argument(
        '--save-config',
        type=str,
        help='Save current settings to configuration file'
    )
    
    # Verbosity and debugging
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose output'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='Suppress non-error output'
    )
    
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug mode with detailed logging'
    )
    
    # Performance options
    parser.add_argument(
        '--timeout',
        type=int,
        default=90,
        help='Timeout in seconds (default: 90 - meets performance requirement)'
    )
    
    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='Disable result caching'
    )
    
    # Template options
    parser.add_argument(
        '--template',
        type=str,
        help='Custom README template file'
    )
    
    parser.add_argument(
        '--sections',
        type=str,
        nargs='+',
        default=['description', 'installation', 'usage', 'structure', 'dependencies'],
        help='README sections to generate'
    )
    
    return parser


def validate_arguments(args: argparse.Namespace) -> None:
    """Validate command-line arguments."""
    # Special commands that don't require project path
    if args.init_config:
        return
    
    # Validate project path
    if not args.project_path:
        raise CLIError("Project path is required. Use --help for usage information.")
    
    project_path = Path(args.project_path).resolve()
    if not project_path.exists():
        raise CLIError(f"Project path does not exist: {args.project_path}")
    
    if not project_path.is_dir():
        raise CLIError(f"Project path is not a directory: {args.project_path}")
    
    # Check if it looks like a Python project
    python_indicators = ['setup.py', 'pyproject.toml', 'requirements.txt', '__init__.py']
    has_python_files = any((project_path / indicator).exists() for indicator in python_indicators)
    has_py_files = any(project_path.glob('**/*.py'))
    
    if not has_python_files and not has_py_files:
        logger.warning(f"No Python project indicators found in {project_path}")
    
    # Validate token limit
    if args.max_tokens < 1000:
        raise CLIError("Maximum tokens must be at least 1000")
    
    # Validate timeout (performance requirement: under 90 seconds)
    if args.timeout < 1:
        raise CLIError("Timeout must be at least 1 second")
    
    if args.timeout > 300:
        logger.warning(f"Timeout of {args.timeout}s exceeds recommended 90s performance target")


def validate_api_requirements(model_name: str, api_key: Optional[str]) -> str:
    """Validate API requirements and return the API key."""
    # Environment variable mappings based on research
    env_key_mapping = {
        'gemini_2_5_pro': 'GEMINI_API_KEY',
        'gemini_2_5_flash': 'GEMINI_API_KEY', 
        'gpt_4o': 'OPENAI_API_KEY',
        'gpt_4o_mini': 'OPENAI_API_KEY',
        'claude_sonnet': 'ANTHROPIC_API_KEY'
    }
    
    # Get API key from argument or environment
    if not api_key:
        env_var = env_key_mapping.get(model_name)
        if env_var:
            api_key = os.getenv(env_var)
    
    if not api_key:
        env_var = env_key_mapping.get(model_name, 'API_KEY')
        raise CLIError(
            f"API key required for {model_name}. "
            f"Provide via --api-key argument or {env_var} environment variable."
        )
    
    # Validate required packages
    if model_name.startswith('gemini') and not genai:
        raise CLIError("google-generativeai package required for Gemini models. Install with: pip install google-generativeai")
    elif model_name.startswith('gpt') and not openai:
        raise CLIError("openai package required for OpenAI models. Install with: pip install openai")
    elif model_name.startswith('claude') and not anthropic:
        raise CLIError("anthropic package required for Claude models. Install with: pip install anthropic")
    
    return api_key


def load_configuration(args: argparse.Namespace) -> Config:
    """Load configuration from file and command-line arguments."""
    config = Config()
    
    # Load from config file if specified
    if args.config:
        try:
            loaded_config = load_config(args.config)
            config.update(loaded_config)
            logger.info(f"Loaded configuration from {args.config}")
        except Exception as e:
            logger.warning(f"Failed to load configuration from {args.config}: {e}")
    
    # Override with command-line arguments
    config.model_name = args.model
    config.max_tokens = args.max_tokens
    config.include_tests = args.include_tests
    config.include_private = args.include_private
    config.verbose = args.verbose
    config.quiet = args.quiet
    config.debug = args.debug
    config.timeout = args.timeout
    config.cache_enabled = not args.no_cache
    
    if args.api_key:
        config.api_key = args.api_key
    
    return config


async def generate_readme_with_llm(project_data: Any, config: Config, api_key: str) -> str:
    """Generate README content using the specified LLM API."""
    try:
        # Serialize project data for LLM prompt
        serialized_data = serialize_project_data(project_data)
        
        # Count tokens to ensure we're within limits
        token_counter = TokenCounter(config.model_name)
        token_count = token_counter.count_tokens_in_dict(serialized_data)
        
        logger.info(f"Sending {token_count:,} tokens to {config.model_name}")
        
        if token_count > config.max_tokens:
            logger.warning(f"Token count ({token_count:,}) exceeds limit ({config.max_tokens:,})")
        
        # Create prompt based on project requirements
        prompt = create_readme_prompt(serialized_data, project_data.metadata.project_name)
        
        # Generate content based on model type
        if config.model_name.startswith('gemini'):
            return await generate_with_gemini(prompt, config, api_key)
        elif config.model_name.startswith('gpt'):
            return await generate_with_openai(prompt, config, api_key)
        elif config.model_name.startswith('claude'):
            return await generate_with_claude(prompt, config, api_key)
        else:
            raise LLMAPIError(f"Unsupported model: {config.model_name}")
            
    except Exception as e:
        logger.error(f"LLM generation failed: {e}")
        raise LLMAPIError(f"Failed to generate README with {config.model_name}: {e}")


def create_readme_prompt(project_data: Dict[str, Any], project_name: str) -> str:
    """Create a comprehensive prompt for README generation based on project requirements."""
    return f"""You are an expert technical writer specializing in creating comprehensive README files for Python projects. 

Generate a professional, well-structured README.md file for the Python project "{project_name}" based on the following parsed project information:

{json.dumps(project_data, indent=2)}

Requirements:
1. Create a complete README with these essential sections:
   - Project Description (clear, concise overview)
   - Installation Instructions (accurate dependency handling)
   - Usage Examples (practical, working code examples)
   - Project Structure (organized file/directory overview)
   - Dependencies (production and development)
   - API Documentation (for key classes and functions)

2. Follow these guidelines:
   - Use proper Markdown formatting
   - Include working code examples where appropriate
   - Be accurate to the actual project structure and dependencies
   - Make installation instructions clear and correct
   - Ensure examples are practical and executable
   - Maintain professional, clear writing style

3. Focus on:
   - Accuracy of project description based on actual code
   - Correctness of installation and dependency information
   - Clarity and usefulness of usage examples
   - Overall coherence and readability

Generate only the README content in Markdown format, without any additional commentary or explanations."""


async def generate_with_gemini(prompt: str, config: Config, api_key: str) -> str:
    """Generate README using Gemini API (recommended model based on research)."""
    if not genai:
        raise LLMAPIError("google-generativeai package not available")
    
    try:
        genai.configure(api_key=api_key)
        
        # Configure model based on research findings
        model_config = {
            "temperature": 0.7,
            "top_p": 0.9,
            "max_output_tokens": 8192,
        }
        
        model_name = "gemini-2.5-pro" if "pro" in config.model_name else "gemini-2.5-flash"
        model = genai.GenerativeModel(model_name=model_name, generation_config=model_config)
        
        logger.info(f"Generating README with {model_name} (1M token context window)")
        
        response = await model.generate_content_async(prompt)
        
        if not response.text:
            raise LLMAPIError("Empty response from Gemini API")
        
        return response.text
        
    except Exception as e:
        logger.error(f"Gemini API error: {e}")
        raise LLMAPIError(f"Gemini API call failed: {e}")


async def generate_with_openai(prompt: str, config: Config, api_key: str) -> str:
    """Generate README using OpenAI API."""
    if not openai:
        raise LLMAPIError("openai package not available")
    
    try:
        client = openai.AsyncOpenAI(api_key=api_key)
        
        model_mapping = {
            'gpt_4o': 'gpt-4o',
            'gpt_4o_mini': 'gpt-4o-mini'
        }
        
        model_name = model_mapping.get(config.model_name, 'gpt-4o-mini')
        
        logger.info(f"Generating README with {model_name} (128K context window)")
        
        response = await client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "You are an expert technical writer specializing in README documentation."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4096
        )
        
        if not response.choices or not response.choices[0].message.content:
            raise LLMAPIError("Empty response from OpenAI API")
        
        return response.choices[0].message.content
        
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        raise LLMAPIError(f"OpenAI API call failed: {e}")


async def generate_with_claude(prompt: str, config: Config, api_key: str) -> str:
    """Generate README using Claude API (highest coding accuracy per research)."""
    if not anthropic:
        raise LLMAPIError("anthropic package not available")
    
    try:
        client = anthropic.AsyncAnthropic(api_key=api_key)
        
        logger.info("Generating README with Claude Sonnet (200K context window, highest coding accuracy)")
        
        response = await client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4096,
            temperature=0.7,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        if not response.content or not response.content[0].text:
            raise LLMAPIError("Empty response from Claude API")
        
        return response.content[0].text
        
    except Exception as e:
        logger.error(f"Claude API error: {e}")
        raise LLMAPIError(f"Claude API call failed: {e}")


def init_config_command(args: argparse.Namespace) -> None:
    """Initialize a configuration file with project-specific defaults."""
    config_path = "readme_generator_config.json"
    
    try:
        # Create configuration with research-based defaults
        config = Config()
        config.model_name = "gemini_2_5_pro"  # Research recommendation
        config.max_tokens = 1_000_000  # Gemini 2.5 Pro context window
        config.timeout = 90  # Performance requirement
        
        save_config(config, config_path)
        
        print(f"✓ Configuration file created: {config_path}")
        print("\nRecommended model settings based on research:")
        print("  - Gemini 2.5 Pro: Best balance of cost, context (1M tokens), and coding accuracy")
        print("  - GPT-4o-mini: Most cost-efficient option")
        print("  - Claude Sonnet: Highest coding accuracy (72.7% SWE-bench)")
        print(f"\nEdit {config_path} to customize settings, then use:")
        print(f"readme-generator /path/to/project --config {config_path}")
        
    except Exception as e:
        raise CLIError(f"Failed to create configuration file: {e}")


async def parse_and_generate_command(args: argparse.Namespace, config: Config) -> None:
    """Execute the main project parsing and README generation command."""
    start_time = time.time()
    
    try:
        project_path = Path(args.project_path).resolve()
        
        logger.info(f"Parsing project: {project_path}")
        logger.info(f"Token budget: {config.max_tokens:,}")
        
        # Parse the project
        parsing_start = time.time()
        result = parse_project(
            project_path=str(project_path),
            model_name=config.model_name,
            max_tokens=config.max_tokens,
            include_tests=config.include_tests,
            include_private=config.include_private
        )
        parsing_time = time.time() - parsing_start
        
        if not result.success:
            raise CLIError(f"Project parsing failed: {'; '.join(result.errors)}")
        
        # Display parsing results
        if not config.quiet:
            print(f"✓ Project parsed successfully in {parsing_time:.2f}s")
            print(f"  Files processed: {result.stats.get('files_processed', 0)}")
            print(f"  Lines processed: {result.stats.get('lines_processed', 0):,}")
            print(f"  Classes found: {result.stats.get('classes_found', 0)}")
            print(f"  Functions found: {result.stats.get('functions_found', 0)}")
            print(f"  Examples found: {result.stats.get('examples_found', 0)}")
            print(f"  Token count: {result.project_data.token_count:,}")
            
            if result.warnings:
                print(f"  Warnings: {len(result.warnings)}")
                if config.verbose:
                    for warning in result.warnings:
                        print(f"    - {warning}")
        
        # Always save JSON output (either to specified file or default)
        json_output_path = args.json_output or f"{project_path.name}_parsed_data.json"
        serialized_data = serialize_project_data(result.project_data)
        success = save_json_to_file(serialized_data, json_output_path)
        
        if success and not config.quiet:
            print(f"✓ JSON data saved to: {json_output_path}")
        
        # Skip README generation if parse-only mode
        if args.parse_only:
            total_time = time.time() - start_time
            if not config.quiet:
                print(f"✓ Parse-only mode completed in {total_time:.2f}s")
            return
        
        # README generation (requires API key)
        api_key = validate_api_requirements(config.model_name, config.api_key)
        
        if not config.quiet:
            print(f"\nGenerating README with {config.model_name}...")
        
        generation_start = time.time()
        readme_content = await generate_readme_with_llm(result.project_data, config, api_key)
        generation_time = time.time() - generation_start
        
        # Save README file
        output_path = Path(args.output)
        if not output_path.is_absolute():
            output_path = project_path / output_path
        
        try:
            create_directory(str(output_path.parent))
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            
            total_time = time.time() - start_time
            
            if not config.quiet:
                print(f"✓ README generated in {generation_time:.2f}s")
                print(f"✓ Total time: {total_time:.2f}s")
                print(f"✓ README saved to: {output_path}")
            
        except Exception as e:
            raise CLIError(f"Failed to write README file: {e}")
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"Command failed after {elapsed_time:.2f}s: {e}")
        raise



def print_error(message: str) -> None:
    """Print error message to stderr."""
    print(f"Error: {message}", file=sys.stderr)


def print_warning(message: str) -> None:
    """Print warning message to stderr."""
    print(f"Warning: {message}", file=sys.stderr)


def main() -> None:
    """Main CLI entry point."""
    try:
        # Parse arguments
        parser = create_parser()
        args = parser.parse_args()
        
        # Handle special commands
        if args.init_config:
            init_config_command(args)
            return
        
        # Validate arguments
        validate_arguments(args)
        
        # Load configuration
        config = load_configuration(args)
        
        # Add parse-only flag to config
        config.parse_only = getattr(args, 'parse_only', False)
        
        # Set up logging level based on config
        if config.debug:
            logging.getLogger().setLevel(logging.DEBUG)
        elif config.verbose:
            logging.getLogger().setLevel(logging.INFO)
        elif config.quiet:
            logging.getLogger().setLevel(logging.ERROR)
        
        # Save configuration if requested
        if args.save_config:
            save_config(config, args.save_config)
            if not config.quiet:
                print(f"✓ Configuration saved to: {args.save_config}")
        
        # Execute main command
        asyncio.run(parse_and_generate_command(args, config))
        
    except CLIError as e:
        print_error(str(e))
        sys.exit(1)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        print_error(f"An unexpected error occurred: {e}")
        sys.exit(1)



if __name__ == "__main__":
    main()


File: /workspaces/SYSC4918/src/__init__.py
"""
README Generator - Automated README generation for Python projects.

This package provides a command-line tool that automatically generates comprehensive
README files for Python projects using Large Language Model APIs. It intelligently
parses and extracts key information from codebases to create high-quality documentation
within LLM context window constraints.

Features:
- Comprehensive project analysis and parsing
- Token-aware content optimization for LLM APIs
- Multiple LLM provider support (Gemini, OpenAI, Claude)
- Intelligent content prioritization
- Error handling and graceful degradation
"""

from .parser.project_parser import parse_project, parse_project_to_json
from .utils.token_counter import estimate_tokens, count_tokens_in_text
from .utils.content_prioritizer import prioritize_project_data
from .utils.json_serializer import serialize_project_data
from .config import Config, load_config, save_config

__version__ = "0.1.0"
__author__ = "Your Name"
__email__ = "your.email@example.com"
__description__ = "Automated README generation for Python projects using LLM APIs"
__url__ = "https://github.com/yourusername/readme-generator"

# Main API exports
__all__ = [
    # Core parsing functions
    "parse_project",
    "parse_project_to_json",
    
    # Utility functions
    "estimate_tokens",
    "count_tokens_in_text",
    "prioritize_project_data",
    "serialize_project_data",
    
    # Configuration
    "Config",
    "load_config",
    "save_config",
    
    # Metadata
    "__version__",
    "__author__",
    "__email__",
    "__description__",
    "__url__",
]

# Package-level configuration
DEFAULT_CONFIG = {
    "model_name": "gemini_2_5_pro",
    "max_tokens": 1_000_000,
    "include_tests": False,
    "include_private": False,
    "output_format": "markdown",
    "verbose": False,
    "cache_enabled": True,
    "timeout": 90,
}

def get_version() -> str:
    """Get the current version of the package."""
    return __version__

def get_package_info() -> dict:
    """Get comprehensive package information."""
    return {
        "name": "readme-generator",
        "version": __version__,
        "author": __author__,
        "email": __email__,
        "description": __description__,
        "url": __url__,
        "python_requires": ">=3.8",
        "license": "MIT",
    }


File: /workspaces/SYSC4918/src/__main__.py
"""
CLI entry point for the README generator package.

This module provides the main entry point when the package is run as a module
using 'python -m readme_generator'. It handles command-line argument parsing
and delegates to the appropriate CLI functions.
"""

import sys
import os
import logging
from pathlib import Path

# Add the package to Python path if running as script
if __name__ == "__main__":
    # Get the directory containing this file
    current_dir = Path(__file__).parent
    # Add parent directory to path so we can import the package
    sys.path.insert(0, str(current_dir.parent))

from cli import main


def setup_logging(verbose: bool = False):
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Set up console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(console_handler)
    
    # Reduce noise from third-party libraries
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('requests').setLevel(logging.WARNING)
    logging.getLogger('httpx').setLevel(logging.WARNING)


def check_python_version():
    """Check if Python version is compatible."""
    if sys.version_info < (3, 8):
        print("Error: Python 3.8 or higher is required.")
        print(f"You are using Python {sys.version}")
        sys.exit(1)


def handle_exceptions():
    """Set up global exception handling."""
    def exception_handler(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            print("\nOperation cancelled by user.")
            sys.exit(1)
        else:
            # Log the exception
            logging.error(
                "Uncaught exception", 
                exc_info=(exc_type, exc_value, exc_traceback)
            )
            print(f"An unexpected error occurred: {exc_value}")
            sys.exit(1)
    
    sys.excepthook = exception_handler


def entry_point():
    """Main entry point for the CLI application."""
    try:
        # Check Python version compatibility
        check_python_version()
        
        # Set up global exception handling
        handle_exceptions()
        
        # Parse arguments and determine verbosity early
        verbose = '--verbose' in sys.argv or '-v' in sys.argv
        
        # Set up logging
        setup_logging(verbose)
        
        # Log startup information
        logger = logging.getLogger(__name__)
        logger.info(f"Starting README Generator v{get_version()}")
        logger.debug(f"Python version: {sys.version}")
        logger.debug(f"Command line arguments: {sys.argv}")
        
        # Run the main CLI function
        main()
        
    except Exception as e:
        print(f"Failed to start README Generator: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # Import version info
    try:
        from readme_generator import get_version
    except ImportError:
        def get_version():
            return "0.1.0"
    
    entry_point()


File: /workspaces/SYSC4918/src/config.py
"""
Configuration management for the README generator.

This module handles loading, saving, and managing configuration settings
for the README generation tool, including model settings, parsing options,
and output preferences.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass, asdict, field
import logging

logger = logging.getLogger(__name__)


@dataclass
class Config:
    """Configuration settings for the README generator."""
    
    # Model configuration
    model_name: str = "gemini_2_5_pro"
    max_tokens: int = 1_000_000
    api_key: Optional[str] = None
    api_base_url: Optional[str] = None
    
    # Parsing options
    include_tests: bool = False
    include_private: bool = False
    include_docs: bool = True
    
    # Output options
    output_format: str = "markdown"
    output_file: str = "README.md"
    template_path: Optional[str] = None
    
    # Performance settings
    timeout: int = 90
    max_file_size: int = 10 * 1024 * 1024  # 10MB
    cache_enabled: bool = True
    cache_dir: Optional[str] = None
    
    # Logging and debugging
    verbose: bool = False
    quiet: bool = False
    debug: bool = False
    log_file: Optional[str] = None
    
    # Token management
    token_budget_allocation: Dict[str, float] = field(default_factory=lambda: {
        "metadata": 0.005,
        "dependencies": 0.01,
        "structure": 0.05,
        "api_documentation": 0.60,
        "examples": 0.20,
        "configuration": 0.035,
        "buffer": 0.10,
    })
    
    # Content prioritization
    priority_weights: Dict[str, float] = field(default_factory=lambda: {
        "public_classes": 10,
        "public_functions": 9,
        "main_entry_points": 10,
        "well_documented": 8,
        "has_examples": 9,
        "framework_related": 7,
        "configuration": 6,
        "private_methods": 3,
        "test_files": 4,
    })
    
    # Model-specific settings
    model_settings: Dict[str, Any] = field(default_factory=lambda: {
        "temperature": 0.7,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "max_retries": 3,
        "retry_delay": 1.0,
    })
    
    # File patterns
    ignore_patterns: list = field(default_factory=lambda: [
        ".git", ".svn", ".hg", ".bzr",
        "__pycache__", "*.pyc", "*.pyo", "*.pyd",
        "venv", "env", ".venv", ".env", "virtualenv",
        ".vscode", ".idea", "*.swp", "*.swo",
        "build", "dist", "*.egg-info", ".tox",
        ".DS_Store", "Thumbs.db",
        "*.tmp", "*.temp", "*.log"
    ])
    
    def __post_init__(self):
        """Post-initialization setup."""
        # Set up cache directory
        if self.cache_dir is None:
            self.cache_dir = str(Path.home() / ".readme_generator" / "cache")
        
        # Load API key from environment if not set
        if self.api_key is None:
            self.api_key = self._get_api_key_from_env()
    
    def _get_api_key_from_env(self) -> Optional[str]:
        """Get API key from environment variables."""
        env_keys = {
            "gemini_2_5_pro": "GEMINI_API_KEY",
            "gemini_2_5_flash": "GEMINI_API_KEY",
            "gpt_4o": "OPENAI_API_KEY",
            "gpt_4o_mini": "OPENAI_API_KEY",
            "claude_sonnet": "ANTHROPIC_API_KEY",
        }
        
        env_key = env_keys.get(self.model_name)
        if env_key:
            return os.getenv(env_key)
        
        return None
    
    def update(self, other: Union['Config', Dict[str, Any]]) -> None:
        """Update configuration with values from another config or dictionary."""
        if isinstance(other, Config):
            other_dict = asdict(other)
        else:
            other_dict = other
        
        for key, value in other_dict.items():
            if hasattr(self, key):
                # Handle nested dictionaries
                if isinstance(getattr(self, key), dict) and isinstance(value, dict):
                    getattr(self, key).update(value)
                else:
                    setattr(self, key, value)
    
    def validate(self) -> None:
        """Validate configuration settings."""
        errors = []
        
        # Validate model name
        valid_models = [
            "gemini_2_5_pro", "gemini_2_5_flash", 
            "gpt_4o", "gpt_4o_mini", "claude_sonnet"
        ]
        if self.model_name not in valid_models:
            errors.append(f"Invalid model name: {self.model_name}")
        
        # Validate token limits
        if self.max_tokens < 1000:
            errors.append("max_tokens must be at least 1000")
        
        # Validate timeout
        if self.timeout < 1:
            errors.append("timeout must be at least 1 second")
        
        # Validate output format
        valid_formats = ["markdown", "json"]
        if self.output_format not in valid_formats:
            errors.append(f"Invalid output format: {self.output_format}")
        
        # Validate token budget allocation
        budget_sum = sum(self.token_budget_allocation.values())
        if abs(budget_sum - 1.0) > 0.001:  # Allow small floating point errors
            errors.append(f"Token budget allocation must sum to 1.0, got {budget_sum}")
        
        if errors:
            raise ValueError("Configuration validation failed:\n" + "\n".join(f"  - {error}" for error in errors))
    
    def get_cache_path(self) -> Path:
        """Get the cache directory path."""
        cache_path = Path(self.cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)
        return cache_path
    
    def get_model_config(self) -> Dict[str, Any]:
        """Get model-specific configuration."""
        base_config = {
            "model_name": self.model_name,
            "max_tokens": self.max_tokens,
            "api_key": self.api_key,
            "api_base_url": self.api_base_url,
            "timeout": self.timeout,
        }
        
        base_config.update(self.model_settings)
        return base_config
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Config':
        """Create configuration from dictionary."""
        # Filter out unknown keys
        valid_keys = {field.name for field in cls.__dataclass_fields__.values()}
        filtered_data = {k: v for k, v in data.items() if k in valid_keys}
        
        return cls(**filtered_data)


def get_default_config_path() -> Path:
    """Get the default configuration file path."""
    return Path.home() / ".readme_generator" / "config.json"


def load_config(config_path: Optional[Union[str, Path]] = None) -> Config:
    """
    Load configuration from file.
    
    Args:
        config_path: Path to configuration file. If None, uses default location.
        
    Returns:
        Config object loaded from file
        
    Raises:
        FileNotFoundError: If configuration file doesn't exist
        ValueError: If configuration file is invalid
    """
    if config_path is None:
        config_path = get_default_config_path()
    else:
        config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        config = Config.from_dict(data)
        config.validate()
        
        logger.info(f"Loaded configuration from {config_path}")
        return config
        
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in configuration file: {e}")
    except Exception as e:
        raise ValueError(f"Failed to load configuration: {e}")


def save_config(config: Config, config_path: Optional[Union[str, Path]] = None) -> None:
    """
    Save configuration to file.
    
    Args:
        config: Configuration object to save
        config_path: Path to save configuration file. If None, uses default location.
        
    Raises:
        ValueError: If configuration is invalid
        IOError: If unable to write to file
    """
    if config_path is None:
        config_path = get_default_config_path()
    else:
        config_path = Path(config_path)
    
    # Validate configuration before saving
    config.validate()
    
    # Create directory if it doesn't exist
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config.to_dict(), f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved configuration to {config_path}")
        
    except Exception as e:
        raise IOError(f"Failed to save configuration: {e}")


def create_default_config() -> Config:
    """Create a default configuration object."""
    return Config()


def get_config_template() -> Dict[str, Any]:
    """Get a configuration template with comments."""
    return {
        "_comment": "README Generator Configuration File",
        "_version": "1.0.0",
        "_description": "Configuration settings for the README generator tool",
        
        "model_name": "gemini_2_5_pro",
        "max_tokens": 1000000,
        "api_key": None,
        "api_base_url": None,
        
        "include_tests": False,
        "include_private": False,
        "include_docs": True,
        
        "output_format": "markdown",
        "output_file": "README.md",
        "template_path": None,
        
        "timeout": 90,
        "max_file_size": 10485760,
        "cache_enabled": True,
        "cache_dir": None,
        
        "verbose": False,
        "quiet": False,
        "debug": False,
        "log_file": None,
        
        "token_budget_allocation": {
            "metadata": 0.005,
            "dependencies": 0.01,
            "structure": 0.05,
            "api_documentation": 0.60,
            "examples": 0.20,
            "configuration": 0.035,
            "buffer": 0.10
        },
        
        "priority_weights": {
            "public_classes": 10,
            "public_functions": 9,
            "main_entry_points": 10,
            "well_documented": 8,
            "has_examples": 9,
            "framework_related": 7,
            "configuration": 6,
            "private_methods": 3,
            "test_files": 4
        },
        
        "model_settings": {
            "temperature": 0.7,
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "max_retries": 3,
            "retry_delay": 1.0
        },
        
        "ignore_patterns": [
            ".git", ".svn", ".hg", ".bzr",
            "__pycache__", "*.pyc", "*.pyo", "*.pyd",
            "venv", "env", ".venv", ".env", "virtualenv",
            ".vscode", ".idea", "*.swp", "*.swo",
            "build", "dist", "*.egg-info", ".tox",
            ".DS_Store", "Thumbs.db",
            "*.tmp", "*.temp", "*.log"
        ]
    }


def merge_configs(base_config: Config, override_config: Dict[str, Any]) -> Config:
    """
    Merge two configurations, with override taking precedence.
    
    Args:
        base_config: Base configuration object
        override_config: Dictionary of override values
        
    Returns:
        New Config object with merged values
    """
    merged_data = base_config.to_dict()
    
    for key, value in override_config.items():
        if key in merged_data:
            if isinstance(merged_data[key], dict) and isinstance(value, dict):
                merged_data[key].update(value)
            else:
                merged_data[key] = value
    
    return Config.from_dict(merged_data)


# Convenience functions for common operations
def init_config_file(config_path: Optional[Union[str, Path]] = None, 
                    template: bool = False) -> None:
    """
    Initialize a configuration file with default values.
    
    Args:
        config_path: Path to create configuration file
        template: Whether to create a template with comments
    """
    if config_path is None:
        config_path = get_default_config_path()
    else:
        config_path = Path(config_path)
    
    # Create directory if it doesn't exist
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    if template:
        config_data = get_config_template()
    else:
        config_data = create_default_config().to_dict()
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Created configuration file: {config_path}")


def validate_config_file(config_path: Union[str, Path]) -> bool:
    """
    Validate a configuration file.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        True if valid, False otherwise
    """
    try:
        load_config(config_path)
        return True
    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        return False


File: /workspaces/SYSC4918/src/utils/token_counter.py
"""
Token counting and estimation utilities for managing LLM context windows.

This module provides functions to count tokens, estimate content size,
and manage the 1 million token budget for Gemini 2.5 Pro integration.
"""

import re
import json
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)

# Token estimation ratios for different content types
TOKEN_RATIOS = {
    'code': 0.3,        # Code is more token-dense
    'docstring': 0.25,  # Documentation is typically less dense
    'comment': 0.25,    # Comments are similar to documentation
    'text': 0.25,       # Regular text
    'json': 0.3,        # JSON structure adds overhead
    'markdown': 0.25,   # Markdown formatting
}

# Maximum token limits
MAX_TOKENS = {
    'gemini_2_5_pro': 1_000_000,
    'gemini_2_5_flash': 1_000_000,
    'gpt_4o': 128_000,
    'gpt_4o_mini': 128_000,
    'claude_sonnet': 200_000,
}

# Token budget allocation percentages
DEFAULT_BUDGET_ALLOCATION = {
    'metadata': 0.005,          # 5,000 tokens
    'dependencies': 0.01,       # 10,000 tokens
    'structure': 0.05,          # 50,000 tokens
    'api_documentation': 0.60,  # 600,000 tokens
    'examples': 0.20,           # 200,000 tokens
    'configuration': 0.035,     # 35,000 tokens
    'buffer': 0.10,             # 100,000 tokens (for LLM prompt)
}


class ContentType(Enum):
    """Types of content for token estimation."""
    CODE = "code"
    DOCSTRING = "docstring"
    COMMENT = "comment"
    TEXT = "text"
    JSON = "json"
    MARKDOWN = "markdown"


@dataclass
class TokenBudget:
    """Token budget allocation for different content types."""
    
    total_budget: int
    metadata: int
    dependencies: int
    structure: int
    api_documentation: int
    examples: int
    configuration: int
    buffer: int
    
    def get_used_budget(self) -> int:
        """Get total allocated budget."""
        return (self.metadata + self.dependencies + self.structure + 
                self.api_documentation + self.examples + self.configuration)
    
    def get_remaining_budget(self) -> int:
        """Get remaining budget after allocations."""
        return self.total_budget - self.get_used_budget() - self.buffer


class TokenCounter:
    """Advanced token counter with content-aware estimation."""
    
    def __init__(self, model_name: str = "gemini_2_5_pro"):
        self.model_name = model_name
        self.max_tokens = MAX_TOKENS.get(model_name, 1_000_000)
        self._token_cache: Dict[str, int] = {}
        
        # Initialize tiktoken if available (for more accurate counting)
        self.tiktoken_encoder = None
        try:
            import tiktoken
            if model_name.startswith('gpt'):
                self.tiktoken_encoder = tiktoken.encoding_for_model(model_name)
            else:
                # Use cl100k_base for other models as approximation
                self.tiktoken_encoder = tiktoken.get_encoding("cl100k_base")
        except ImportError:
            logger.warning("tiktoken not available, using estimation")
        except Exception as e:
            logger.warning(f"Error initializing tiktoken: {e}")
    
    def count_tokens(self, text: str, content_type: ContentType = ContentType.TEXT) -> int:
        """
        Count tokens in text with content-type awareness.
        
        Args:
            text: Text to count tokens for
            content_type: Type of content for better estimation
            
        Returns:
            Estimated token count
        """
        if not text:
            return 0
            
        # Use cached result if available
        cache_key = f"{content_type.value}:{hash(text)}"
        if cache_key in self._token_cache:
            return self._token_cache[cache_key]
        
        # Use tiktoken if available
        if self.tiktoken_encoder:
            try:
                token_count = len(self.tiktoken_encoder.encode(text))
                self._token_cache[cache_key] = token_count
                return token_count
            except Exception as e:
                logger.warning(f"Error with tiktoken encoding: {e}")
        
        # Fallback to estimation
        token_count = self._estimate_tokens(text, content_type)
        self._token_cache[cache_key] = token_count
        return token_count
    
    def _estimate_tokens(self, text: str, content_type: ContentType) -> int:
        """
        Estimate tokens using heuristics.
        
        Args:
            text: Text to estimate
            content_type: Type of content
            
        Returns:
            Estimated token count
        """
        # Basic character-to-token ratio
        char_count = len(text)
        base_ratio = TOKEN_RATIOS.get(content_type.value, 0.25)
        
        # Adjust for various factors
        adjustments = 0
        
        # More tokens for code due to symbols and structure
        if content_type == ContentType.CODE:
            symbol_count = len(re.findall(r'[{}()\[\],;:.]', text))
            adjustments += symbol_count * 0.5
        
        # More tokens for JSON due to structure
        elif content_type == ContentType.JSON:
            brace_count = text.count('{') + text.count('[')
            adjustments += brace_count * 2
        
        # Adjust for whitespace (whitespace is often tokenized separately)
        whitespace_count = len(re.findall(r'\s+', text))
        adjustments += whitespace_count * 0.1
        
        # Adjust for long words (often split into multiple tokens)
        long_words = len(re.findall(r'\b\w{8,}\b', text))
        adjustments += long_words * 0.5
        
        estimated_tokens = int((char_count * base_ratio) + adjustments)
        return max(1, estimated_tokens)  # Minimum 1 token
    
    def count_tokens_in_dict(self, data: Dict[str, Any], content_type: ContentType = ContentType.JSON) -> int:
        """
        Count tokens in a dictionary/JSON structure.
        
        Args:
            data: Dictionary to count tokens for
            content_type: Type of content
            
        Returns:
            Estimated token count
        """
        json_str = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        return self.count_tokens(json_str, content_type)
    
    def get_budget_allocation(self, custom_allocation: Optional[Dict[str, float]] = None) -> TokenBudget:
        """
        Get token budget allocation.
        
        Args:
            custom_allocation: Custom allocation percentages
            
        Returns:
            TokenBudget object with allocations
        """
        allocation = custom_allocation or DEFAULT_BUDGET_ALLOCATION
        
        return TokenBudget(
            total_budget=self.max_tokens,
            metadata=int(self.max_tokens * allocation['metadata']),
            dependencies=int(self.max_tokens * allocation['dependencies']),
            structure=int(self.max_tokens * allocation['structure']),
            api_documentation=int(self.max_tokens * allocation['api_documentation']),
            examples=int(self.max_tokens * allocation['examples']),
            configuration=int(self.max_tokens * allocation['configuration']),
            buffer=int(self.max_tokens * allocation['buffer'])
        )


def estimate_tokens(text: str, content_type: str = "text") -> int:
    """
    Simple token estimation function.
    
    Args:
        text: Text to estimate tokens for
        content_type: Type of content (code, text, json, etc.)
        
    Returns:
        Estimated token count
    """
    counter = TokenCounter()
    try:
        content_enum = ContentType(content_type.lower())
    except ValueError:
        content_enum = ContentType.TEXT
    
    return counter.count_tokens(text, content_enum)


def count_tokens_in_text(text: str, model_name: str = "gemini_2_5_pro") -> int:
    """
    Count tokens in text for a specific model.
    
    Args:
        text: Text to count tokens for
        model_name: Name of the model
        
    Returns:
        Token count
    """
    counter = TokenCounter(model_name)
    return counter.count_tokens(text)


def get_token_budget_allocation(model_name: str = "gemini_2_5_pro") -> TokenBudget:
    """
    Get default token budget allocation.
    
    Args:
        model_name: Name of the model
        
    Returns:
        TokenBudget object
    """
    counter = TokenCounter(model_name)
    return counter.get_budget_allocation()


def optimize_content_for_tokens(content: str, max_tokens: int, content_type: str = "text") -> str:
    """
    Optimize content to fit within token limit.
    
    Args:
        content: Content to optimize
        max_tokens: Maximum allowed tokens
        content_type: Type of content
        
    Returns:
        Optimized content
    """
    counter = TokenCounter()
    try:
        content_enum = ContentType(content_type.lower())
    except ValueError:
        content_enum = ContentType.TEXT
    
    current_tokens = counter.count_tokens(content, content_enum)
    
    if current_tokens <= max_tokens:
        return content
    
    # Calculate reduction ratio
    reduction_ratio = max_tokens / current_tokens
    
    # Reduce content length
    target_length = int(len(content) * reduction_ratio * 0.9)  # 90% to be safe
    
    if content_type == "code":
        # For code, try to preserve structure
        lines = content.split('\n')
        total_chars = sum(len(line) for line in lines)
        
        result_lines = []
        current_chars = 0
        
        for line in lines:
            if current_chars + len(line) <= target_length:
                result_lines.append(line)
                current_chars += len(line)
            else:
                # Add ellipsis to indicate truncation
                result_lines.append("# ... (truncated)")
                break
        
        return '\n'.join(result_lines)
    
    else:
        # For text, simple truncation with ellipsis
        return content[:target_length] + "..." if len(content) > target_length else content


def analyze_token_distribution(data: Dict[str, Any]) -> Dict[str, int]:
    """
    Analyze token distribution across different data sections.
    
    Args:
        data: Data to analyze
        
    Returns:
        Dictionary with token counts per section
    """
    counter = TokenCounter()
    distribution = {}
    
    for key, value in data.items():
        if isinstance(value, str):
            distribution[key] = counter.count_tokens(value, ContentType.TEXT)
        elif isinstance(value, (dict, list)):
            distribution[key] = counter.count_tokens_in_dict(value if isinstance(value, dict) else {"data": value})
        else:
            distribution[key] = counter.count_tokens(str(value), ContentType.TEXT)
    
    return distribution


def format_token_summary(token_count: int, budget: TokenBudget) -> str:
    """
    Format a human-readable token usage summary.
    
    Args:
        token_count: Current token count
        budget: Token budget
        
    Returns:
        Formatted summary string
    """
    percentage = (token_count / budget.total_budget) * 100
    remaining = budget.total_budget - token_count
    
    return f"""
Token Usage Summary:
- Used: {token_count:,} tokens ({percentage:.1f}%)
- Remaining: {remaining:,} tokens
- Budget: {budget.total_budget:,} tokens
- Status: {'✓ Within budget' if token_count <= budget.total_budget else '⚠ Over budget'}
"""


File: /workspaces/SYSC4918/src/utils/__init__.py
"""
Utility modules for the README generator project.

This package contains utility functions for file operations, token counting,
content prioritization, and JSON serialization used throughout the project.
"""

from .file_utils import (
    read_file_safely,
    find_files_by_pattern,
    get_project_files,
    detect_encoding,
    is_python_file,
    get_file_size,
    create_directory,
    FileReader
)

from .token_counter import (
    TokenCounter,
    estimate_tokens,
    count_tokens_in_text,
    get_token_budget_allocation,
    optimize_content_for_tokens
)

from .content_prioritizer import (
    ContentPrioritizer,
    PriorityScore,
    prioritize_project_data,
    filter_content_by_priority,
    compress_content_for_budget
)

from .json_serializer import (
    ProjectDataSerializer,
    serialize_project_data,
    validate_json_output,
    format_json_output,
    save_json_to_file
)

__all__ = [
    # File utilities
    "read_file_safely",
    "find_files_by_pattern",
    "get_project_files",
    "detect_encoding",
    "is_python_file",
    "get_file_size",
    "create_directory",
    "FileReader",
    
    # Token counting
    "TokenCounter",
    "estimate_tokens",
    "count_tokens_in_text",
    "get_token_budget_allocation",
    "optimize_content_for_tokens",
    
    # Content prioritization
    "ContentPrioritizer",
    "PriorityScore",
    "prioritize_project_data",
    "filter_content_by_priority",
    "compress_content_for_budget",
    
    # JSON serialization
    "ProjectDataSerializer",
    "serialize_project_data",
    "validate_json_output",
    "format_json_output",
    "save_json_to_file"
]

__version__ = "0.1.0"


File: /workspaces/SYSC4918/src/utils/json_serializer.py
"""
JSON serialization utilities for project data output.

This module provides functions to serialize project data to JSON format
with proper formatting, validation, and error handling.
"""

import json
import datetime
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from dataclasses import asdict, is_dataclass
from enum import Enum
import logging

from models.project_data import ProjectData, ProjectType, LicenseType
from models.schemas import validate_project_data, PROJECT_DATA_SCHEMA
from .token_counter import TokenCounter, ContentType

logger = logging.getLogger(__name__)


class ProjectDataEncoder(json.JSONEncoder):
    """Custom JSON encoder for project data objects."""
    
    def default(self, obj):
        """Handle special object types for JSON serialization."""
        if is_dataclass(obj):
            return asdict(obj)
        elif isinstance(obj, Enum):
            return obj.value
        elif isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, datetime.datetime):
            return obj.isoformat()
        elif isinstance(obj, set):
            return list(obj)
        elif hasattr(obj, '__dict__'):
            return obj.__dict__
        else:
            return super().default(obj)


class ProjectDataSerializer:
    """Advanced project data serializer with validation and optimization."""
    
    def __init__(self, token_counter: Optional[TokenCounter] = None):
        self.token_counter = token_counter or TokenCounter()
        self.encoder = ProjectDataEncoder()
        
    def serialize(self, project_data: ProjectData, 
                  validate: bool = True, 
                  optimize_for_tokens: bool = True) -> Dict[str, Any]:
        """
        Serialize project data to dictionary format.
        
        Args:
            project_data: Project data to serialize
            validate: Whether to validate against schema
            optimize_for_tokens: Whether to optimize for token usage
            
        Returns:
            Serialized project data dictionary
        """
        try:
            # Convert to dictionary
            data_dict = self._convert_to_dict(project_data)
            
            # Add metadata
            data_dict['serialization_info'] = {
                'timestamp': datetime.datetime.now().isoformat(),
                'version': '1.0.0',
                'token_optimized': optimize_for_tokens
            }
            
            # Optimize for tokens if requested
            if optimize_for_tokens:
                data_dict = self._optimize_for_tokens(data_dict)
            
            # Update token count
            data_dict['token_count'] = self._calculate_total_tokens(data_dict)
            
            # Validate if requested
            if validate:
                validation_result = validate_project_data(data_dict)
                if not validation_result.get('valid', True):
                    logger.warning(f"Validation warnings: {validation_result.get('errors', [])}")
            
            return data_dict
            
        except Exception as e:
            logger.error(f"Error serializing project data: {e}")
            raise
    
    def _convert_to_dict(self, project_data: ProjectData) -> Dict[str, Any]:
        """Convert project data to dictionary format."""
        return {
            'metadata': self._serialize_metadata(project_data.metadata),
            'dependencies': self._serialize_dependencies(project_data.dependencies),
            'structure': self._serialize_structure(project_data.structure),
            'configuration': self._serialize_configuration(project_data.configuration),
            'examples': self._serialize_examples(project_data.examples),
            'tests': self._serialize_tests(project_data.tests),
            'documentation': self._serialize_documentation(project_data.documentation),
            'parsing_errors': project_data.parsing_errors,
            'parsing_timestamp': project_data.parsing_timestamp
        }
    
    def _serialize_metadata(self, metadata) -> Dict[str, Any]:
        """Serialize metadata with proper type handling."""
        return {
            'project_name': metadata.project_name,
            'description': metadata.description,
            'version': metadata.version,
            'author': metadata.author,
            'author_email': metadata.author_email,
            'license': metadata.license.value if metadata.license else None,
            'homepage': metadata.homepage,
            'repository': metadata.repository,
            'python_version': metadata.python_version,
            'project_type': metadata.project_type.value,
            'keywords': metadata.keywords,
            'classifiers': metadata.classifiers
        }
    
    def _serialize_dependencies(self, dependencies) -> Dict[str, Any]:
        """Serialize dependencies information."""
        return {
            'production': dependencies.production,
            'development': dependencies.development,
            'optional': dependencies.optional,
            'python_requires': dependencies.python_requires,
            'extras_require': dependencies.extras_require
        }
    
    def _serialize_structure(self, structure) -> Dict[str, Any]:
        """Serialize project structure information."""
        return {
            'root_path': structure.root_path,
            'main_package': structure.main_package,
            'src_layout': structure.src_layout,
            'packages': structure.packages,
            'modules': [self._serialize_module(module) for module in structure.modules],
            'entry_points': [self._serialize_entry_point(ep) for ep in structure.entry_points],
            'config_files': structure.config_files,
            'data_directories': structure.data_directories,
            'test_directories': structure.test_directories,
            'doc_directories': structure.doc_directories,
            'total_files': structure.total_files,
            'total_lines': structure.total_lines
        }
    
    def _serialize_module(self, module) -> Dict[str, Any]:
        """Serialize module information."""
        return {
            'name': module.name,
            'file_path': module.file_path,
            'docstring': module.docstring,
            'classes': [self._serialize_class(cls) for cls in module.classes],
            'functions': [self._serialize_function(func) for func in module.functions],
            'constants': module.constants,
            'imports': module.imports,
            'is_package': module.is_package,
            'is_main': module.is_main,
            'line_count': module.line_count
        }
    
    def _serialize_class(self, class_info) -> Dict[str, Any]:
        """Serialize class information."""
        return {
            'name': class_info.name,
            'docstring': class_info.docstring,
            'methods': [self._serialize_function(method) for method in class_info.methods],
            'properties': [self._serialize_function(prop) for prop in class_info.properties],
            'inheritance': class_info.inheritance,
            'decorators': class_info.decorators,
            'is_abstract': class_info.is_abstract,
            'is_dataclass': class_info.is_dataclass,
            'is_enum': class_info.is_enum,
            'line_number': class_info.line_number,
            'file_path': class_info.file_path,
            'attributes': class_info.attributes
        }
    
    def _serialize_function(self, function) -> Dict[str, Any]:
        """Serialize function information."""
        return {
            'name': function.name,
            'signature': function.signature,
            'docstring': function.docstring,
            'is_public': function.is_public,
            'is_async': function.is_async,
            'is_property': function.is_property,
            'is_classmethod': function.is_classmethod,
            'is_staticmethod': function.is_staticmethod,
            'decorators': function.decorators,
            'line_number': function.line_number,
            'file_path': function.file_path,
            'return_type': function.return_type,
            'parameters': function.parameters,
            'complexity_score': function.complexity_score
        }
    
    def _serialize_entry_point(self, entry_point) -> Dict[str, Any]:
        """Serialize entry point information."""
        return {
            'name': entry_point.name,
            'module': entry_point.module,
            'function': entry_point.function,
            'script_path': entry_point.script_path,
            'description': entry_point.description
        }
    
    def _serialize_configuration(self, config) -> Dict[str, Any]:
        """Serialize configuration information."""
        if not config:
            return {}
            
        return {
            'config_files': config.config_files,
            'environment_variables': config.environment_variables,
            'default_settings': config.default_settings,
            'config_examples': [self._serialize_example(ex) for ex in config.config_examples]
        }
    
    def _serialize_examples(self, examples) -> List[Dict[str, Any]]:
        """Serialize examples list."""
        return [self._serialize_example(example) for example in examples]
    
    def _serialize_example(self, example) -> Dict[str, Any]:
        """Serialize a single example."""
        return {
            'title': example.title,
            'code': example.code,
            'description': example.description,
            'file_path': example.file_path,
            'line_number': example.line_number,
            'example_type': example.example_type,
            'language': example.language,
            'is_executable': example.is_executable,
            'expected_output': example.expected_output
        }
    
    def _serialize_tests(self, tests) -> Optional[Dict[str, Any]]:
        """Serialize test information."""
        if not tests:
            return None
            
        return {
            'test_directories': tests.test_directories,
            'test_files': tests.test_files,
            'test_framework': tests.test_framework,
            'coverage_files': tests.coverage_files,
            'total_tests': tests.total_tests
        }
    
    def _serialize_documentation(self, documentation) -> Optional[Dict[str, Any]]:
        """Serialize documentation information."""
        if not documentation:
            return None
            
        return {
            'readme_file': documentation.readme_file,
            'changelog_file': documentation.changelog_file,
            'license_file': documentation.license_file,
            'doc_directories': documentation.doc_directories,
            'doc_files': documentation.doc_files,
            'has_sphinx': documentation.has_sphinx,
            'has_mkdocs': documentation.has_mkdocs
        }
    
    def _optimize_for_tokens(self, data_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize data dictionary for token usage."""
        # Remove empty or None values
        cleaned_dict = self._remove_empty_values(data_dict)
        
        # Truncate long docstrings if needed
        cleaned_dict = self._truncate_long_content(cleaned_dict)
        
        # Prioritize content based on importance
        cleaned_dict = self._prioritize_content(cleaned_dict)
        
        return cleaned_dict
    
    def _remove_empty_values(self, data: Any) -> Any:
        """Recursively remove empty values from data structure."""
        if isinstance(data, dict):
            return {k: self._remove_empty_values(v) for k, v in data.items() 
                   if v is not None and v != [] and v != {}}
        elif isinstance(data, list):
            return [self._remove_empty_values(item) for item in data if item is not None]
        else:
            return data
    
    def _truncate_long_content(self, data_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Truncate long content to optimize token usage."""
        # Truncate very long docstrings
        if 'structure' in data_dict and 'modules' in data_dict['structure']:
            for module in data_dict['structure']['modules']:
                # Truncate module docstring
                if 'docstring' in module and module['docstring']:
                    module['docstring'] = self._truncate_text(module['docstring'], 500)
                
                # Truncate class docstrings
                for class_info in module.get('classes', []):
                    if 'docstring' in class_info and class_info['docstring']:
                        class_info['docstring'] = self._truncate_text(class_info['docstring'], 300)
                    
                    # Truncate method docstrings
                    for method in class_info.get('methods', []):
                        if 'docstring' in method and method['docstring']:
                            method['docstring'] = self._truncate_text(method['docstring'], 200)
                
                # Truncate function docstrings
                for function in module.get('functions', []):
                    if 'docstring' in function and function['docstring']:
                        function['docstring'] = self._truncate_text(function['docstring'], 200)
        
        return data_dict
    
    def _truncate_text(self, text: str, max_length: int) -> str:
        """Truncate text to maximum length."""
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."
    
    def _prioritize_content(self, data_dict: Dict[str, Any]) -> Dict[str, Any]:
        """Prioritize content based on importance."""
        # Keep only public APIs for classes and functions
        if 'structure' in data_dict and 'modules' in data_dict['structure']:
            for module in data_dict['structure']['modules']:
                # Filter classes to keep only public ones
                if 'classes' in module:
                    module['classes'] = [cls for cls in module['classes'] 
                                       if not cls.get('name', '').startswith('_')]
                
                # Filter functions to keep only public ones
                if 'functions' in module:
                    module['functions'] = [func for func in module['functions'] 
                                         if func.get('is_public', True)]
        
        return data_dict
    
    def _calculate_total_tokens(self, data_dict: Dict[str, Any]) -> int:
        """Calculate total token count for the serialized data."""
        json_str = json.dumps(data_dict, cls=ProjectDataEncoder, separators=(',', ':'))
        return self.token_counter.count_tokens(json_str, ContentType.JSON)


def serialize_project_data(project_data: ProjectData, 
                          validate: bool = True, 
                          optimize_for_tokens: bool = True) -> Dict[str, Any]:
    """
    Serialize project data to dictionary format.
    
    Args:
        project_data: Project data to serialize
        validate: Whether to validate against schema
        optimize_for_tokens: Whether to optimize for token usage
        
    Returns:
        Serialized project data dictionary
    """
    serializer = ProjectDataSerializer()
    return serializer.serialize(project_data, validate, optimize_for_tokens)


def validate_json_output(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate JSON output against schema.
    
    Args:
        data: Data to validate
        
    Returns:
        Validation result dictionary
    """
    return validate_project_data(data)


def format_json_output(data: Dict[str, Any], indent: int = 2) -> str:
    """
    Format data as JSON string with proper indentation.
    
    Args:
        data: Data to format
        indent: Number of spaces for indentation
        
    Returns:
        Formatted JSON string
    """
    return json.dumps(data, cls=ProjectDataEncoder, indent=indent, ensure_ascii=False)


def save_json_to_file(data: Dict[str, Any], 
                     file_path: str, 
                     indent: int = 2, 
                     validate: bool = True) -> bool:
    """
    Save data to JSON file.
    
    Args:
        data: Data to save
        file_path: Path to save file
        indent: JSON indentation
        validate: Whether to validate before saving
        
    Returns:
        True if successful, False otherwise
    """
    try:
        if validate:
            validation_result = validate_json_output(data)
            if not validation_result.get('valid', True):
                logger.warning(f"Validation warnings: {validation_result.get('errors', [])}")
        
        output_path = Path(file_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(format_json_output(data, indent))
        
        logger.info(f"Successfully saved JSON to {file_path}")
        return True
        
    except Exception as e:
        logger.error(f"Error saving JSON to {file_path}: {e}")
        return False


def load_json_from_file(file_path: str) -> Optional[Dict[str, Any]]:
    """
    Load JSON data from file.
    
    Args:
        file_path: Path to JSON file
        
    Returns:
        Loaded data or None if error
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading JSON from {file_path}: {e}")
        return None


def get_json_size_info(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Get size information for JSON data.
    
    Args:
        data: Data to analyze
        
    Returns:
        Size information dictionary
    """
    json_str = format_json_output(data, indent=None)
    
    token_counter = TokenCounter()
    
    return {
        'characters': len(json_str),
        'bytes': len(json_str.encode('utf-8')),
        'lines': json_str.count('\n') + 1,
        'tokens': token_counter.count_tokens(json_str, ContentType.JSON),
        'size_mb': len(json_str.encode('utf-8')) / (1024 * 1024)
    }


File: /workspaces/SYSC4918/src/utils/file_utils.py
"""
File system utilities for safe and efficient file operations.

This module provides functions for reading files, detecting encodings,
traversing directories, and handling various file types commonly found
in Python projects.
"""

import os
import re
import chardet
import mimetypes
from pathlib import Path
from typing import List, Optional, Dict, Generator, Set, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

# Common file patterns to ignore
IGNORE_PATTERNS = {
    # Version control
    '.git', '.svn', '.hg', '.bzr',
    # Python cache
    '__pycache__', '*.pyc', '*.pyo', '*.pyd',
    # Virtual environments
    'venv', 'env', '.venv', '.env', 'virtualenv',
    # IDE files
    '.vscode', '.idea', '*.swp', '*.swo',
    # Build artifacts
    'build', 'dist', '*.egg-info', '.tox',
    # OS files
    '.DS_Store', 'Thumbs.db',
    # Temporary files
    '*.tmp', '*.temp', '*.log'
}

# Python file extensions
PYTHON_EXTENSIONS = {'.py', '.pyw', '.pyx', '.pyi'}

# Configuration file patterns
CONFIG_FILES = {
    'setup.py', 'setup.cfg', 'pyproject.toml', 'requirements.txt',
    'requirements-dev.txt', 'Pipfile', 'Pipfile.lock', 'poetry.lock',
    'tox.ini', 'pytest.ini', '.flake8', '.pylintrc', 'mypy.ini',
    '.gitignore', '.gitattributes', 'MANIFEST.in', 'LICENSE', 'README.md',
    'README.rst', 'README.txt', 'CHANGELOG.md', 'CHANGELOG.rst'
}


@dataclass
class FileInfo:
    """Information about a file."""
    path: str
    size: int
    encoding: Optional[str] = None
    mime_type: Optional[str] = None
    is_text: bool = False
    is_python: bool = False
    is_config: bool = False


class FileReader:
    """Enhanced file reader with encoding detection and error handling."""
    
    def __init__(self, max_file_size: int = 10 * 1024 * 1024):  # 10MB default
        self.max_file_size = max_file_size
        self._encoding_cache: Dict[str, str] = {}
    
    def read_file(self, file_path: str) -> Optional[str]:
        """
        Read a file with automatic encoding detection.
        
        Args:
            file_path: Path to the file to read
            
        Returns:
            File content as string, or None if reading fails
        """
        try:
            path = Path(file_path)
            if not path.exists() or not path.is_file():
                logger.warning(f"File not found: {file_path}")
                return None
                
            if path.stat().st_size > self.max_file_size:
                logger.warning(f"File too large: {file_path} ({path.stat().st_size} bytes)")
                return None
                
            encoding = self.detect_encoding(file_path)
            if not encoding:
                logger.warning(f"Could not detect encoding for: {file_path}")
                return None
                
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                return f.read()
                
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            return None
    
    def detect_encoding(self, file_path: str) -> Optional[str]:
        """
        Detect file encoding using chardet.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Detected encoding or None
        """
        if file_path in self._encoding_cache:
            return self._encoding_cache[file_path]
            
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(8192)  # Read first 8KB for detection
                
            if not raw_data:
                return 'utf-8'
                
            result = chardet.detect(raw_data)
            encoding = result.get('encoding', 'utf-8')
            
            if encoding and result.get('confidence', 0) > 0.7:
                self._encoding_cache[file_path] = encoding
                return encoding
            else:
                # Fallback to common encodings
                for fallback in ['utf-8', 'latin-1', 'cp1252']:
                    try:
                        with open(file_path, 'r', encoding=fallback) as f:
                            f.read(1024)  # Try reading some content
                        self._encoding_cache[file_path] = fallback
                        return fallback
                    except UnicodeDecodeError:
                        continue
                        
        except Exception as e:
            logger.error(f"Error detecting encoding for {file_path}: {e}")
            
        return None


def read_file_safely(file_path: str, max_size: int = 10 * 1024 * 1024) -> Optional[str]:
    """
    Safely read a file with encoding detection and size limits.
    
    Args:
        file_path: Path to the file
        max_size: Maximum file size in bytes
        
    Returns:
        File content or None if reading fails
    """
    reader = FileReader(max_size)
    return reader.read_file(file_path)


def detect_encoding(file_path: str) -> Optional[str]:
    """
    Detect file encoding.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Detected encoding or None
    """
    reader = FileReader()
    return reader.detect_encoding(file_path)


def is_python_file(file_path: str) -> bool:
    """
    Check if a file is a Python file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        True if the file is a Python file
    """
    path = Path(file_path)
    
    # Check extension
    if path.suffix.lower() in PYTHON_EXTENSIONS:
        return True
        
    # Check shebang for extensionless files
    if not path.suffix:
        try:
            with open(file_path, 'rb') as f:
                first_line = f.readline(100).decode('utf-8', errors='ignore')
                if first_line.startswith('#!') and 'python' in first_line:
                    return True
        except Exception:
            pass
            
    return False


def is_text_file(file_path: str) -> bool:
    """
    Check if a file is a text file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        True if the file is likely a text file
    """
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type and mime_type.startswith('text/'):
        return True
        
    # Check for common text file extensions
    path = Path(file_path)
    text_extensions = {
        '.txt', '.md', '.rst', '.yaml', '.yml', '.json', '.xml',
        '.cfg', '.ini', '.toml', '.conf', '.config', '.env'
    }
    
    return path.suffix.lower() in text_extensions


def should_ignore_file(file_path: str, ignore_patterns: Optional[Set[str]] = None) -> bool:
    """
    Check if a file should be ignored based on patterns.
    
    Args:
        file_path: Path to the file
        ignore_patterns: Custom ignore patterns to use
        
    Returns:
        True if the file should be ignored
    """
    if ignore_patterns is None:
        ignore_patterns = IGNORE_PATTERNS
        
    path = Path(file_path)
    
    # Check if any part of the path matches ignore patterns
    for part in path.parts:
        for pattern in ignore_patterns:
            if '*' in pattern:
                # Handle glob patterns
                import fnmatch
                if fnmatch.fnmatch(part, pattern):
                    return True
            else:
                # Exact match
                if part == pattern:
                    return True
                    
    return False


def find_files_by_pattern(directory: str, pattern: str, recursive: bool = True) -> List[str]:
    """
    Find files matching a pattern in a directory.
    
    Args:
        directory: Directory to search
        pattern: File pattern to match
        recursive: Whether to search recursively
        
    Returns:
        List of matching file paths
    """
    import fnmatch
    
    matches = []
    directory = Path(directory)
    
    if not directory.exists():
        return matches
        
    search_pattern = "**/" + pattern if recursive else pattern
    
    try:
        for file_path in directory.glob(search_pattern):
            if file_path.is_file() and not should_ignore_file(str(file_path)):
                matches.append(str(file_path))
    except Exception as e:
        logger.error(f"Error finding files with pattern '{pattern}': {e}")
        
    return matches


def get_project_files(project_path: str, include_tests: bool = False) -> Dict[str, List[FileInfo]]:
    """
    Get categorized files from a project directory.
    
    Args:
        project_path: Path to the project root
        include_tests: Whether to include test files
        
    Returns:
        Dictionary of categorized file information
    """
    project_path = Path(project_path)
    if not project_path.exists():
        raise ValueError(f"Project path does not exist: {project_path}")
        
    files = {
        'python': [],
        'config': [],
        'documentation': [],
        'tests': [],
        'other': []
    }
    
    try:
        for root, dirs, filenames in os.walk(project_path):
            # Skip ignored directories
            dirs[:] = [d for d in dirs if not should_ignore_file(os.path.join(root, d))]
            
            for filename in filenames:
                file_path = os.path.join(root, filename)
                
                if should_ignore_file(file_path):
                    continue
                    
                # Skip test files if not requested
                if not include_tests and ('test' in filename.lower() or 'test' in Path(file_path).parent.name.lower()):
                    continue
                    
                file_info = _get_file_info(file_path)
                
                # Categorize files
                if file_info.is_python:
                    files['python'].append(file_info)
                elif file_info.is_config:
                    files['config'].append(file_info)
                elif filename.lower() in ['readme.md', 'readme.rst', 'readme.txt', 'changelog.md', 'changelog.rst']:
                    files['documentation'].append(file_info)
                elif 'test' in filename.lower():
                    files['tests'].append(file_info)
                else:
                    files['other'].append(file_info)
                    
    except Exception as e:
        logger.error(f"Error scanning project files: {e}")
        
    return files


def _get_file_info(file_path: str) -> FileInfo:
    """
    Get detailed information about a file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        FileInfo object with file details
    """
    path = Path(file_path)
    
    try:
        size = path.stat().st_size
        mime_type, _ = mimetypes.guess_type(file_path)
        
        file_info = FileInfo(
            path=str(path),
            size=size,
            mime_type=mime_type,
            is_text=is_text_file(file_path),
            is_python=is_python_file(file_path),
            is_config=path.name in CONFIG_FILES
        )
        
        if file_info.is_text:
            file_info.encoding = detect_encoding(file_path)
            
        return file_info
        
    except Exception as e:
        logger.error(f"Error getting file info for {file_path}: {e}")
        return FileInfo(path=str(path), size=0)


def get_file_size(file_path: str) -> int:
    """
    Get file size in bytes.
    
    Args:
        file_path: Path to the file
        
    Returns:
        File size in bytes, or 0 if error
    """
    try:
        return Path(file_path).stat().st_size
    except Exception:
        return 0


def create_directory(directory_path: str) -> bool:
    """
    Create a directory if it doesn't exist.
    
    Args:
        directory_path: Path to the directory
        
    Returns:
        True if directory was created or already exists
    """
    try:
        Path(directory_path).mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        logger.error(f"Error creating directory {directory_path}: {e}")
        return False


def get_relative_path(file_path: str, base_path: str) -> str:
    """
    Get relative path from base path.
    
    Args:
        file_path: Full path to file
        base_path: Base path to calculate relative from
        
    Returns:
        Relative path
    """
    try:
        return str(Path(file_path).relative_to(Path(base_path)))
    except ValueError:
        return file_path


File: /workspaces/SYSC4918/src/utils/content_prioritizer.py
"""
Content prioritization logic for managing token budgets and ensuring
the most important information is included in README generation.

This module implements sophisticated algorithms to prioritize, filter,
and compress content based on importance scores and token constraints.
"""

import re
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import logging

from models.project_data import ProjectData, ClassInfo, FunctionInfo, ModuleInfo
from .token_counter import TokenCounter, ContentType, TokenBudget

logger = logging.getLogger(__name__)


class PriorityLevel(Enum):
    """Priority levels for content."""
    CRITICAL = 10
    HIGH = 8
    MEDIUM = 6
    LOW = 4
    MINIMAL = 2


@dataclass
class PriorityScore:
    """Represents a priority score for a piece of content."""
    
    item_id: str
    item_type: str
    score: float
    token_count: int
    efficiency: float = field(init=False)
    reasons: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """Calculate efficiency score (priority per token)."""
        self.efficiency = self.score / max(self.token_count, 1)


class ContentPrioritizer:
    """Advanced content prioritization system."""
    
    def __init__(self, token_counter: Optional[TokenCounter] = None):
        self.token_counter = token_counter or TokenCounter()
        
        # Base priority scores for different content types
        self.base_priorities = {
            'public_class': 10,
            'public_function': 9,
            'main_entry_point': 10,
            'well_documented': 8,
            'has_examples': 9,
            'framework_related': 7,
            'configuration': 6,
            'private_method': 3,
            'test_code': 4,
            'utility_function': 5,
            'property': 6,
            'magic_method': 4,
            'deprecated': 2,
        }
        
        # Keywords that indicate importance
        self.important_keywords = {
            'main', 'init', 'setup', 'configure', 'run', 'execute', 'process',
            'create', 'build', 'generate', 'parse', 'validate', 'authenticate',
            'client', 'server', 'api', 'handler', 'manager', 'controller',
            'service', 'factory', 'builder', 'adapter', 'strategy'
        }
        
        # Framework detection patterns
        self.framework_patterns = {
            'flask': ['flask', 'route', 'blueprint', 'request', 'response'],
            'django': ['django', 'models', 'views', 'urls', 'forms'],
            'fastapi': ['fastapi', 'pydantic', 'basemodel', 'depends'],
            'sqlalchemy': ['sqlalchemy', 'declarative_base', 'session', 'query'],
            'pytest': ['pytest', 'fixture', 'parametrize', 'mark'],
            'click': ['click', 'command', 'option', 'argument'],
            'asyncio': ['asyncio', 'async', 'await', 'coroutine'],
        }
    
    def calculate_priority_scores(self, project_data: ProjectData) -> List[PriorityScore]:
        """
        Calculate priority scores for all content in project data.
        
        Args:
            project_data: Project data to prioritize
            
        Returns:
            List of priority scores sorted by importance
        """
        priority_scores = []
        
        # Process modules
        for module in project_data.structure.modules:
            module_scores = self._score_module(module)
            priority_scores.extend(module_scores)
        
        # Process examples
        for i, example in enumerate(project_data.examples):
            score = self._score_example(example, i)
            priority_scores.append(score)
        
        # Process configuration
        if project_data.configuration:
            config_score = self._score_configuration(project_data.configuration)
            priority_scores.append(config_score)
        
        # Sort by efficiency (priority per token)
        priority_scores.sort(key=lambda x: x.efficiency, reverse=True)
        
        return priority_scores
    
    def _score_module(self, module: ModuleInfo) -> List[PriorityScore]:
        """Score all content in a module."""
        scores = []
        
        # Score classes
        for class_info in module.classes:
            score = self._score_class(class_info, module)
            scores.append(score)
            
            # Score methods
            for method in class_info.methods:
                method_score = self._score_method(method, class_info, module)
                scores.append(method_score)
        
        # Score functions
        for function in module.functions:
            score = self._score_function(function, module)
            scores.append(score)
        
        return scores
    
    def _score_class(self, class_info: ClassInfo, module: ModuleInfo) -> PriorityScore:
        """Calculate priority score for a class."""
        base_score = self.base_priorities['public_class'] if class_info.name and not class_info.name.startswith('_') else 3
        
        reasons = []
        multiplier = 1.0
        
        # Check if well documented
        if class_info.docstring and len(class_info.docstring.strip()) > 50:
            multiplier *= 1.3
            reasons.append("well_documented")
        
        # Check for inheritance (likely important base classes)
        if class_info.inheritance:
            multiplier *= 1.2
            reasons.append("has_inheritance")
        
        # Check for framework patterns
        class_text = f"{class_info.name} {class_info.docstring or ''}"
        framework = self._detect_framework(class_text)
        if framework:
            multiplier *= 1.2
            reasons.append(f"framework_{framework}")
        
        # Check for important keywords
        if any(keyword in class_info.name.lower() for keyword in self.important_keywords):
            multiplier *= 1.3
            reasons.append("important_keyword")
        
        # Check if it's a main entry point
        if module.is_main and class_info.name in ['Main', 'App', 'Application']:
            multiplier *= 1.5
            reasons.append("main_entry_point")
        
        # Penalize private classes
        if class_info.name.startswith('_'):
            multiplier *= 0.5
            reasons.append("private_class")
        
        final_score = base_score * multiplier
        
        # Calculate token count
        content = f"{class_info.name}\n{class_info.docstring or ''}"
        token_count = self.token_counter.count_tokens(content, ContentType.CODE)
        
        return PriorityScore(
            item_id=f"class_{class_info.name}_{module.name}",
            item_type="class",
            score=final_score,
            token_count=token_count,
            reasons=reasons
        )
    
    def _score_method(self, method: FunctionInfo, class_info: ClassInfo, module: ModuleInfo) -> PriorityScore:
        """Calculate priority score for a method."""
        base_score = self.base_priorities['public_function'] if method.is_public else self.base_priorities['private_method']
        
        reasons = []
        multiplier = 1.0
        
        # Check if well documented
        if method.docstring and len(method.docstring.strip()) > 30:
            multiplier *= 1.2
            reasons.append("well_documented")
        
        # Check for special methods
        if method.name.startswith('__') and method.name.endswith('__'):
            base_score = self.base_priorities['magic_method']
            reasons.append("magic_method")
        
        # Check for properties
        if method.is_property:
            base_score = self.base_priorities['property']
            reasons.append("property")
        
        # Check for important keywords
        if any(keyword in method.name.lower() for keyword in self.important_keywords):
            multiplier *= 1.2
            reasons.append("important_keyword")
        
        # Check for framework patterns
        method_text = f"{method.name} {method.docstring or ''}"
        framework = self._detect_framework(method_text)
        if framework:
            multiplier *= 1.1
            reasons.append(f"framework_{framework}")
        
        # Penalize private methods
        if method.name.startswith('_') and not method.name.startswith('__'):
            multiplier *= 0.6
            reasons.append("private_method")
        
        final_score = base_score * multiplier
        
        # Calculate token count
        content = f"{method.signature}\n{method.docstring or ''}"
        token_count = self.token_counter.count_tokens(content, ContentType.CODE)
        
        return PriorityScore(
            item_id=f"method_{method.name}_{class_info.name}_{module.name}",
            item_type="method",
            score=final_score,
            token_count=token_count,
            reasons=reasons
        )
    
    def _score_function(self, function: FunctionInfo, module: ModuleInfo) -> PriorityScore:
        """Calculate priority score for a function."""
        base_score = self.base_priorities['public_function'] if function.is_public else 3
        
        reasons = []
        multiplier = 1.0
        
        # Check if well documented
        if function.docstring and len(function.docstring.strip()) > 30:
            multiplier *= 1.2
            reasons.append("well_documented")
        
        # Check for main entry point
        if function.name == 'main' or module.is_main:
            multiplier *= 1.5
            reasons.append("main_entry_point")
        
        # Check for important keywords
        if any(keyword in function.name.lower() for keyword in self.important_keywords):
            multiplier *= 1.2
            reasons.append("important_keyword")
        
        # Check for framework patterns
        function_text = f"{function.name} {function.docstring or ''}"
        framework = self._detect_framework(function_text)
        if framework:
            multiplier *= 1.1
            reasons.append(f"framework_{framework}")
        
        # Penalize private functions
        if function.name.startswith('_'):
            multiplier *= 0.6
            reasons.append("private_function")
        
        final_score = base_score * multiplier
        
        # Calculate token count
        content = f"{function.signature}\n{function.docstring or ''}"
        token_count = self.token_counter.count_tokens(content, ContentType.CODE)
        
        return PriorityScore(
            item_id=f"function_{function.name}_{module.name}",
            item_type="function",
            score=final_score,
            token_count=token_count,
            reasons=reasons
        )
    
    def _score_example(self, example: Any, index: int) -> PriorityScore:
        """Calculate priority score for an example."""
        base_score = self.base_priorities['has_examples']
        
        reasons = ["code_example"]
        multiplier = 1.0
        
        # Higher priority for basic usage examples
        if hasattr(example, 'example_type') and example.example_type == 'basic_usage':
            multiplier *= 1.3
            reasons.append("basic_usage")
        
        # Higher priority for executable examples
        if hasattr(example, 'is_executable') and example.is_executable:
            multiplier *= 1.1
            reasons.append("executable")
        
        final_score = base_score * multiplier
        
        # Calculate token count
        content = getattr(example, 'code', str(example))
        token_count = self.token_counter.count_tokens(content, ContentType.CODE)
        
        return PriorityScore(
            item_id=f"example_{index}",
            item_type="example",
            score=final_score,
            token_count=token_count,
            reasons=reasons
        )
    
    def _score_configuration(self, config: Any) -> PriorityScore:
        """Calculate priority score for configuration."""
        base_score = self.base_priorities['configuration']
        
        reasons = ["configuration"]
        
        # Calculate token count
        content = str(config)
        token_count = self.token_counter.count_tokens(content, ContentType.JSON)
        
        return PriorityScore(
            item_id="configuration",
            item_type="configuration",
            score=base_score,
            token_count=token_count,
            reasons=reasons
        )
    
    def _detect_framework(self, text: str) -> Optional[str]:
        """Detect framework usage in text."""
        text_lower = text.lower()
        
        for framework, patterns in self.framework_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                return framework
        
        return None
    
    def filter_by_budget(self, priority_scores: List[PriorityScore], budget: TokenBudget) -> List[PriorityScore]:
        """
        Filter content to fit within token budget.
        
        Args:
            priority_scores: List of priority scores
            budget: Token budget constraints
            
        Returns:
            Filtered list that fits within budget
        """
        selected_scores = []
        
        # Separate by type for budget allocation
        by_type = {}
        for score in priority_scores:
            if score.item_type not in by_type:
                by_type[score.item_type] = []
            by_type[score.item_type].append(score)
        
        # Allocate budget by type
        type_budgets = {
            'class': budget.api_documentation // 2,
            'method': budget.api_documentation // 3,
            'function': budget.api_documentation // 6,
            'example': budget.examples,
            'configuration': budget.configuration,
        }
        
        for content_type, scores in by_type.items():
            type_budget = type_budgets.get(content_type, 10000)
            selected = self._select_by_budget(scores, type_budget)
            selected_scores.extend(selected)
        
        return selected_scores
    
    def _select_by_budget(self, scores: List[PriorityScore], budget: int) -> List[PriorityScore]:
        """Select items that fit within a specific budget."""
        scores.sort(key=lambda x: x.efficiency, reverse=True)
        
        selected = []
        used_tokens = 0
        
        for score in scores:
            if used_tokens + score.token_count <= budget:
                selected.append(score)
                used_tokens += score.token_count
            else:
                break
        
        return selected
    
    def compress_content(self, content: str, max_tokens: int, content_type: ContentType) -> str:
        """
        Compress content to fit within token limit.
        
        Args:
            content: Content to compress
            max_tokens: Maximum token limit
            content_type: Type of content
            
        Returns:
            Compressed content
        """
        current_tokens = self.token_counter.count_tokens(content, content_type)
        
        if current_tokens <= max_tokens:
            return content
        
        # Different compression strategies by content type
        if content_type == ContentType.CODE:
            return self._compress_code(content, max_tokens)
        elif content_type == ContentType.DOCSTRING:
            return self._compress_docstring(content, max_tokens)
        else:
            return self._compress_text(content, max_tokens)
    
    def _compress_code(self, code: str, max_tokens: int) -> str:
        """Compress code content."""
        lines = code.split('\n')
        
        # Keep important lines (class/function definitions, docstrings)
        important_lines = []
        for line in lines:
            stripped = line.strip()
            if (stripped.startswith(('class ', 'def ', '"""', "'''")) or
                stripped.startswith('#') or
                not stripped):
                important_lines.append(line)
        
        # Add other lines until we hit the token limit
        compressed = '\n'.join(important_lines)
        current_tokens = self.token_counter.count_tokens(compressed, ContentType.CODE)
        
        if current_tokens <= max_tokens:
            return compressed
        
        # If still too long, truncate with ellipsis
        target_length = int(len(compressed) * (max_tokens / current_tokens) * 0.9)
        return compressed[:target_length] + "\n# ... (truncated)"
    
    def _compress_docstring(self, docstring: str, max_tokens: int) -> str:
        """Compress docstring content."""
        # Keep first paragraph and examples
        paragraphs = docstring.split('\n\n')
        
        compressed = paragraphs[0]  # Always keep first paragraph
        
        # Add additional paragraphs if they fit
        for para in paragraphs[1:]:
            test_content = compressed + '\n\n' + para
            if self.token_counter.count_tokens(test_content, ContentType.DOCSTRING) <= max_tokens:
                compressed = test_content
            else:
                break
        
        return compressed
    
    def _compress_text(self, text: str, max_tokens: int) -> str:
        """Compress general text content."""
        sentences = re.split(r'[.!?]+', text)
        
        compressed = sentences[0]
        
        for sentence in sentences[1:]:
            test_content = compressed + '. ' + sentence
            if self.token_counter.count_tokens(test_content, ContentType.TEXT) <= max_tokens:
                compressed = test_content
            else:
                break
        
        return compressed + '...' if len(compressed) < len(text) else compressed


def prioritize_project_data(project_data: ProjectData, token_budget: TokenBudget) -> List[PriorityScore]:
    """
    Prioritize project data content based on importance and token budget.
    
    Args:
        project_data: Project data to prioritize
        token_budget: Token budget constraints
        
    Returns:
        List of prioritized content items
    """
    prioritizer = ContentPrioritizer()
    priority_scores = prioritizer.calculate_priority_scores(project_data)
    return prioritizer.filter_by_budget(priority_scores, token_budget)


def filter_content_by_priority(content_items: List[Any], min_priority: float) -> List[Any]:
    """
    Filter content items by minimum priority score.
    
    Args:
        content_items: List of content items with priority scores
        min_priority: Minimum priority threshold
        
    Returns:
        Filtered list of content items
    """
    return [item for item in content_items if hasattr(item, 'score') and item.score >= min_priority]


def compress_content_for_budget(content: Dict[str, Any], token_budget: TokenBudget) -> Dict[str, Any]:
    """
    Compress content to fit within token budget.
    
    Args:
        content: Content dictionary to compress
        token_budget: Token budget constraints
        
    Returns:
        Compressed content dictionary
    """
    prioritizer = ContentPrioritizer()
    compressed = {}
    
    for key, value in content.items():
        if key == 'api_documentation':
            budget = token_budget.api_documentation
        elif key == 'examples':
            budget = token_budget.examples
        elif key == 'configuration':
            budget = token_budget.configuration
        else:
            budget = token_budget.metadata
        
        if isinstance(value, str):
            compressed[key] = prioritizer.compress_content(value, budget, ContentType.TEXT)
        elif isinstance(value, (dict, list)):
            # Convert to JSON and compress
            json_str = str(value)
            compressed[key] = prioritizer.compress_content(json_str, budget, ContentType.JSON)
        else:
            compressed[key] = value
    
    return compressed


File: /workspaces/SYSC4918/src/parser/__init__.py
"""
Parser module for the README generator project.

This module contains the main orchestrator and all sub-parsers responsible
for extracting information from Python projects for README generation.
"""

from .project_parser import (
    ProjectParser,
    ParseResult,
    ParsingError,
    parse_project,
    parse_project_to_json
)

from .metadata_parser import (
    MetadataParser,
    extract_project_metadata,
    detect_project_type,
    find_license_info
)

from .dependency_parser import (
    DependencyParser,
    extract_dependencies,
    parse_requirements_file,
    detect_python_version
)

from .code_parser import (
    CodeParser,
    extract_code_information,
    parse_python_file,
    analyze_ast_node
)

from .structure_parser import (
    StructureParser,
    analyze_project_structure,
    find_entry_points,
    categorize_directories
)

from .example_parser import (
    ExampleParser,
    extract_code_examples,
    parse_docstring_examples,
    find_usage_patterns
)

__all__ = [
    # Main orchestrator
    "ProjectParser",
    "ParseResult", 
    "ParsingError",
    "parse_project",
    "parse_project_to_json",
    
    # Metadata parser
    "MetadataParser",
    "extract_project_metadata",
    "detect_project_type",
    "find_license_info",
    
    # Dependency parser
    "DependencyParser",
    "extract_dependencies",
    "parse_requirements_file",
    "detect_python_version",
    
    # Code parser
    "CodeParser",
    "extract_code_information",
    "parse_python_file",
    "analyze_ast_node",
    
    # Structure parser
    "StructureParser",
    "analyze_project_structure",
    "find_entry_points",
    "categorize_directories",
    
    # Example parser
    "ExampleParser",
    "extract_code_examples",
    "parse_docstring_examples",
    "find_usage_patterns"
]

__version__ = "0.1.0"


File: /workspaces/SYSC4918/src/parser/project_parser.py
"""
Main project parser orchestrator that coordinates all parsing activities.

This module provides the primary interface for parsing Python projects,
managing the parsing workflow, and consolidating results from all sub-parsers.
"""

import os
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import traceback

from models.project_data import (
    ProjectData, ProjectMetadata, DependencyInfo, ProjectStructure, 
    ConfigurationInfo, CodeExample, TestInfo, DocumentationInfo
)
from utils.file_utils import get_project_files, FileInfo
from utils.token_counter import TokenCounter, get_token_budget_allocation
from utils.content_prioritizer import ContentPrioritizer, prioritize_project_data
from utils.json_serializer import ProjectDataSerializer

from .metadata_parser import MetadataParser
from .dependency_parser import DependencyParser
from .code_parser import CodeParser
from .structure_parser import StructureParser
from .example_parser import ExampleParser

logger = logging.getLogger(__name__)


class ParsingError(Exception):
    """Custom exception for parsing errors."""
    
    def __init__(self, message: str, parser_name: str = None, file_path: str = None):
        self.message = message
        self.parser_name = parser_name
        self.file_path = file_path
        super().__init__(self.message)


@dataclass
class ParseResult:
    """Result of project parsing operation."""
    
    project_data: ProjectData
    success: bool
    duration: float
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    stats: Dict[str, Any] = field(default_factory=dict)


class ProjectParser:
    """
    Main orchestrator for parsing Python projects.
    
    This class coordinates all sub-parsers and manages the overall parsing workflow,
    including error handling, prioritization, and token management.
    """
    
    def __init__(self, 
                 model_name: str = "gemini_2_5_pro",
                 max_tokens: int = 1_000_000,
                 include_tests: bool = False,
                 include_private: bool = False,
                 enable_caching: bool = True):
        """
        Initialize the project parser.
        
        Args:
            model_name: Target LLM model name for token management
            max_tokens: Maximum token budget
            include_tests: Whether to include test files in parsing
            include_private: Whether to include private methods/classes
            enable_caching: Whether to enable result caching
        """
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.include_tests = include_tests
        self.include_private = include_private
        self.enable_caching = enable_caching
        
        # Initialize components
        self.token_counter = TokenCounter(model_name)
        self.content_prioritizer = ContentPrioritizer(self.token_counter)
        self.serializer = ProjectDataSerializer(self.token_counter)
        
        # Initialize sub-parsers
        self.metadata_parser = MetadataParser()
        self.dependency_parser = DependencyParser()
        self.code_parser = CodeParser(include_private=include_private)
        self.structure_parser = StructureParser()
        self.example_parser = ExampleParser()
        
        # Parsing statistics
        self.stats = {
            'files_processed': 0,
            'lines_processed': 0,
            'classes_found': 0,
            'functions_found': 0,
            'examples_found': 0,
            'errors_encountered': 0,
            'warnings_generated': 0
        }
        
        # Error tracking
        self.parsing_errors = []
        self.parsing_warnings = []
    
    def parse_project(self, project_path: str) -> ParseResult:
        """
        Parse a Python project and extract all relevant information.
        
        Args:
            project_path: Path to the project root directory
            
        Returns:
            ParseResult containing parsed project data and metadata
        """
        start_time = time.time()
        
        try:
            logger.info(f"Starting to parse project: {project_path}")
            
            # Validate project path
            project_path = Path(project_path).resolve()
            if not project_path.exists():
                raise ParsingError(f"Project path does not exist: {project_path}")
            
            if not project_path.is_dir():
                raise ParsingError(f"Project path is not a directory: {project_path}")
            
            # Initialize project data structure
            project_data = self._initialize_project_data(project_path)
            
            # Get project files
            logger.info("Scanning project files...")
            project_files = get_project_files(str(project_path), include_tests=self.include_tests)
            self.stats['files_processed'] = sum(len(files) for files in project_files.values())
            
            # Parse different aspects of the project
            logger.info("Parsing project metadata...")
            project_data.metadata = self._parse_metadata(project_path, project_files)
            
            logger.info("Parsing project dependencies...")
            project_data.dependencies = self._parse_dependencies(project_path, project_files)
            
            logger.info("Parsing project structure...")
            project_data.structure = self._parse_structure(project_path, project_files)
            
            logger.info("Parsing code information...")
            self._parse_code_information(project_data.structure)
            
            logger.info("Parsing configuration...")
            project_data.configuration = self._parse_configuration(project_path, project_files)
            
            logger.info("Extracting examples...")
            project_data.examples = self._extract_examples(project_data.structure)
            
            logger.info("Parsing test information...")
            project_data.tests = self._parse_tests(project_path, project_files)
            
            logger.info("Parsing documentation...")
            project_data.documentation = self._parse_documentation(project_path, project_files)
            
            # Apply prioritization and token management
            logger.info("Applying content prioritization...")
            project_data = self._apply_prioritization(project_data)
            
            # Set final metadata
            project_data.parsing_errors = self.parsing_errors
            project_data.parsing_timestamp = datetime.now().isoformat()
            
            # Calculate final token count
            serialized_data = self.serializer.serialize(project_data, validate=False)
            project_data.token_count = self.token_counter.count_tokens_in_dict(serialized_data)
            
            duration = time.time() - start_time
            logger.info(f"Project parsing completed in {duration:.2f} seconds")
            
            # Update final statistics
            self.stats['lines_processed'] = project_data.structure.total_lines
            self.stats['classes_found'] = sum(len(module.classes) for module in project_data.structure.modules)
            self.stats['functions_found'] = sum(len(module.functions) for module in project_data.structure.modules)
            self.stats['examples_found'] = len(project_data.examples)
            self.stats['errors_encountered'] = len(self.parsing_errors)
            self.stats['warnings_generated'] = len(self.parsing_warnings)
            
            return ParseResult(
                project_data=project_data,
                success=True,
                duration=duration,
                errors=self.parsing_errors,
                warnings=self.parsing_warnings,
                stats=self.stats
            )
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"Failed to parse project: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            # Create minimal project data for error case
            project_data = ProjectData(
                metadata=ProjectMetadata(project_name=project_path.name),
                dependencies=DependencyInfo(),
                structure=ProjectStructure(root_path=str(project_path)),
                configuration=ConfigurationInfo(),
                parsing_errors=[error_msg],
                parsing_timestamp=datetime.now().isoformat()
            )
            
            return ParseResult(
                project_data=project_data,
                success=False,
                duration=duration,
                errors=[error_msg],
                warnings=self.parsing_warnings,
                stats=self.stats
            )
    
    def _initialize_project_data(self, project_path: Path) -> ProjectData:
        """Initialize the project data structure."""
        return ProjectData(
            metadata=ProjectMetadata(project_name=project_path.name),
            dependencies=DependencyInfo(),
            structure=ProjectStructure(root_path=str(project_path)),
            configuration=ConfigurationInfo()
        )
    
    def _parse_metadata(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> ProjectMetadata:
        """Parse project metadata using the metadata parser."""
        try:
            return self.metadata_parser.parse(project_path, project_files)
        except Exception as e:
            error_msg = f"Error parsing metadata: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return ProjectMetadata(project_name=project_path.name)
    
    def _parse_dependencies(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> DependencyInfo:
        """Parse project dependencies using the dependency parser."""
        try:
            return self.dependency_parser.parse(project_path, project_files)
        except Exception as e:
            error_msg = f"Error parsing dependencies: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return DependencyInfo()
    
    def _parse_structure(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> ProjectStructure:
        """Parse project structure using the structure parser."""
        try:
            return self.structure_parser.parse(project_path, project_files)
        except Exception as e:
            error_msg = f"Error parsing project structure: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return ProjectStructure(root_path=str(project_path))
    
    def _parse_code_information(self, structure: ProjectStructure) -> None:
        """Parse code information for all modules in the structure."""
        for module in structure.modules:
            try:
                # Parse the Python file and extract code information
                code_info = self.code_parser.parse_file(module.file_path)
                
                # Update module with parsed information
                module.classes = code_info.classes
                module.functions = code_info.functions
                module.constants = code_info.constants
                module.imports = code_info.imports
                module.docstring = code_info.docstring
                
            except Exception as e:
                error_msg = f"Error parsing code in {module.file_path}: {str(e)}"
                logger.error(error_msg)
                self.parsing_errors.append(error_msg)
    
    def _parse_configuration(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> ConfigurationInfo:
        """Parse project configuration information."""
        try:
            config_info = ConfigurationInfo()
            
            # Find configuration files
            config_files = []
            for file_info in project_files.get('config', []):
                config_files.append(file_info.path)
            
            config_info.config_files = config_files
            
            # Parse environment variables and settings
            # This would be implemented based on specific configuration file formats
            
            return config_info
            
        except Exception as e:
            error_msg = f"Error parsing configuration: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return ConfigurationInfo()
    
    def _extract_examples(self, structure: ProjectStructure) -> List[CodeExample]:
        """Extract code examples using the example parser."""
        try:
            examples = []
            
            for module in structure.modules:
                module_examples = self.example_parser.extract_from_module(module)
                examples.extend(module_examples)
            
            return examples
            
        except Exception as e:
            error_msg = f"Error extracting examples: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return []
    
    def _parse_tests(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> Optional[TestInfo]:
        """Parse test information."""
        try:
            if not self.include_tests:
                return None
            
            test_files = [file_info.path for file_info in project_files.get('tests', [])]
            
            if not test_files:
                return None
            
            # Detect test framework
            test_framework = self._detect_test_framework(test_files)
            
            return TestInfo(
                test_directories=list(set(os.path.dirname(tf) for tf in test_files)),
                test_files=test_files,
                test_framework=test_framework,
                total_tests=len(test_files)
            )
            
        except Exception as e:
            error_msg = f"Error parsing tests: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return None
    
    def _parse_documentation(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> Optional[DocumentationInfo]:
        """Parse documentation information."""
        try:
            doc_files = [file_info.path for file_info in project_files.get('documentation', [])]
            
            if not doc_files:
                return None
            
            # Find specific documentation files
            readme_file = None
            license_file = None
            changelog_file = None
            
            for file_path in doc_files:
                filename = os.path.basename(file_path).lower()
                if filename.startswith('readme'):
                    readme_file = file_path
                elif filename.startswith('license'):
                    license_file = file_path
                elif filename.startswith('changelog'):
                    changelog_file = file_path
            
            # Check for documentation tools
            has_sphinx = any('sphinx' in str(f).lower() for f in project_path.rglob('*'))
            has_mkdocs = any('mkdocs' in str(f).lower() for f in project_path.rglob('*'))
            
            return DocumentationInfo(
                readme_file=readme_file,
                license_file=license_file,
                changelog_file=changelog_file,
                doc_files=doc_files,
                has_sphinx=has_sphinx,
                has_mkdocs=has_mkdocs
            )
            
        except Exception as e:
            error_msg = f"Error parsing documentation: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return None
    
    def _detect_test_framework(self, test_files: List[str]) -> Optional[str]:
        """Detect the test framework being used."""
        framework_patterns = {
            'pytest': ['pytest', 'conftest.py', '@pytest'],
            'unittest': ['unittest', 'TestCase'],
            'nose': ['nose', 'nosetests'],
            'doctest': ['doctest']
        }
        
        for file_path in test_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                for framework, patterns in framework_patterns.items():
                    if any(pattern in content for pattern in patterns):
                        return framework
                        
            except Exception:
                continue
        
        return None
    
    def _apply_prioritization(self, project_data: ProjectData) -> ProjectData:
        """Apply content prioritization and token management."""
        try:
            # Get token budget
            token_budget = self.token_counter.get_budget_allocation()
            
            # Calculate current token usage
            serialized_data = self.serializer.serialize(project_data, validate=False)
            current_tokens = self.token_counter.count_tokens_in_dict(serialized_data)
            
            logger.info(f"Current token count: {current_tokens}")
            logger.info(f"Token budget: {token_budget.total_budget}")
            
            # If we're over budget, apply prioritization
            if current_tokens > token_budget.total_budget:
                logger.info("Applying content prioritization to fit token budget")
                
                # Get priority scores
                priority_scores = self.content_prioritizer.calculate_priority_scores(project_data)
                
                # Filter content by budget
                filtered_scores = self.content_prioritizer.filter_by_budget(priority_scores, token_budget)
                
                # Apply filtering to project data
                project_data = self._filter_project_data(project_data, filtered_scores)
                
                # Recalculate token count
                serialized_data = self.serializer.serialize(project_data, validate=False)
                final_tokens = self.token_counter.count_tokens_in_dict(serialized_data)
                
                logger.info(f"Final token count after prioritization: {final_tokens}")
                
                if final_tokens > token_budget.total_budget:
                    warning_msg = f"Token count ({final_tokens}) still exceeds budget ({token_budget.total_budget})"
                    logger.warning(warning_msg)
                    self.parsing_warnings.append(warning_msg)
            
            return project_data
            
        except Exception as e:
            error_msg = f"Error applying prioritization: {str(e)}"
            logger.error(error_msg)
            self.parsing_errors.append(error_msg)
            return project_data
    
    def _filter_project_data(self, project_data: ProjectData, priority_scores: List[Any]) -> ProjectData:
        """Filter project data based on priority scores."""
        # This would implement the actual filtering logic
        # For now, we'll return the original data
        # In a full implementation, this would remove low-priority content
        return project_data
    
    def get_parsing_summary(self) -> Dict[str, Any]:
        """Get a summary of the parsing operation."""
        return {
            'statistics': self.stats,
            'errors': self.parsing_errors,
            'warnings': self.parsing_warnings,
            'configuration': {
                'model_name': self.model_name,
                'max_tokens': self.max_tokens,
                'include_tests': self.include_tests,
                'include_private': self.include_private,
                'enable_caching': self.enable_caching
            }
        }


# Convenience functions for external use
def parse_project(project_path: str, 
                 model_name: str = "gemini_2_5_pro",
                 max_tokens: int = 1_000_000,
                 include_tests: bool = False,
                 include_private: bool = False) -> ParseResult:
    """
    Parse a Python project and return structured data.
    
    Args:
        project_path: Path to the project root directory
        model_name: Target LLM model name
        max_tokens: Maximum token budget
        include_tests: Whether to include test files
        include_private: Whether to include private methods/classes
        
    Returns:
        ParseResult containing parsed project data
    """
    parser = ProjectParser(
        model_name=model_name,
        max_tokens=max_tokens,
        include_tests=include_tests,
        include_private=include_private
    )
    
    return parser.parse_project(project_path)


def parse_project_to_json(project_path: str,
                         output_path: Optional[str] = None,
                         model_name: str = "gemini_2_5_pro",
                         max_tokens: int = 1_000_000,
                         include_tests: bool = False,
                         include_private: bool = False) -> Tuple[bool, str]:
    """
    Parse a Python project and save results to JSON file.
    
    Args:
        project_path: Path to the project root directory
        output_path: Path to save JSON file (defaults to project_path/parsed_data.json)
        model_name: Target LLM model name
        max_tokens: Maximum token budget
        include_tests: Whether to include test files
        include_private: Whether to include private methods/classes
        
    Returns:
        Tuple of (success, message)
    """
    try:
        # Parse project
        result = parse_project(
            project_path=project_path,
            model_name=model_name,
            max_tokens=max_tokens,
            include_tests=include_tests,
            include_private=include_private
        )
        
        if not result.success:
            return False, f"Parsing failed: {'; '.join(result.errors)}"
        
        # Determine output path
        if output_path is None:
            output_path = os.path.join(project_path, "parsed_data.json")
        
        # Serialize and save
        serializer = ProjectDataSerializer()
        serialized_data = serializer.serialize(result.project_data)
        
        from ..utils.json_serializer import save_json_to_file
        success = save_json_to_file(serialized_data, output_path)
        
        if success:
            return True, f"Project parsed successfully. Output saved to: {output_path}"
        else:
            return False, f"Failed to save output to: {output_path}"
            
    except Exception as e:
        return False, f"Error parsing project: {str(e)}"


File: /workspaces/SYSC4918/src/parser/code_parser.py
"""
Code parser for extracting classes, functions, and methods from Python files using AST.

This module provides comprehensive AST-based parsing to extract detailed information
about Python code structures including classes, functions, methods, decorators,
type hints, and docstrings.
"""

import ast
import re
import inspect
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
import logging

from models.project_data import (
    ClassInfo, FunctionInfo, ModuleInfo, CodeExample
)
from utils.file_utils import read_file_safely

logger = logging.getLogger(__name__)


class CodeParsingError(Exception):
    """Custom exception for code parsing errors."""
    pass


class ASTVisitor(ast.NodeVisitor):
    """Custom AST visitor for extracting code information."""
    
    def __init__(self, file_path: str, include_private: bool = False):
        self.file_path = file_path
        self.include_private = include_private
        self.classes = []
        self.functions = []
        self.constants = []
        self.imports = []
        self.module_docstring = None
        self.current_class = None
        self.current_function_level = 0
        
    def visit_Module(self, node: ast.Module) -> None:
        """Visit module node to extract module-level docstring."""
        self.module_docstring = ast.get_docstring(node)
        self.generic_visit(node)
    
    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        """Visit class definition node."""
        # Skip private classes unless explicitly included
        if not self.include_private and node.name.startswith('_'):
            return
            
        class_info = ClassInfo(
            name=node.name,
            docstring=ast.get_docstring(node),
            line_number=node.lineno,
            file_path=self.file_path
        )
        
        # Extract inheritance
        class_info.inheritance = self._extract_inheritance(node)
        
        # Extract decorators
        class_info.decorators = self._extract_decorators(node)
        
        # Check for special class types
        class_info.is_abstract = self._is_abstract_class(node)
        class_info.is_dataclass = self._is_dataclass(node)
        class_info.is_enum = self._is_enum_class(node)
        
        # Extract class attributes
        class_info.attributes = self._extract_class_attributes(node)
        
        # Process methods
        previous_class = self.current_class
        self.current_class = class_info
        
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                method_info = self._extract_function_info(item, is_method=True)
                if method_info:
                    if method_info.is_property:
                        class_info.properties.append(method_info)
                    else:
                        class_info.methods.append(method_info)
        
        self.current_class = previous_class
        self.classes.append(class_info)
    
    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """Visit function definition node."""
        # Skip if we're inside a class (handled by visit_ClassDef)
        if self.current_class is not None:
            return
            
        # Skip private functions unless explicitly included
        if not self.include_private and node.name.startswith('_'):
            return
            
        function_info = self._extract_function_info(node, is_method=False)
        if function_info:
            self.functions.append(function_info)
    
    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
        """Visit async function definition node."""
        # Skip if we're inside a class
        if self.current_class is not None:
            return
            
        # Skip private functions unless explicitly included
        if not self.include_private and node.name.startswith('_'):
            return
            
        function_info = self._extract_function_info(node, is_method=False, is_async=True)
        if function_info:
            self.functions.append(function_info)
    
    def visit_Assign(self, node: ast.Assign) -> None:
        """Visit assignment node to extract constants."""
        # Extract module-level constants
        if self.current_class is None and self.current_function_level == 0:
            for target in node.targets:
                if isinstance(target, ast.Name):
                    # Check if it's a constant (all uppercase)
                    if target.id.isupper():
                        constant_info = {
                            'name': target.id,
                            'value': self._extract_value(node.value),
                            'type': self._infer_type(node.value),
                            'line_number': node.lineno
                        }
                        self.constants.append(constant_info)
        
        self.generic_visit(node)
    
    def visit_Import(self, node: ast.Import) -> None:
        """Visit import node."""
        for alias in node.names:
            import_name = alias.asname if alias.asname else alias.name
            self.imports.append(import_name)
    
    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        """Visit import from node."""
        module = node.module or ''
        for alias in node.names:
            if alias.name == '*':
                self.imports.append(f"from {module} import *")
            else:
                import_name = alias.asname if alias.asname else alias.name
                self.imports.append(f"from {module} import {import_name}")
    
    def _extract_function_info(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef], 
                             is_method: bool = False, is_async: bool = False) -> Optional[FunctionInfo]:
        """Extract detailed function information from AST node."""
        try:
            function_info = FunctionInfo(
                name=node.name,
                signature=self._extract_signature(node),
                docstring=ast.get_docstring(node),
                is_public=not node.name.startswith('_'),
                is_async=is_async or isinstance(node, ast.AsyncFunctionDef),
                line_number=node.lineno,
                file_path=self.file_path
            )
            
            # Extract decorators
            function_info.decorators = self._extract_decorators(node)
            
            # Check for special method types
            function_info.is_property = '@property' in function_info.decorators
            function_info.is_classmethod = '@classmethod' in function_info.decorators
            function_info.is_staticmethod = '@staticmethod' in function_info.decorators
            
            # Extract return type
            if hasattr(node, 'returns') and node.returns:
                function_info.return_type = ast.unparse(node.returns)
            
            # Extract parameters
            function_info.parameters = self._extract_parameters(node)
            
            # Calculate complexity score
            function_info.complexity_score = self._calculate_complexity(node)
            
            return function_info
            
        except Exception as e:
            logger.error(f"Error extracting function info for {node.name}: {e}")
            return None
    
    def _extract_signature(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> str:
        """Extract function signature as string."""
        try:
            # Build signature manually for better control
            parts = []
            
            # Add async keyword if needed
            if isinstance(node, ast.AsyncFunctionDef):
                parts.append('async')
            
            parts.append('def')
            parts.append(node.name)
            
            # Build arguments
            args_parts = []
            
            # Positional-only arguments
            if hasattr(node.args, 'posonlyargs'):
                for arg in node.args.posonlyargs:
                    arg_str = self._format_arg(arg)
                    args_parts.append(arg_str)
                if node.args.posonlyargs:
                    args_parts.append('/')
            
            # Regular arguments
            defaults = node.args.defaults
            num_defaults = len(defaults)
            num_args = len(node.args.args)
            
            for i, arg in enumerate(node.args.args):
                arg_str = self._format_arg(arg)
                
                # Add default if available
                default_index = i - (num_args - num_defaults)
                if default_index >= 0:
                    default_val = ast.unparse(defaults[default_index])
                    arg_str += f"={default_val}"
                
                args_parts.append(arg_str)
            
            # Varargs
            if node.args.vararg:
                args_parts.append(f"*{self._format_arg(node.args.vararg)}")
            
            # Keyword-only arguments
            if node.args.kwonlyargs:
                if not node.args.vararg:
                    args_parts.append('*')
                
                kw_defaults = node.args.kw_defaults
                for i, arg in enumerate(node.args.kwonlyargs):
                    arg_str = self._format_arg(arg)
                    if i < len(kw_defaults) and kw_defaults[i]:
                        default_val = ast.unparse(kw_defaults[i])
                        arg_str += f"={default_val}"
                    args_parts.append(arg_str)
            
            # Kwargs
            if node.args.kwarg:
                args_parts.append(f"**{self._format_arg(node.args.kwarg)}")
            
            signature = f"{' '.join(parts)}({', '.join(args_parts)})"
            
            # Add return type annotation
            if hasattr(node, 'returns') and node.returns:
                signature += f" -> {ast.unparse(node.returns)}"
            
            return signature
            
        except Exception as e:
            logger.error(f"Error extracting signature for {node.name}: {e}")
            return f"def {node.name}(...)"
    
    def _format_arg(self, arg: ast.arg) -> str:
        """Format argument with type annotation."""
        arg_str = arg.arg
        if arg.annotation:
            arg_str += f": {ast.unparse(arg.annotation)}"
        return arg_str
    
    def _extract_parameters(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> List[Dict[str, Any]]:
        """Extract detailed parameter information."""
        parameters = []
        
        try:
            # Regular arguments
            defaults = node.args.defaults
            num_defaults = len(defaults)
            num_args = len(node.args.args)
            
            for i, arg in enumerate(node.args.args):
                param_info = {
                    'name': arg.arg,
                    'type': ast.unparse(arg.annotation) if arg.annotation else None,
                    'default': None,
                    'description': None
                }
                
                # Add default if available
                default_index = i - (num_args - num_defaults)
                if default_index >= 0:
                    param_info['default'] = ast.unparse(defaults[default_index])
                
                parameters.append(param_info)
            
            # Varargs
            if node.args.vararg:
                param_info = {
                    'name': f"*{node.args.vararg.arg}",
                    'type': ast.unparse(node.args.vararg.annotation) if node.args.vararg.annotation else None,
                    'default': None,
                    'description': None
                }
                parameters.append(param_info)
            
            # Keyword-only arguments
            kw_defaults = node.args.kw_defaults
            for i, arg in enumerate(node.args.kwonlyargs):
                param_info = {
                    'name': arg.arg,
                    'type': ast.unparse(arg.annotation) if arg.annotation else None,
                    'default': None,
                    'description': None
                }
                
                if i < len(kw_defaults) and kw_defaults[i]:
                    param_info['default'] = ast.unparse(kw_defaults[i])
                
                parameters.append(param_info)
            
            # Kwargs
            if node.args.kwarg:
                param_info = {
                    'name': f"**{node.args.kwarg.arg}",
                    'type': ast.unparse(node.args.kwarg.annotation) if node.args.kwarg.annotation else None,
                    'default': None,
                    'description': None
                }
                parameters.append(param_info)
            
        except Exception as e:
            logger.error(f"Error extracting parameters: {e}")
        
        return parameters
    
    def _extract_inheritance(self, node: ast.ClassDef) -> List[str]:
        """Extract class inheritance information."""
        inheritance = []
        for base in node.bases:
            try:
                inheritance.append(ast.unparse(base))
            except Exception:
                inheritance.append(str(base))
        return inheritance
    
    def _extract_decorators(self, node: Union[ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef]) -> List[str]:
        """Extract decorator information."""
        decorators = []
        for decorator in node.decorator_list:
            try:
                decorators.append(f"@{ast.unparse(decorator)}")
            except Exception:
                decorators.append(f"@{str(decorator)}")
        return decorators
    
    def _is_abstract_class(self, node: ast.ClassDef) -> bool:
        """Check if class is abstract."""
        # Check for ABC inheritance
        for base in node.bases:
            if isinstance(base, ast.Name) and base.id in ['ABC', 'ABCMeta']:
                return True
            elif isinstance(base, ast.Attribute):
                if base.attr in ['ABC', 'ABCMeta']:
                    return True
        
        # Check for abstract methods
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                for decorator in item.decorator_list:
                    if isinstance(decorator, ast.Name) and decorator.id == 'abstractmethod':
                        return True
                    elif isinstance(decorator, ast.Attribute) and decorator.attr == 'abstractmethod':
                        return True
        
        return False
    
    def _is_dataclass(self, node: ast.ClassDef) -> bool:
        """Check if class is a dataclass."""
        for decorator in node.decorator_list:
            if isinstance(decorator, ast.Name) and decorator.id == 'dataclass':
                return True
            elif isinstance(decorator, ast.Attribute) and decorator.attr == 'dataclass':
                return True
        return False
    
    def _is_enum_class(self, node: ast.ClassDef) -> bool:
        """Check if class is an enum."""
        for base in node.bases:
            if isinstance(base, ast.Name) and base.id == 'Enum':
                return True
            elif isinstance(base, ast.Attribute) and base.attr == 'Enum':
                return True
        return False
    
    def _extract_class_attributes(self, node: ast.ClassDef) -> List[Dict[str, Any]]:
        """Extract class attributes."""
        attributes = []
        
        for item in node.body:
            if isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):
                # Type-annotated attribute
                attr_info = {
                    'name': item.target.id,
                    'type': ast.unparse(item.annotation) if item.annotation else None,
                    'description': None
                }
                attributes.append(attr_info)
            elif isinstance(item, ast.Assign):
                # Regular assignment
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        attr_info = {
                            'name': target.id,
                            'type': None,
                            'description': None
                        }
                        attributes.append(attr_info)
        
        return attributes
    
    def _extract_value(self, node: ast.AST) -> Any:
        """Extract value from AST node."""
        try:
            if isinstance(node, ast.Constant):
                return node.value
            elif isinstance(node, ast.Num):  # For older Python versions
                return node.n
            elif isinstance(node, ast.Str):  # For older Python versions
                return node.s
            elif isinstance(node, ast.NameConstant):  # For older Python versions
                return node.value
            else:
                return ast.unparse(node)
        except Exception:
            return None
    
    def _infer_type(self, node: ast.AST) -> Optional[str]:
        """Infer type from AST node."""
        try:
            if isinstance(node, ast.Constant):
                return type(node.value).__name__
            elif isinstance(node, ast.List):
                return 'list'
            elif isinstance(node, ast.Dict):
                return 'dict'
            elif isinstance(node, ast.Set):
                return 'set'
            elif isinstance(node, ast.Tuple):
                return 'tuple'
            else:
                return None
        except Exception:
            return None
    
    def _calculate_complexity(self, node: ast.AST) -> int:
        """Calculate cyclomatic complexity."""
        complexity = 1  # Base complexity
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        
        return complexity


class CodeParser:
    """Main code parser for extracting information from Python files."""
    
    def __init__(self, include_private: bool = False):
        self.include_private = include_private
    
    def parse_file(self, file_path: str) -> ModuleInfo:
        """
        Parse a Python file and extract code information.
        
        Args:
            file_path: Path to the Python file
            
        Returns:
            ModuleInfo object with extracted information
        """
        try:
            content = read_file_safely(file_path)
            if not content:
                raise CodeParsingError(f"Could not read file: {file_path}")
            
            # Parse the file
            tree = ast.parse(content, filename=file_path)
            
            # Extract information using visitor
            visitor = ASTVisitor(file_path, self.include_private)
            visitor.visit(tree)
            
            # Create module info
            module_name = Path(file_path).stem
            module_info = ModuleInfo(
                name=module_name,
                file_path=file_path,
                docstring=visitor.module_docstring,
                classes=visitor.classes,
                functions=visitor.functions,
                constants=visitor.constants,
                imports=visitor.imports,
                is_package=module_name == '__init__',
                is_main=module_name == '__main__' or module_name == 'main',
                line_count=len(content.splitlines())
            )
            
            return module_info
            
        except SyntaxError as e:
            logger.error(f"Syntax error in {file_path}: {e}")
            raise CodeParsingError(f"Syntax error in {file_path}: {e}")
        except Exception as e:
            logger.error(f"Error parsing {file_path}: {e}")
            raise CodeParsingError(f"Error parsing {file_path}: {e}")
    
    def parse_code_string(self, code: str, file_path: str = "<string>") -> ModuleInfo:
        """
        Parse Python code from string.
        
        Args:
            code: Python code as string
            file_path: Virtual file path for reference
            
        Returns:
            ModuleInfo object with extracted information
        """
        try:
            tree = ast.parse(code, filename=file_path)
            
            visitor = ASTVisitor(file_path, self.include_private)
            visitor.visit(tree)
            
            module_name = Path(file_path).stem
            module_info = ModuleInfo(
                name=module_name,
                file_path=file_path,
                docstring=visitor.module_docstring,
                classes=visitor.classes,
                functions=visitor.functions,
                constants=visitor.constants,
                imports=visitor.imports,
                line_count=len(code.splitlines())
            )
            
            return module_info
            
        except SyntaxError as e:
            logger.error(f"Syntax error in code: {e}")
            raise CodeParsingError(f"Syntax error in code: {e}")
        except Exception as e:
            logger.error(f"Error parsing code: {e}")
            raise CodeParsingError(f"Error parsing code: {e}")


# Convenience functions
def extract_code_information(file_path: str, include_private: bool = False) -> ModuleInfo:
    """
    Extract code information from a Python file.
    
    Args:
        file_path: Path to the Python file
        include_private: Whether to include private methods/classes
        
    Returns:
        ModuleInfo object with extracted information
    """
    parser = CodeParser(include_private=include_private)
    return parser.parse_file(file_path)


def parse_python_file(file_path: str) -> ModuleInfo:
    """
    Parse a Python file and return module information.
    
    Args:
        file_path: Path to the Python file
        
    Returns:
        ModuleInfo object
    """
    parser = CodeParser()
    return parser.parse_file(file_path)


def analyze_ast_node(node: ast.AST) -> Dict[str, Any]:
    """
    Analyze an AST node and return information about it.
    
    Args:
        node: AST node to analyze
        
    Returns:
        Dictionary with node information
    """
    return {
        'type': type(node).__name__,
        'line_number': getattr(node, 'lineno', None),
        'col_offset': getattr(node, 'col_offset', None),
        'fields': node._fields if hasattr(node, '_fields') else []
    }


File: /workspaces/SYSC4918/src/parser/example_parser.py
"""
Example parser for extracting code examples and usage patterns from Python projects.

This module extracts code examples from docstrings, example files, and usage patterns
throughout the codebase to help generate comprehensive README documentation.
"""

import re
import ast
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging

from models.project_data import CodeExample, ModuleInfo, ClassInfo, FunctionInfo
from utils.file_utils import read_file_safely, find_files_by_pattern

logger = logging.getLogger(__name__)


class ExampleParser:
    """Parser for extracting code examples and usage patterns."""
    
    def __init__(self):
        # Patterns for identifying examples
        self.example_patterns = {
            'doctest': re.compile(r'^\s*>>>\s+(.+)$', re.MULTILINE),
            'code_block': re.compile(r'``````', re.DOTALL),
            'example_section': re.compile(r'(?:Examples?|Usage):?\s*\n(.*?)(?=\n\s*(?:[A-Z][a-z]+:|\Z))', re.DOTALL | re.IGNORECASE),
            'import_statement': re.compile(r'(?:^|\n)((?:from\s+\w+(?:\.\w+)*\s+)?import\s+[^\n]+)', re.MULTILINE),
            'function_call': re.compile(r'(\w+)\s*\([^)]*\)', re.MULTILINE),
        }
        
        # Common example file patterns
        self.example_file_patterns = [
            'example*.py', 'examples*.py', 'demo*.py', 'sample*.py',
            'tutorial*.py', 'quickstart*.py', 'getting_started*.py'
        ]
        
        # Usage indicators
        self.usage_indicators = [
            'usage', 'how to', 'example', 'demo', 'sample', 'tutorial',
            'quickstart', 'getting started', 'basic usage', 'advanced usage'
        ]
    
    def extract_from_module(self, module: ModuleInfo) -> List[CodeExample]:
        """
        Extract code examples from a module.
        
        Args:
            module: ModuleInfo object to extract examples from
            
        Returns:
            List of CodeExample objects
        """
        examples = []
        
        try:
            # Extract from module docstring
            if module.docstring:
                module_examples = self._extract_from_docstring(
                    module.docstring, 
                    f"Module {module.name}",
                    module.file_path
                )
                examples.extend(module_examples)
            
            # Extract from classes
            for class_info in module.classes:
                class_examples = self._extract_from_class(class_info)
                examples.extend(class_examples)
            
            # Extract from functions
            for function_info in module.functions:
                function_examples = self._extract_from_function(function_info)
                examples.extend(function_examples)
            
            # Extract from source code
            source_examples = self._extract_from_source_file(module.file_path)
            examples.extend(source_examples)
            
        except Exception as e:
            logger.error(f"Error extracting examples from module {module.name}: {e}")
        
        return examples
    
    def extract_from_project(self, project_path: str) -> List[CodeExample]:
        """
        Extract code examples from an entire project.
        
        Args:
            project_path: Path to project root
            
        Returns:
            List of CodeExample objects
        """
        examples = []
        
        try:
            project_path = Path(project_path)
            
            # Find example files
            example_files = self._find_example_files(project_path)
            
            # Extract from example files
            for file_path in example_files:
                file_examples = self._extract_from_example_file(file_path)
                examples.extend(file_examples)
            
            # Extract from README and documentation
            readme_examples = self._extract_from_readme(project_path)
            examples.extend(readme_examples)
            
            # Extract from test files (for usage patterns)
            test_examples = self._extract_from_test_files(project_path)
            examples.extend(test_examples)
            
        except Exception as e:
            logger.error(f"Error extracting examples from project: {e}")
        
        return examples
    
    def _extract_from_docstring(self, docstring: str, context: str, file_path: Optional[str] = None) -> List[CodeExample]:
        """Extract examples from a docstring."""
        examples = []
        
        # Extract doctests
        doctest_examples = self._extract_doctests(docstring, context, file_path)
        examples.extend(doctest_examples)
        
        # Extract code blocks
        code_block_examples = self._extract_code_blocks(docstring, context, file_path)
        examples.extend(code_block_examples)
        
        # Extract example sections
        example_sections = self._extract_example_sections(docstring, context, file_path)
        examples.extend(example_sections)
        
        return examples
    
    def _extract_from_class(self, class_info: ClassInfo) -> List[CodeExample]:
        """Extract examples from a class."""
        examples = []
        
        # Extract from class docstring
        if class_info.docstring:
            class_examples = self._extract_from_docstring(
                class_info.docstring,
                f"Class {class_info.name}",
                class_info.file_path
            )
            examples.extend(class_examples)
        
        # Extract from method docstrings
        for method in class_info.methods:
            if method.docstring:
                method_examples = self._extract_from_docstring(
                    method.docstring,
                    f"Method {class_info.name}.{method.name}",
                    method.file_path
                )
                examples.extend(method_examples)
        
        return examples
    
    def _extract_from_function(self, function_info: FunctionInfo) -> List[CodeExample]:
        """Extract examples from a function."""
        examples = []
        
        if function_info.docstring:
            function_examples = self._extract_from_docstring(
                function_info.docstring,
                f"Function {function_info.name}",
                function_info.file_path
            )
            examples.extend(function_examples)
        
        return examples
    
    def _extract_doctests(self, docstring: str, context: str, file_path: Optional[str] = None) -> List[CodeExample]:
        """Extract doctest examples from docstring."""
        examples = []
        
        # Find all doctest blocks
        doctest_blocks = []
        current_block = []
        
        for line in docstring.split('\n'):
            if re.match(r'^\s*>>>\s+', line):
                # Start of doctest line
                current_block.append(line.strip())
            elif re.match(r'^\s*\.\.\.\s+', line):
                # Continuation line
                current_block.append(line.strip())
            elif current_block and not line.strip():
                # Empty line, might be end of block
                continue
            elif current_block and not re.match(r'^\s*>>>', line):
                # Non-doctest line after doctest block
                if current_block:
                    doctest_blocks.append(current_block)
                    current_block = []
        
        # Add final block
        if current_block:
            doctest_blocks.append(current_block)
        
        # Convert blocks to examples
        for i, block in enumerate(doctest_blocks):
            if len(block) > 0:
                # Clean up doctest syntax
                code_lines = []
                for line in block:
                    if line.startswith('>>>'):
                        code_lines.append(line[3:].strip())
                    elif line.startswith('...'):
                        code_lines.append(line[3:].strip())
                
                code = '\n'.join(code_lines)
                
                example = CodeExample(
                    title=f"Doctest example from {context}",
                    code=code,
                    description=f"Doctest example {i+1}",
                    file_path=file_path,
                    example_type="doctest",
                    is_executable=True
                )
                examples.append(example)
        
        return examples
    
    def _extract_code_blocks(self, docstring: str, context: str, file_path: Optional[str] = None) -> List[CodeExample]:
        """Extract code blocks from docstring."""
        examples = []
        
        # Find code blocks
        code_blocks = self.example_patterns['code_block'].findall(docstring)
        
        for i, code_block in enumerate(code_blocks):
            code = code_block.strip()
            
            if code:
                example = CodeExample(
                    title=f"Code example from {context}",
                    code=code,
                    description=f"Code block {i+1}",
                    file_path=file_path,
                    example_type="code_block",
                    is_executable=self._is_executable_code(code)
                )
                examples.append(example)
        
        return examples
    
    def _extract_example_sections(self, docstring: str, context: str, file_path: Optional[str] = None) -> List[CodeExample]:
        """Extract example sections from docstring."""
        examples = []
        
        # Find example sections
        example_sections = self.example_patterns['example_section'].findall(docstring)
        
        for i, section in enumerate(example_sections):
            # Look for code within the section
            code_lines = []
            for line in section.split('\n'):
                line = line.strip()
                # Skip empty lines and common documentation patterns
                if line and not line.startswith(('Args:', 'Returns:', 'Raises:', 'Note:')):
                    # Check if line looks like code
                    if (line.startswith(('>>>', '...', 'import ', 'from ')) or
                        '(' in line or '=' in line or line.endswith(':')):
                        code_lines.append(line)
            
            if code_lines:
                code = '\n'.join(code_lines)
                
                example = CodeExample(
                    title=f"Example from {context}",
                    code=code,
                    description=f"Example section {i+1}",
                    file_path=file_path,
                    example_type="example_section",
                    is_executable=self._is_executable_code(code)
                )
                examples.append(example)
        
        return examples
    
    def _extract_from_source_file(self, file_path: str) -> List[CodeExample]:
        """Extract examples from source file by analyzing the code."""
        examples = []
        
        try:
            content = read_file_safely(file_path)
            if not content:
                return examples
            
            # Parse the file to find example patterns
            tree = ast.parse(content)
            
            # Look for if __name__ == "__main__": blocks
            main_examples = self._extract_main_block_examples(tree, file_path)
            examples.extend(main_examples)
            
            # Look for example functions
            example_functions = self._extract_example_functions(tree, file_path)
            examples.extend(example_functions)
            
        except Exception as e:
            logger.error(f"Error extracting examples from source file {file_path}: {e}")
        
        return examples
    
    def _extract_main_block_examples(self, tree: ast.AST, file_path: str) -> List[CodeExample]:
        """Extract examples from if __name__ == "__main__": blocks."""
        examples = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.If):
                # Check if this is a main block
                if self._is_main_block(node):
                    # Extract code from the main block
                    code_lines = []
                    for stmt in node.body:
                        try:
                            code_lines.append(ast.unparse(stmt))
                        except Exception:
                            pass
                    
                    if code_lines:
                        code = '\n'.join(code_lines)
                        
                        example = CodeExample(
                            title=f"Main block example from {Path(file_path).name}",
                            code=code,
                            description="Example usage from main block",
                            file_path=file_path,
                            line_number=node.lineno,
                            example_type="main_block",
                            is_executable=True
                        )
                        examples.append(example)
        
        return examples
    
    def _extract_example_functions(self, tree: ast.AST, file_path: str) -> List[CodeExample]:
        """Extract example functions."""
        examples = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # Check if function name suggests it's an example
                if any(indicator in node.name.lower() for indicator in self.usage_indicators):
                    # Extract function body
                    code_lines = []
                    for stmt in node.body:
                        try:
                            code_lines.append(ast.unparse(stmt))
                        except Exception:
                            pass
                    
                    if code_lines:
                        code = '\n'.join(code_lines)
                        
                        example = CodeExample(
                            title=f"Example function: {node.name}",
                            code=code,
                            description=f"Usage example from function {node.name}",
                            file_path=file_path,
                            line_number=node.lineno,
                            example_type="example_function",
                            is_executable=True
                        )
                        examples.append(example)
        
        return examples
    
    def _find_example_files(self, project_path: Path) -> List[str]:
        """Find example files in the project."""
        example_files = []
        
        # Common example directories
        example_dirs = ['examples', 'example', 'demos', 'demo', 'samples', 'sample']
        
        for dir_name in example_dirs:
            example_dir = project_path / dir_name
            if example_dir.exists():
                for pattern in self.example_file_patterns:
                    files = find_files_by_pattern(str(example_dir), pattern)
                    example_files.extend(files)
        
        # Also check root directory
        for pattern in self.example_file_patterns:
            files = find_files_by_pattern(str(project_path), pattern, recursive=False)
            example_files.extend(files)
        
        return example_files
    
    def _extract_from_example_file(self, file_path: str) -> List[CodeExample]:
        """Extract examples from example files."""
        examples = []
        
        try:
            content = read_file_safely(file_path)
            if not content:
                return examples
            
            # Parse the file
            tree = ast.parse(content)
            
            # Extract module docstring
            module_docstring = ast.get_docstring(tree)
            if module_docstring:
                docstring_examples = self._extract_from_docstring(
                    module_docstring,
                    f"Example file {Path(file_path).name}",
                    file_path
                )
                examples.extend(docstring_examples)
            
            # Extract main content as example
            example = CodeExample(
                title=f"Example: {Path(file_path).stem}",
                code=content,
                description=f"Complete example from {Path(file_path).name}",
                file_path=file_path,
                example_type="example_file",
                is_executable=True
            )
            examples.append(example)
            
        except Exception as e:
            logger.error(f"Error extracting from example file {file_path}: {e}")
        
        return examples
    
    def _extract_from_readme(self, project_path: Path) -> List[CodeExample]:
        """Extract examples from README files."""
        examples = []
        
        # Look for README files
        readme_patterns = ['README.md', 'README.rst', 'README.txt', 'readme.md', 'readme.rst']
        
        for pattern in readme_patterns:
            readme_file = project_path / pattern
            if readme_file.exists():
                try:
                    content = read_file_safely(str(readme_file))
                    if content:
                        # Extract code blocks
                        code_blocks = self.example_patterns['code_block'].findall(content)
                        
                        for i, code_block in enumerate(code_blocks):
                            code = code_block.strip()
                            if code and ('import ' in code or 'from ' in code):
                                example = CodeExample(
                                    title=f"README example {i+1}",
                                    code=code,
                                    description=f"Usage example from README",
                                    file_path=str(readme_file),
                                    example_type="readme",
                                    is_executable=self._is_executable_code(code)
                                )
                                examples.append(example)
                
                except Exception as e:
                    logger.error(f"Error extracting from README {readme_file}: {e}")
        
        return examples
    
    def _extract_from_test_files(self, project_path: Path) -> List[CodeExample]:
        """Extract usage patterns from test files."""
        examples = []
        
        # Find test files
        test_patterns = ['test_*.py', '*_test.py']
        test_dirs = ['tests', 'test']
        
        for test_dir in test_dirs:
            test_directory = project_path / test_dir
            if test_directory.exists():
                for pattern in test_patterns:
                    test_files = find_files_by_pattern(str(test_directory), pattern)
                    
                    for test_file in test_files[:5]:  # Limit to 5 test files
                        try:
                            file_examples = self._extract_usage_from_test_file(test_file)
                            examples.extend(file_examples)
                        except Exception as e:
                            logger.error(f"Error extracting from test file {test_file}: {e}")
        
        return examples
    
    def _extract_usage_from_test_file(self, test_file: str) -> List[CodeExample]:
        """Extract usage patterns from a test file."""
        examples = []
        
        try:
            content = read_file_safely(test_file)
            if not content:
                return examples
            
            tree = ast.parse(content)
            
            # Look for test functions
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
                    # Extract simple usage patterns
                    code_lines = []
                    for stmt in node.body:
                        # Look for simple statements that show usage
                        if isinstance(stmt, ast.Expr) or isinstance(stmt, ast.Assign):
                            try:
                                line = ast.unparse(stmt)
                                if any(pattern in line for pattern in ['assert', '=', '(']):
                                    code_lines.append(line)
                            except Exception:
                                pass
                    
                    if code_lines and len(code_lines) <= 5:  # Keep it simple
                        code = '\n'.join(code_lines)
                        
                        example = CodeExample(
                            title=f"Test usage: {node.name}",
                            code=code,
                            description=f"Usage pattern from test {node.name}",
                            file_path=test_file,
                            line_number=node.lineno,
                            example_type="test_usage",
                            is_executable=False
                        )
                        examples.append(example)
        
        except Exception as e:
            logger.error(f"Error parsing test file {test_file}: {e}")
        
        return examples
    
    def _is_main_block(self, node: ast.If) -> bool:
        """Check if an if statement is a main block."""
        try:
            # Check if condition is __name__ == "__main__"
            if isinstance(node.test, ast.Compare):
                left = node.test.left
                comparators = node.test.comparators
                
                if (isinstance(left, ast.Name) and left.id == '__name__' and
                    len(comparators) == 1 and isinstance(comparators[0], ast.Constant) and
                    comparators[0].value == '__main__'):
                    return True
        except Exception:
            pass
        
        return False
    
    def _is_executable_code(self, code: str) -> bool:
        """Check if code is likely executable."""
        # Simple heuristics to determine if code is executable
        indicators = [
            'import ', 'from ', 'def ', 'class ', 'if ', 'for ', 'while ',
            'try:', 'with ', 'assert ', 'return ', 'yield ', 'print('
        ]
        
        return any(indicator in code for indicator in indicators)


# Convenience functions
def extract_code_examples(module: ModuleInfo) -> List[CodeExample]:
    """
    Extract code examples from a module.
    
    Args:
        module: ModuleInfo object to extract examples from
        
    Returns:
        List of CodeExample objects
    """
    parser = ExampleParser()
    return parser.extract_from_module(module)


def parse_docstring_examples(docstring: str, context: str = "Unknown") -> List[CodeExample]:
    """
    Parse code examples from a docstring.
    
    Args:
        docstring: Docstring text to parse
        context: Context description for the examples
        
    Returns:
        List of CodeExample objects
    """
    parser = ExampleParser()
    return parser._extract_from_docstring(docstring, context)


def find_usage_patterns(project_path: str) -> List[CodeExample]:
    """
    Find usage patterns in a Python project.
    
    Args:
        project_path: Path to project root
        
    Returns:
        List of CodeExample objects
    """
    parser = ExampleParser()
    return parser.extract_from_project(project_path)


File: /workspaces/SYSC4918/src/parser/dependency_parser.py
"""
Dependency parser for extracting project dependencies and requirements.

This module parses dependencies from requirements.txt, pyproject.toml, setup.py,
and other dependency specification files commonly found in Python projects.
"""

import os
import re
import ast
import configparser
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
import logging

from models.project_data import DependencyInfo
from utils.file_utils import read_file_safely, FileInfo

logger = logging.getLogger(__name__)

# Try to import TOML parsers
try:
    import tomllib  # Python 3.11+
except ImportError:
    try:
        import tomli as tomllib  # Fallback for older Python versions
    except ImportError:
        tomllib = None


class DependencyParser:
    """Parser for extracting project dependencies from various sources."""
    
    def __init__(self):
        self.requirement_patterns = {
            'basic': re.compile(r'^([a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])'),
            'versioned': re.compile(r'^([a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])\s*([<>=!~]+.*)?'),
            'extras': re.compile(r'^([a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])\[([^\]]+)\]'),
            'git': re.compile(r'^git\+https?://'),
            'url': re.compile(r'^https?://'),
            'file': re.compile(r'^file://'),
            'editable': re.compile(r'^-e\s+(.+)'),
            'constraint': re.compile(r'^-c\s+(.+)'),
            'requirement': re.compile(r'^-r\s+(.+)'),
        }
        
        # Development dependency indicators
        self.dev_indicators = {
            'test', 'testing', 'dev', 'development', 'docs', 'documentation',
            'lint', 'linting', 'format', 'formatting', 'quality', 'qa',
            'build', 'deploy', 'ci', 'cd', 'coverage', 'profiling'
        }
    
    def parse(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> DependencyInfo:
        """
        Parse project dependencies from various sources.
        
        Args:
            project_path: Path to project root
            project_files: Dictionary of categorized project files
            
        Returns:
            DependencyInfo object with extracted dependencies
        """
        dependency_info = DependencyInfo()
        
        # Parse from different sources
        sources_parsed = []
        
        # 1. Parse pyproject.toml (modern standard)
        if self._parse_pyproject_toml(project_path, dependency_info):
            sources_parsed.append('pyproject.toml')
        
        # 2. Parse requirements.txt files
        req_files_parsed = self._parse_requirements_files(project_path, dependency_info)
        sources_parsed.extend(req_files_parsed)
        
        # 3. Parse setup.py
        if self._parse_setup_py(project_path, dependency_info):
            sources_parsed.append('setup.py')
        
        # 4. Parse setup.cfg
        if self._parse_setup_cfg(project_path, dependency_info):
            sources_parsed.append('setup.cfg')
        
        # 5. Parse Pipfile (if present)
        if self._parse_pipfile(project_path, dependency_info):
            sources_parsed.append('Pipfile')
        
        # 6. Detect Python version requirements
        self._detect_python_version(project_path, dependency_info)
        
        # 7. Clean and deduplicate dependencies
        self._clean_dependencies(dependency_info)
        
        logger.info(f"Parsed dependencies from sources: {sources_parsed}")
        return dependency_info
    
    def _parse_pyproject_toml(self, project_path: Path, dependency_info: DependencyInfo) -> bool:
        """Parse dependencies from pyproject.toml file."""
        if not tomllib:
            logger.warning("TOML parser not available, skipping pyproject.toml")
            return False
        
        pyproject_file = project_path / 'pyproject.toml'
        if not pyproject_file.exists():
            return False
        
        try:
            content = read_file_safely(str(pyproject_file))
            if not content:
                return False
            
            data = tomllib.loads(content)
            
            # Parse [project] section (PEP 621)
            if 'project' in data:
                project_data = data['project']
                
                # Main dependencies
                if 'dependencies' in project_data:
                    deps = project_data['dependencies']
                    dependency_info.production.extend(self._normalize_dependencies(deps))
                
                # Optional dependencies
                if 'optional-dependencies' in project_data:
                    optional_deps = project_data['optional-dependencies']
                    for group_name, deps in optional_deps.items():
                        normalized_deps = self._normalize_dependencies(deps)
                        if self._is_dev_dependency_group(group_name):
                            dependency_info.development.extend(normalized_deps)
                        else:
                            dependency_info.optional[group_name] = normalized_deps
                
                # Python version requirement
                if 'requires-python' in project_data:
                    dependency_info.python_requires = project_data['requires-python']
            
            # Parse [tool.poetry] section
            if 'tool' in data and 'poetry' in data['tool']:
                poetry_data = data['tool']['poetry']
                
                # Main dependencies
                if 'dependencies' in poetry_data:
                    deps = poetry_data['dependencies']
                    # Remove python version requirement from dependencies
                    python_req = deps.pop('python', None)
                    if python_req and not dependency_info.python_requires:
                        dependency_info.python_requires = python_req
                    
                    # Convert Poetry format to standard format
                    poetry_deps = self._convert_poetry_dependencies(deps)
                    dependency_info.production.extend(poetry_deps)
                
                # Development dependencies
                if 'group' in poetry_data:
                    groups = poetry_data['group']
                    for group_name, group_data in groups.items():
                        if 'dependencies' in group_data:
                            deps = self._convert_poetry_dependencies(group_data['dependencies'])
                            if self._is_dev_dependency_group(group_name):
                                dependency_info.development.extend(deps)
                            else:
                                dependency_info.optional[group_name] = deps
                
                # Legacy dev-dependencies
                if 'dev-dependencies' in poetry_data:
                    dev_deps = self._convert_poetry_dependencies(poetry_data['dev-dependencies'])
                    dependency_info.development.extend(dev_deps)
            
            # Parse [tool.pdm] section
            if 'tool' in data and 'pdm' in data['tool']:
                pdm_data = data['tool']['pdm']
                
                if 'dev-dependencies' in pdm_data:
                    dev_groups = pdm_data['dev-dependencies']
                    for group_name, deps in dev_groups.items():
                        normalized_deps = self._normalize_dependencies(deps)
                        if self._is_dev_dependency_group(group_name):
                            dependency_info.development.extend(normalized_deps)
                        else:
                            dependency_info.optional[group_name] = normalized_deps
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing pyproject.toml: {e}")
            return False
    
    def _parse_requirements_files(self, project_path: Path, dependency_info: DependencyInfo) -> List[str]:
        """Parse various requirements.txt files."""
        files_parsed = []
        
        # Common requirements file patterns
        req_patterns = [
            'requirements.txt',
            'requirements-dev.txt',
            'requirements-test.txt',
            'requirements-docs.txt',
            'requirements-build.txt',
            'dev-requirements.txt',
            'test-requirements.txt',
            'docs-requirements.txt',
        ]
        
        for pattern in req_patterns:
            req_file = project_path / pattern
            if req_file.exists():
                try:
                    deps = self._parse_requirements_file(str(req_file))
                    if deps:
                        # Categorize based on filename
                        if any(indicator in pattern.lower() for indicator in self.dev_indicators):
                            dependency_info.development.extend(deps)
                        else:
                            dependency_info.production.extend(deps)
                        files_parsed.append(pattern)
                except Exception as e:
                    logger.error(f"Error parsing {pattern}: {e}")
        
        return files_parsed
    
    def _parse_requirements_file(self, file_path: str) -> List[str]:
        """Parse a single requirements.txt file."""
        content = read_file_safely(file_path)
        if not content:
            return []
        
        dependencies = []
        
        for line in content.split('\n'):
            line = line.strip()
            
            # Skip empty lines and comments
            if not line or line.startswith('#'):
                continue
            
            # Handle different requirement formats
            if line.startswith('-r '):
                # Reference to another requirements file
                # TODO: Could recursively parse referenced files
                continue
            elif line.startswith('-c '):
                # Constraint file
                continue
            elif line.startswith('-e '):
                # Editable install
                editable_match = self.requirement_patterns['editable'].match(line)
                if editable_match:
                    dep_spec = editable_match.group(1)
                    dependencies.append(dep_spec)
            elif line.startswith('-'):
                # Other pip options, skip
                continue
            else:
                # Regular dependency
                # Remove inline comments
                if '#' in line:
                    line = line.split('#')[0].strip()
                
                if line:
                    dependencies.append(line)
        
        return dependencies
    
    def _parse_setup_py(self, project_path: Path, dependency_info: DependencyInfo) -> bool:
        """Parse dependencies from setup.py file."""
        setup_file = project_path / 'setup.py'
        if not setup_file.exists():
            return False
        
        try:
            content = read_file_safely(str(setup_file))
            if not content:
                return False
            
            # Parse setup.py as AST
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Call):
                    if (isinstance(node.func, ast.Name) and node.func.id == 'setup') or \
                       (isinstance(node.func, ast.Attribute) and node.func.attr == 'setup'):
                        
                        # Extract dependency-related arguments
                        for keyword in node.keywords:
                            self._extract_setup_dependencies(keyword, dependency_info)
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing setup.py: {e}")
            return False
    
    def _parse_setup_cfg(self, project_path: Path, dependency_info: DependencyInfo) -> bool:
        """Parse dependencies from setup.cfg file."""
        setup_cfg_file = project_path / 'setup.cfg'
        if not setup_cfg_file.exists():
            return False
        
        try:
            content = read_file_safely(str(setup_cfg_file))
            if not content:
                return False
            
            config = configparser.ConfigParser()
            config.read_string(content)
            
            # Parse [options] section
            if 'options' in config:
                options = config['options']
                
                # Install requires
                if 'install_requires' in options:
                    deps = self._parse_multiline_deps(options['install_requires'])
                    dependency_info.production.extend(deps)
                
                # Python requires
                if 'python_requires' in options:
                    dependency_info.python_requires = options['python_requires']
            
            # Parse [options.extras_require] section
            if 'options.extras_require' in config:
                extras = config['options.extras_require']
                for extra_name, deps_str in extras.items():
                    deps = self._parse_multiline_deps(deps_str)
                    if self._is_dev_dependency_group(extra_name):
                        dependency_info.development.extend(deps)
                    else:
                        dependency_info.extras_require[extra_name] = deps
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing setup.cfg: {e}")
            return False
    
    def _parse_pipfile(self, project_path: Path, dependency_info: DependencyInfo) -> bool:
        """Parse dependencies from Pipfile."""
        if not tomllib:
            return False
        
        pipfile = project_path / 'Pipfile'
        if not pipfile.exists():
            return False
        
        try:
            content = read_file_safely(str(pipfile))
            if not content:
                return False
            
            data = tomllib.loads(content)
            
            # Parse [packages] section
            if 'packages' in data:
                packages = data['packages']
                deps = self._convert_pipfile_dependencies(packages)
                dependency_info.production.extend(deps)
            
            # Parse [dev-packages] section
            if 'dev-packages' in data:
                dev_packages = data['dev-packages']
                deps = self._convert_pipfile_dependencies(dev_packages)
                dependency_info.development.extend(deps)
            
            # Parse [requires] section for Python version
            if 'requires' in data:
                requires = data['requires']
                if 'python_version' in requires:
                    dependency_info.python_requires = f">={requires['python_version']}"
                elif 'python_full_version' in requires:
                    dependency_info.python_requires = f">={requires['python_full_version']}"
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing Pipfile: {e}")
            return False
    
    def _detect_python_version(self, project_path: Path, dependency_info: DependencyInfo) -> None:
        """Detect Python version requirements from various sources."""
        if dependency_info.python_requires:
            return  # Already detected
        
        # Check for .python-version file
        python_version_file = project_path / '.python-version'
        if python_version_file.exists():
            try:
                content = read_file_safely(str(python_version_file))
                if content:
                    version = content.strip()
                    dependency_info.python_requires = f">={version}"
                    return
            except Exception:
                pass
        
        # Check for runtime.txt (Heroku)
        runtime_file = project_path / 'runtime.txt'
        if runtime_file.exists():
            try:
                content = read_file_safely(str(runtime_file))
                if content:
                    match = re.search(r'python-(\d+\.\d+\.\d+)', content)
                    if match:
                        version = match.group(1)
                        dependency_info.python_requires = f">={version}"
                        return
            except Exception:
                pass
        
        # Check for GitHub Actions workflow files
        github_workflows = project_path / '.github' / 'workflows'
        if github_workflows.exists():
            try:
                for workflow_file in github_workflows.glob('*.yml'):
                    content = read_file_safely(str(workflow_file))
                    if content:
                        # Look for python-version in matrix
                        python_versions = re.findall(r'python-version:.*?[\[\'"]([\d.]+)', content)
                        if python_versions:
                            min_version = min(python_versions)
                            dependency_info.python_requires = f">={min_version}"
                            return
            except Exception:
                pass
    
    def _normalize_dependencies(self, deps: List[str]) -> List[str]:
        """Normalize dependency specifications."""
        normalized = []
        
        for dep in deps:
            if isinstance(dep, str):
                # Clean up the dependency string
                dep = dep.strip()
                if dep:
                    normalized.append(dep)
            elif isinstance(dep, dict):
                # Handle dict format (from some TOML files)
                if 'name' in dep:
                    dep_str = dep['name']
                    if 'version' in dep:
                        dep_str += dep['version']
                    normalized.append(dep_str)
        
        return normalized
    
    def _convert_poetry_dependencies(self, deps: Dict[str, Any]) -> List[str]:
        """Convert Poetry dependency format to standard format."""
        dependencies = []
        
        for name, spec in deps.items():
            if isinstance(spec, str):
                # Simple version specification
                if spec == '*':
                    dependencies.append(name)
                else:
                    dependencies.append(f"{name}{spec}")
            elif isinstance(spec, dict):
                # Complex specification
                dep_str = name
                
                if 'version' in spec:
                    version = spec['version']
                    if version != '*':
                        dep_str += version
                
                if 'extras' in spec:
                    extras = spec['extras']
                    if isinstance(extras, list):
                        extras_str = ','.join(extras)
                        dep_str = f"{name}[{extras_str}]"
                        if 'version' in spec and spec['version'] != '*':
                            dep_str += spec['version']
                
                dependencies.append(dep_str)
        
        return dependencies
    
    def _convert_pipfile_dependencies(self, deps: Dict[str, Any]) -> List[str]:
        """Convert Pipfile dependency format to standard format."""
        dependencies = []
        
        for name, spec in deps.items():
            if isinstance(spec, str):
                # Simple version specification
                if spec == '*':
                    dependencies.append(name)
                else:
                    dependencies.append(f"{name}{spec}")
            elif isinstance(spec, dict):
                # Complex specification
                dep_str = name
                
                if 'version' in spec:
                    version = spec['version']
                    if version != '*':
                        dep_str += version
                
                dependencies.append(dep_str)
        
        return dependencies
    
    def _extract_setup_dependencies(self, keyword: ast.keyword, dependency_info: DependencyInfo) -> None:
        """Extract dependencies from setup() call arguments."""
        try:
            arg_name = keyword.arg
            
            if arg_name == 'install_requires':
                deps = self._extract_list_from_ast(keyword.value)
                dependency_info.production.extend(deps)
            
            elif arg_name == 'extras_require':
                if isinstance(keyword.value, ast.Dict):
                    for key, value in zip(keyword.value.keys, keyword.value.values):
                        if isinstance(key, ast.Constant):
                            extra_name = key.value
                            deps = self._extract_list_from_ast(value)
                            if self._is_dev_dependency_group(extra_name):
                                dependency_info.development.extend(deps)
                            else:
                                dependency_info.extras_require[extra_name] = deps
            
            elif arg_name == 'python_requires':
                if isinstance(keyword.value, ast.Constant):
                    dependency_info.python_requires = keyword.value.value
        
        except Exception as e:
            logger.debug(f"Error extracting setup dependencies for {keyword.arg}: {e}")
    
    def _extract_list_from_ast(self, node: ast.AST) -> List[str]:
        """Extract list of strings from AST node."""
        if isinstance(node, ast.List):
            result = []
            for item in node.elts:
                if isinstance(item, ast.Constant):
                    result.append(item.value)
            return result
        elif isinstance(node, ast.Constant):
            return [node.value]
        else:
            return []
    
    def _parse_multiline_deps(self, deps_str: str) -> List[str]:
        """Parse multiline dependency string from setup.cfg."""
        deps = []
        for line in deps_str.split('\n'):
            line = line.strip()
            if line and not line.startswith('#'):
                deps.append(line)
        return deps
    
    def _is_dev_dependency_group(self, group_name: str) -> bool:
        """Check if a dependency group is for development."""
        return any(indicator in group_name.lower() for indicator in self.dev_indicators)
    
    def _clean_dependencies(self, dependency_info: DependencyInfo) -> None:
        """Clean and deduplicate dependencies."""
        # Remove duplicates while preserving order
        dependency_info.production = list(dict.fromkeys(dependency_info.production))
        dependency_info.development = list(dict.fromkeys(dependency_info.development))
        
        # Clean optional dependencies
        for group_name in dependency_info.optional:
            dependency_info.optional[group_name] = list(dict.fromkeys(dependency_info.optional[group_name]))
        
        # Clean extras_require
        for extra_name in dependency_info.extras_require:
            dependency_info.extras_require[extra_name] = list(dict.fromkeys(dependency_info.extras_require[extra_name]))
        
        # Remove empty groups
        dependency_info.optional = {k: v for k, v in dependency_info.optional.items() if v}
        dependency_info.extras_require = {k: v for k, v in dependency_info.extras_require.items() if v}


# Convenience functions
def extract_dependencies(project_path: str, project_files: Dict[str, List[FileInfo]]) -> DependencyInfo:
    """
    Extract dependencies from a project directory.
    
    Args:
        project_path: Path to project root
        project_files: Dictionary of categorized project files
        
    Returns:
        DependencyInfo object
    """
    parser = DependencyParser()
    return parser.parse(Path(project_path), project_files)


def parse_requirements_file(file_path: str) -> List[str]:
    """
    Parse a single requirements.txt file.
    
    Args:
        file_path: Path to requirements file
        
    Returns:
        List of dependency specifications
    """
    parser = DependencyParser()
    return parser._parse_requirements_file(file_path)


def detect_python_version(project_path: str) -> Optional[str]:
    """
    Detect Python version requirements for a project.
    
    Args:
        project_path: Path to project root
        
    Returns:
        Python version requirement string or None
    """
    parser = DependencyParser()
    dependency_info = DependencyInfo()
    parser._detect_python_version(Path(project_path), dependency_info)
    return dependency_info.python_requires


File: /workspaces/SYSC4918/src/parser/metadata_parser.py
"""
Project metadata parser for extracting information from various configuration files.

This module extracts project metadata from setup.py, pyproject.toml, setup.cfg,
and other configuration files commonly found in Python projects.
"""

import os
import re
import ast
import configparser
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import logging

from models.project_data import ProjectMetadata, ProjectType, LicenseType
from utils.file_utils import read_file_safely, FileInfo

logger = logging.getLogger(__name__)

# Try to import TOML parsers
try:
    import tomllib  # Python 3.11+
except ImportError:
    try:
        import tomli as tomllib  # Fallback for older Python versions
    except ImportError:
        tomllib = None

# Try to import git for repository information
try:
    import git
except ImportError:
    git = None


class MetadataParser:
    """Parser for extracting project metadata from various configuration files."""
    
    def __init__(self):
        self.license_patterns = {
            'MIT': ['mit', 'mit license'],
            'Apache-2.0': ['apache', 'apache license', 'apache-2.0', 'apache 2.0'],
            'GPL-3.0': ['gpl', 'gpl-3.0', 'gpl v3', 'gnu general public license'],
            'BSD-3-Clause': ['bsd', 'bsd license', 'bsd-3-clause'],
            'Unlicense': ['unlicense', 'public domain'],
        }
        
        # Common project type indicators
        self.project_type_patterns = {
            ProjectType.CLI_TOOL: ['cli', 'command', 'tool', 'script'],
            ProjectType.WEB_APPLICATION: ['web', 'webapp', 'django', 'flask', 'fastapi'],
            ProjectType.API: ['api', 'rest', 'graphql', 'endpoint'],
            ProjectType.LIBRARY: ['library', 'lib', 'package', 'module'],
            ProjectType.APPLICATION: ['app', 'application', 'gui', 'desktop'],
        }
    
    def parse(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> ProjectMetadata:
        """
        Parse project metadata from various configuration files.
        
        Args:
            project_path: Path to project root
            project_files: Dictionary of categorized project files
            
        Returns:
            ProjectMetadata object with extracted information
        """
        metadata = ProjectMetadata(project_name=project_path.name)
        
        # Parse from different sources in order of preference
        sources_parsed = []
        
        # 1. Try pyproject.toml first (modern standard)
        if self._parse_pyproject_toml(project_path, metadata):
            sources_parsed.append('pyproject.toml')
        
        # 2. Try setup.py
        if self._parse_setup_py(project_path, metadata):
            sources_parsed.append('setup.py')
        
        # 3. Try setup.cfg
        if self._parse_setup_cfg(project_path, metadata):
            sources_parsed.append('setup.cfg')
        
        # 4. Try package __init__.py files
        self._parse_package_init(project_path, metadata, project_files)
        
        # 5. Extract git information
        self._extract_git_info(project_path, metadata)
        
        # 6. Detect project type
        metadata.project_type = self._detect_project_type(metadata, project_files)
        
        # 7. Find license information
        metadata.license = self._detect_license(project_path, metadata)
        
        logger.info(f"Parsed metadata from sources: {sources_parsed}")
        return metadata
    
    def _parse_pyproject_toml(self, project_path: Path, metadata: ProjectMetadata) -> bool:
        """Parse metadata from pyproject.toml file."""
        if not tomllib:
            logger.warning("TOML parser not available, skipping pyproject.toml")
            return False
        
        pyproject_file = project_path / 'pyproject.toml'
        if not pyproject_file.exists():
            return False
        
        try:
            content = read_file_safely(str(pyproject_file))
            if not content:
                return False
            
            # Parse TOML content
            data = tomllib.loads(content)
            
            # Extract project metadata from [project] section
            if 'project' in data:
                project_data = data['project']
                
                metadata.project_name = project_data.get('name', metadata.project_name)
                metadata.description = project_data.get('description')
                metadata.version = project_data.get('version')
                metadata.python_version = project_data.get('requires-python')
                
                # Handle authors
                if 'authors' in project_data:
                    authors = project_data['authors']
                    if isinstance(authors, list) and authors:
                        author_info = authors[0]
                        if isinstance(author_info, dict):
                            metadata.author = author_info.get('name')
                            metadata.author_email = author_info.get('email')
                        else:
                            metadata.author = str(author_info)
                
                # Handle maintainers as fallback
                if not metadata.author and 'maintainers' in project_data:
                    maintainers = project_data['maintainers']
                    if isinstance(maintainers, list) and maintainers:
                        maintainer_info = maintainers[0]
                        if isinstance(maintainer_info, dict):
                            metadata.author = maintainer_info.get('name')
                            metadata.author_email = maintainer_info.get('email')
                
                # Handle URLs
                if 'urls' in project_data:
                    urls = project_data['urls']
                    metadata.homepage = urls.get('homepage') or urls.get('Home')
                    metadata.repository = urls.get('repository') or urls.get('Repository')
                
                # Handle license
                if 'license' in project_data:
                    license_info = project_data['license']
                    if isinstance(license_info, dict):
                        if 'text' in license_info:
                            metadata.license = self._parse_license_string(license_info['text'])
                        elif 'file' in license_info:
                            license_file = project_path / license_info['file']
                            if license_file.exists():
                                license_content = read_file_safely(str(license_file))
                                if license_content:
                                    metadata.license = self._parse_license_string(license_content)
                    else:
                        metadata.license = self._parse_license_string(str(license_info))
                
                # Handle keywords and classifiers
                metadata.keywords = project_data.get('keywords', [])
                metadata.classifiers = project_data.get('classifiers', [])
            
            # Extract build system info
            if 'build-system' in data:
                build_system = data['build-system']
                if 'requires' in build_system:
                    # This could help determine project type
                    build_requires = build_system['requires']
                    if any('setuptools' in req for req in build_requires):
                        # Traditional setuptools project
                        pass
            
            # Extract tool-specific metadata
            if 'tool' in data:
                self._parse_tool_metadata(data['tool'], metadata)
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing pyproject.toml: {e}")
            return False
    
    def _parse_setup_py(self, project_path: Path, metadata: ProjectMetadata) -> bool:
        """Parse metadata from setup.py file."""
        setup_file = project_path / 'setup.py'
        if not setup_file.exists():
            return False
        
        try:
            content = read_file_safely(str(setup_file))
            if not content:
                return False
            
            # Parse setup.py as AST to extract setup() call arguments
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Call):
                    if (isinstance(node.func, ast.Name) and node.func.id == 'setup') or \
                       (isinstance(node.func, ast.Attribute) and node.func.attr == 'setup'):
                        
                        # Extract arguments from setup() call
                        for keyword in node.keywords:
                            self._extract_setup_argument(keyword, metadata, content)
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing setup.py: {e}")
            return False
    
    def _parse_setup_cfg(self, project_path: Path, metadata: ProjectMetadata) -> bool:
        """Parse metadata from setup.cfg file."""
        setup_cfg_file = project_path / 'setup.cfg'
        if not setup_cfg_file.exists():
            return False
        
        try:
            content = read_file_safely(str(setup_cfg_file))
            if not content:
                return False
            
            config = configparser.ConfigParser()
            config.read_string(content)
            
            # Extract metadata from [metadata] section
            if 'metadata' in config:
                metadata_section = config['metadata']
                
                metadata.project_name = metadata_section.get('name', metadata.project_name)
                metadata.version = metadata_section.get('version', metadata.version)
                metadata.description = metadata_section.get('description', metadata.description)
                metadata.author = metadata_section.get('author', metadata.author)
                metadata.author_email = metadata_section.get('author_email', metadata.author_email)
                metadata.homepage = metadata_section.get('url', metadata.homepage)
                
                # Handle license
                license_str = metadata_section.get('license')
                if license_str:
                    metadata.license = self._parse_license_string(license_str)
                
                # Handle classifiers
                classifiers_str = metadata_section.get('classifiers')
                if classifiers_str:
                    metadata.classifiers = [c.strip() for c in classifiers_str.split('\n') if c.strip()]
                
                # Handle keywords
                keywords_str = metadata_section.get('keywords')
                if keywords_str:
                    metadata.keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
            
            return True
            
        except Exception as e:
            logger.error(f"Error parsing setup.cfg: {e}")
            return False
    
    def _parse_package_init(self, project_path: Path, metadata: ProjectMetadata, 
                           project_files: Dict[str, List[FileInfo]]) -> None:
        """Parse metadata from package __init__.py files."""
        try:
            # Look for __init__.py files in the project
            init_files = []
            for file_info in project_files.get('python', []):
                if file_info.path.endswith('__init__.py'):
                    init_files.append(file_info.path)
            
            # Find the main package __init__.py
            main_init = None
            for init_file in init_files:
                rel_path = os.path.relpath(init_file, project_path)
                # Look for the shortest path (likely main package)
                if main_init is None or len(rel_path.split(os.sep)) < len(os.path.relpath(main_init, project_path).split(os.sep)):
                    main_init = init_file
            
            if main_init:
                content = read_file_safely(main_init)
                if content:
                    # Extract version from __version__ variable
                    version_match = re.search(r'__version__\s*=\s*["\']([^"\']+)["\']', content)
                    if version_match and not metadata.version:
                        metadata.version = version_match.group(1)
                    
                    # Extract author from __author__ variable
                    author_match = re.search(r'__author__\s*=\s*["\']([^"\']+)["\']', content)
                    if author_match and not metadata.author:
                        metadata.author = author_match.group(1)
                    
                    # Extract description from docstring
                    if not metadata.description:
                        docstring_match = re.search(r'"""([^"]+)"""', content)
                        if docstring_match:
                            metadata.description = docstring_match.group(1).strip()
        
        except Exception as e:
            logger.error(f"Error parsing package __init__.py: {e}")
    
    def _extract_git_info(self, project_path: Path, metadata: ProjectMetadata) -> None:
        """Extract repository information from git."""
        if not git:
            return
        
        try:
            repo = git.Repo(project_path)
            
            # Get remote URL
            if repo.remotes:
                remote_url = repo.remotes.origin.url
                if remote_url:
                    # Clean up git URL
                    if remote_url.startswith('git@'):
                        # Convert SSH to HTTPS
                        remote_url = remote_url.replace('git@github.com:', 'https://github.com/')
                        remote_url = remote_url.replace('.git', '')
                    elif remote_url.endswith('.git'):
                        remote_url = remote_url[:-4]
                    
                    if not metadata.repository:
                        metadata.repository = remote_url
                    if not metadata.homepage:
                        metadata.homepage = remote_url
        
        except Exception as e:
            logger.debug(f"Could not extract git info: {e}")
    
    def _detect_project_type(self, metadata: ProjectMetadata, 
                           project_files: Dict[str, List[FileInfo]]) -> ProjectType:
        """Detect project type based on metadata and file structure."""
        # Check description and name for keywords
        text_to_check = f"{metadata.description or ''} {metadata.project_name}".lower()
        
        for project_type, keywords in self.project_type_patterns.items():
            if any(keyword in text_to_check for keyword in keywords):
                return project_type
        
        # Check file structure
        python_files = project_files.get('python', [])
        
        # Look for specific patterns
        has_main = any('main.py' in f.path for f in python_files)
        has_cli = any('cli' in f.path.lower() for f in python_files)
        has_webapp_files = any(any(framework in f.path.lower() for framework in ['django', 'flask', 'fastapi']) 
                              for f in python_files)
        
        if has_cli or has_main:
            return ProjectType.CLI_TOOL
        elif has_webapp_files:
            return ProjectType.WEB_APPLICATION
        elif len(python_files) > 5:  # Larger projects likely libraries
            return ProjectType.LIBRARY
        else:
            return ProjectType.APPLICATION
    
    def _detect_license(self, project_path: Path, metadata: ProjectMetadata) -> Optional[LicenseType]:
        """Detect license from license files or metadata."""
        # If already detected from metadata, return it
        if metadata.license:
            return metadata.license
        
        # Look for license files
        license_files = ['LICENSE', 'LICENSE.txt', 'LICENSE.md', 'COPYING', 'COPYING.txt']
        
        for license_file in license_files:
            file_path = project_path / license_file
            if file_path.exists():
                content = read_file_safely(str(file_path))
                if content:
                    detected_license = self._parse_license_string(content)
                    if detected_license:
                        return detected_license
        
        return None
    
    def _parse_license_string(self, license_str: str) -> Optional[LicenseType]:
        """Parse license string to determine license type."""
        license_lower = license_str.lower()
        
        for license_type, patterns in self.license_patterns.items():
            if any(pattern in license_lower for pattern in patterns):
                return LicenseType(license_type)
        
        return None
    
    def _extract_setup_argument(self, keyword: ast.keyword, metadata: ProjectMetadata, content: str) -> None:
        """Extract argument from setup() call."""
        try:
            arg_name = keyword.arg
            
            if arg_name == 'name' and isinstance(keyword.value, ast.Constant):
                metadata.project_name = keyword.value.value
            elif arg_name == 'version' and isinstance(keyword.value, ast.Constant):
                metadata.version = keyword.value.value
            elif arg_name == 'description' and isinstance(keyword.value, ast.Constant):
                metadata.description = keyword.value.value
            elif arg_name == 'author' and isinstance(keyword.value, ast.Constant):
                metadata.author = keyword.value.value
            elif arg_name == 'author_email' and isinstance(keyword.value, ast.Constant):
                metadata.author_email = keyword.value.value
            elif arg_name == 'url' and isinstance(keyword.value, ast.Constant):
                metadata.homepage = keyword.value.value
            elif arg_name == 'license' and isinstance(keyword.value, ast.Constant):
                metadata.license = self._parse_license_string(keyword.value.value)
            elif arg_name == 'classifiers' and isinstance(keyword.value, ast.List):
                classifiers = []
                for item in keyword.value.elts:
                    if isinstance(item, ast.Constant):
                        classifiers.append(item.value)
                metadata.classifiers = classifiers
            elif arg_name == 'keywords' and isinstance(keyword.value, ast.List):
                keywords = []
                for item in keyword.value.elts:
                    if isinstance(item, ast.Constant):
                        keywords.append(item.value)
                metadata.keywords = keywords
        
        except Exception as e:
            logger.debug(f"Error extracting setup argument {keyword.arg}: {e}")
    
    def _parse_tool_metadata(self, tool_data: Dict[str, Any], metadata: ProjectMetadata) -> None:
        """Parse tool-specific metadata from pyproject.toml."""
        # Handle Poetry metadata
        if 'poetry' in tool_data:
            poetry_data = tool_data['poetry']
            
            if not metadata.project_name:
                metadata.project_name = poetry_data.get('name', metadata.project_name)
            if not metadata.version:
                metadata.version = poetry_data.get('version')
            if not metadata.description:
                metadata.description = poetry_data.get('description')
            if not metadata.author:
                authors = poetry_data.get('authors', [])
                if authors and isinstance(authors, list):
                    # Parse "Name <email>" format
                    author_str = authors[0]
                    if '<' in author_str:
                        name, email = author_str.split('<', 1)
                        metadata.author = name.strip()
                        metadata.author_email = email.strip('>')
                    else:
                        metadata.author = author_str
            if not metadata.homepage:
                metadata.homepage = poetry_data.get('homepage')
            if not metadata.repository:
                metadata.repository = poetry_data.get('repository')
            if not metadata.license:
                license_str = poetry_data.get('license')
                if license_str:
                    metadata.license = self._parse_license_string(license_str)
            if not metadata.keywords:
                metadata.keywords = poetry_data.get('keywords', [])
            if not metadata.classifiers:
                metadata.classifiers = poetry_data.get('classifiers', [])


# Convenience functions
def extract_project_metadata(project_path: str, project_files: Dict[str, List[FileInfo]]) -> ProjectMetadata:
    """
    Extract project metadata from a project directory.
    
    Args:
        project_path: Path to project root
        project_files: Dictionary of categorized project files
        
    Returns:
        ProjectMetadata object
    """
    parser = MetadataParser()
    return parser.parse(Path(project_path), project_files)


def detect_project_type(project_path: str, project_files: Dict[str, List[FileInfo]]) -> ProjectType:
    """
    Detect the type of a Python project.
    
    Args:
        project_path: Path to project root
        project_files: Dictionary of categorized project files
        
    Returns:
        ProjectType enum value
    """
    parser = MetadataParser()
    metadata = parser.parse(Path(project_path), project_files)
    return metadata.project_type


def find_license_info(project_path: str) -> Optional[LicenseType]:
    """
    Find license information for a project.
    
    Args:
        project_path: Path to project root
        
    Returns:
        LicenseType enum value or None
    """
    parser = MetadataParser()
    metadata = ProjectMetadata(project_name="temp")
    return parser._detect_license(Path(project_path), metadata)


File: /workspaces/SYSC4918/src/parser/structure_parser.py
"""
Project structure parser for analyzing Python project organization.

This module analyzes the overall structure of Python projects, identifying
main packages, entry points, directory organization, and project layout patterns.
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
import configparser
import logging

from models.project_data import ProjectStructure, ModuleInfo, EntryPoint
from utils.file_utils import read_file_safely, is_python_file, FileInfo

logger = logging.getLogger(__name__)

# Try to import TOML parsers
try:
    import tomllib  # Python 3.11+
except ImportError:
    try:
        import tomli as tomllib  # Fallback for older Python versions
    except ImportError:
        tomllib = None


class StructureParser:
    """Parser for analyzing Python project structure."""
    
    def __init__(self):
        # Common entry point indicators
        self.entry_point_patterns = [
            'main.py', '__main__.py', 'app.py', 'cli.py', 'run.py',
            'manage.py', 'wsgi.py', 'asgi.py', 'server.py'
        ]
        
        # Common package structures
        self.package_indicators = [
            '__init__.py', 'setup.py', 'pyproject.toml', 'setup.cfg'
        ]
        
        # Directory patterns to analyze
        self.important_dirs = {
            'src': 'source code',
            'lib': 'library code',
            'tests': 'test files',
            'test': 'test files',
            'docs': 'documentation',
            'doc': 'documentation', 
            'examples': 'example code',
            'example': 'example code',
            'scripts': 'utility scripts',
            'bin': 'executable scripts',
            'data': 'data files',
            'assets': 'static assets',
            'static': 'static files',
            'templates': 'template files',
            'config': 'configuration files',
            'configs': 'configuration files',
        }
    
    def parse(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> ProjectStructure:
        """
        Parse project structure from project directory.
        
        Args:
            project_path: Path to project root
            project_files: Dictionary of categorized project files
            
        Returns:
            ProjectStructure object with analyzed structure
        """
        try:
            structure = ProjectStructure(root_path=str(project_path))
            
            # Analyze layout type
            structure.src_layout = self._detect_src_layout(project_path)
            
            # Find main package
            structure.main_package = self._find_main_package(project_path, project_files)
            
            # Discover packages
            structure.packages = self._discover_packages(project_path, project_files)
            
            # Create module info objects
            structure.modules = self._create_module_info_list(project_files)
            
            # Find entry points
            structure.entry_points = self._find_entry_points(project_path, project_files)
            
            # Categorize directories
            structure.config_files = self._find_config_files(project_path)
            structure.data_directories = self._find_data_directories(project_path)
            structure.test_directories = self._find_test_directories(project_path)
            structure.doc_directories = self._find_doc_directories(project_path)
            
            # Calculate statistics
            structure.total_files = sum(len(files) for files in project_files.values())
            structure.total_lines = self._calculate_total_lines(project_files)
            
            return structure
            
        except Exception as e:
            logger.error(f"Error analyzing project structure: {e}")
            return ProjectStructure(root_path=str(project_path))
    
    def _detect_src_layout(self, project_path: Path) -> bool:
        """Detect if project uses src/ layout."""
        src_dir = project_path / 'src'
        return src_dir.exists() and src_dir.is_dir()
    
    def _find_main_package(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> Optional[str]:
        """Find the main package of the project."""
        # Look for packages in order of preference
        candidates = []
        
        # Check for src/ layout
        if self._detect_src_layout(project_path):
            src_dir = project_path / 'src'
            for item in src_dir.iterdir():
                if item.is_dir() and (item / '__init__.py').exists():
                    candidates.append(item.name)
        
        # Check for direct package layout
        for item in project_path.iterdir():
            if (item.is_dir() and 
                (item / '__init__.py').exists() and 
                not item.name.startswith('.') and
                item.name not in ['tests', 'test', 'docs', 'doc', 'examples']):
                candidates.append(item.name)
        
        # Prefer package with same name as project directory
        project_name = project_path.name.replace('-', '_')
        if project_name in candidates:
            return project_name
        
        # Return first candidate
        return candidates[0] if candidates else None
    
    def _discover_packages(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> List[str]:
        """Discover all Python packages in the project."""
        packages = []
        
        for file_info in project_files.get('python', []):
            file_path = Path(file_info.path)
            
            # Check if it's an __init__.py file
            if file_path.name == '__init__.py':
                # Get package name from directory
                package_dir = file_path.parent
                rel_path = package_dir.relative_to(project_path)
                
                # Convert path to package name
                package_name = str(rel_path).replace(os.sep, '.')
                
                # Skip if it's in common non-package directories
                if not any(part in ['tests', 'test', 'docs', 'doc', 'examples'] 
                          for part in rel_path.parts):
                    packages.append(package_name)
        
        return sorted(packages)
    
    def _create_module_info_list(self, project_files: Dict[str, List[FileInfo]]) -> List[ModuleInfo]:
        """Create ModuleInfo objects for Python files."""
        modules = []
        
        for file_info in project_files.get('python', []):
            if is_python_file(file_info.path):
                module_name = Path(file_info.path).stem
                
                # Create basic module info (will be populated by code parser)
                module_info = ModuleInfo(
                    name=module_name,
                    file_path=file_info.path,
                    is_package=module_name == '__init__',
                    is_main=module_name in ['__main__', 'main'],
                    line_count=self._count_lines_in_file(file_info.path)
                )
                
                modules.append(module_info)
        
        return modules
    
    def _find_entry_points(self, project_path: Path, project_files: Dict[str, List[FileInfo]]) -> List[EntryPoint]:
        """Find project entry points."""
        entry_points = []
        
        # Check for common entry point files
        for file_info in project_files.get('python', []):
            file_path = Path(file_info.path)
            
            if file_path.name in self.entry_point_patterns:
                entry_point = EntryPoint(
                    name=file_path.stem,
                    module=self._path_to_module_name(file_path, project_path),
                    script_path=file_info.path,
                    description=f"Entry point: {file_path.name}"
                )
                entry_points.append(entry_point)
        
        # Check setup.py for console scripts
        setup_py = project_path / 'setup.py'
        if setup_py.exists():
            console_scripts = self._extract_console_scripts_from_setup_py(setup_py)
            entry_points.extend(console_scripts)
        
        # Check pyproject.toml for scripts
        pyproject_toml = project_path / 'pyproject.toml'
        if pyproject_toml.exists():
            scripts = self._extract_scripts_from_pyproject_toml(pyproject_toml)
            entry_points.extend(scripts)
        
        return entry_points
    
    def _extract_console_scripts_from_setup_py(self, setup_py: Path) -> List[EntryPoint]:
        """Extract console scripts from setup.py."""
        entry_points = []
        
        try:
            content = read_file_safely(str(setup_py))
            if not content:
                return entry_points
            
            # Look for entry_points in setup() call
            # This is a simplified approach - in practice you'd want to parse AST
            entry_points_match = re.search(
                r'entry_points\s*=\s*{[^}]*["\']console_scripts["\']\s*:\s*\[(.*?)\]',
                content, re.DOTALL
            )
            
            if entry_points_match:
                scripts_text = entry_points_match.group(1)
                # Extract individual script definitions
                for match in re.finditer(r'["\']([^"\']+)\s*=\s*([^"\']+)["\']', scripts_text):
                    script_name = match.group(1)
                    module_func = match.group(2)
                    
                    if ':' in module_func:
                        module, func = module_func.split(':', 1)
                        entry_point = EntryPoint(
                            name=script_name,
                            module=module,
                            function=func,
                            description=f"Console script: {script_name}"
                        )
                        entry_points.append(entry_point)
        
        except Exception as e:
            logger.error(f"Error extracting console scripts from setup.py: {e}")
        
        return entry_points
    
    def _extract_scripts_from_pyproject_toml(self, pyproject_toml: Path) -> List[EntryPoint]:
        """Extract scripts from pyproject.toml."""
        entry_points = []
        
        if not tomllib:
            return entry_points
        
        try:
            content = read_file_safely(str(pyproject_toml))
            if not content:
                return entry_points
            
            data = tomllib.loads(content)
            
            # Check [project.scripts] section
            if 'project' in data and 'scripts' in data['project']:
                scripts = data['project']['scripts']
                for script_name, module_func in scripts.items():
                    if ':' in module_func:
                        module, func = module_func.split(':', 1)
                        entry_point = EntryPoint(
                            name=script_name,
                            module=module,
                            function=func,
                            description=f"Project script: {script_name}"
                        )
                        entry_points.append(entry_point)
            
            # Check [tool.poetry.scripts] section
            if 'tool' in data and 'poetry' in data['tool'] and 'scripts' in data['tool']['poetry']:
                scripts = data['tool']['poetry']['scripts']
                for script_name, module_func in scripts.items():
                    if ':' in module_func:
                        module, func = module_func.split(':', 1)
                        entry_point = EntryPoint(
                            name=script_name,
                            module=module,
                            function=func,
                            description=f"Poetry script: {script_name}"
                        )
                        entry_points.append(entry_point)
        
        except Exception as e:
            logger.error(f"Error extracting scripts from pyproject.toml: {e}")
        
        return entry_points
    
    def _find_config_files(self, project_path: Path) -> List[str]:
        """Find configuration files in the project."""
        config_files = []
        
        # Common configuration file patterns
        config_patterns = [
            '*.ini', '*.cfg', '*.conf', '*.config', '*.yaml', '*.yml',
            '*.json', '*.toml', '.env*', '*.properties'
        ]
        
        for pattern in config_patterns:
            for file_path in project_path.glob(f"**/{pattern}"):
                if file_path.is_file():
                    config_files.append(str(file_path))
        
        # Add specific configuration files
        specific_configs = [
            'setup.cfg', 'pyproject.toml', 'tox.ini', 'pytest.ini',
            '.flake8', '.pylintrc', 'mypy.ini', '.pre-commit-config.yaml'
        ]
        
        for config_name in specific_configs:
            config_path = project_path / config_name
            if config_path.exists():
                config_files.append(str(config_path))
        
        return sorted(config_files)
    
    def _find_data_directories(self, project_path: Path) -> List[str]:
        """Find data directories in the project."""
        data_dirs = []
        
        data_patterns = ['data', 'assets', 'static', 'resources', 'fixtures']
        
        for pattern in data_patterns:
            for dir_path in project_path.glob(f"**/{pattern}"):
                if dir_path.is_dir():
                    data_dirs.append(str(dir_path))
        
        return sorted(data_dirs)
    
    def _find_test_directories(self, project_path: Path) -> List[str]:
        """Find test directories in the project."""
        test_dirs = []
        
        test_patterns = ['tests', 'test', 'testing']
        
        for pattern in test_patterns:
            for dir_path in project_path.glob(f"**/{pattern}"):
                if dir_path.is_dir():
                    test_dirs.append(str(dir_path))
        
        return sorted(test_dirs)
    
    def _find_doc_directories(self, project_path: Path) -> List[str]:
        """Find documentation directories in the project."""
        doc_dirs = []
        
        doc_patterns = ['docs', 'doc', 'documentation', 'sphinx']
        
        for pattern in doc_patterns:
            for dir_path in project_path.glob(f"**/{pattern}"):
                if dir_path.is_dir():
                    doc_dirs.append(str(dir_path))
        
        return sorted(doc_dirs)
    
    def _path_to_module_name(self, file_path: Path, project_path: Path) -> str:
        """Convert file path to module name."""
        try:
            rel_path = file_path.relative_to(project_path)
            
            # Remove .py extension
            if rel_path.suffix == '.py':
                rel_path = rel_path.with_suffix('')
            
            # Convert to module name
            module_name = str(rel_path).replace(os.sep, '.')
            
            # Handle __init__.py files
            if module_name.endswith('.__init__'):
                module_name = module_name[:-9]
            
            return module_name
        
        except ValueError:
            # File is not under project path
            return file_path.stem
    
    def _calculate_total_lines(self, project_files: Dict[str, List[FileInfo]]) -> int:
        """Calculate total lines of code in the project."""
        total_lines = 0
        
        for file_info in project_files.get('python', []):
            total_lines += self._count_lines_in_file(file_info.path)
        
        return total_lines
    
    def _count_lines_in_file(self, file_path: str) -> int:
        """Count lines in a file."""
        try:
            content = read_file_safely(file_path)
            if content:
                return len(content.splitlines())
        except Exception:
            pass
        return 0


# Convenience functions
def analyze_project_structure(project_path: str, project_files: Dict[str, List[FileInfo]]) -> ProjectStructure:
    """
    Analyze the structure of a Python project.
    
    Args:
        project_path: Path to project root
        project_files: Dictionary of categorized project files
        
    Returns:
        ProjectStructure object
    """
    parser = StructureParser()
    return parser.parse(Path(project_path), project_files)


def find_entry_points(project_path: str) -> List[EntryPoint]:
    """
    Find entry points in a Python project.
    
    Args:
        project_path: Path to project root
        
    Returns:
        List of EntryPoint objects
    """
    parser = StructureParser()
    
    # Get project files (simplified for this function)
    from ..utils.file_utils import get_project_files
    project_files = get_project_files(project_path)
    
    return parser._find_entry_points(Path(project_path), project_files)


def categorize_directories(project_path: str) -> Dict[str, List[str]]:
    """
    Categorize directories in a Python project.
    
    Args:
        project_path: Path to project root
        
    Returns:
        Dictionary mapping categories to directory lists
    """
    parser = StructureParser()
    project_path = Path(project_path)
    
    return {
        'config': parser._find_config_files(project_path),
        'data': parser._find_data_directories(project_path),
        'tests': parser._find_test_directories(project_path),
        'docs': parser._find_doc_directories(project_path)
    }


File: /workspaces/SYSC4918/src/models/__init__.py


File: /workspaces/SYSC4918/src/models/project_data.py
"""
Data classes representing parsed project information for README generation.

This module contains all the data structures used to store and organize
information extracted from Python projects during the parsing process.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Union
from enum import Enum
from pathlib import Path


class ProjectType(Enum):
    """Enumeration of common Python project types."""
    LIBRARY = "library"
    APPLICATION = "application"
    CLI_TOOL = "cli_tool"
    WEB_APPLICATION = "web_application"
    API = "api"
    PACKAGE = "package"
    UNKNOWN = "unknown"


class LicenseType(Enum):
    """Enumeration of common license types."""
    MIT = "MIT"
    GPL_V3 = "GPL-3.0"
    APACHE_2 = "Apache-2.0"
    BSD_3 = "BSD-3-Clause"
    UNLICENSE = "Unlicense"
    PROPRIETARY = "Proprietary"
    UNKNOWN = "Unknown"


@dataclass
class ProjectMetadata:
    """Core project metadata extracted from setup files and git."""
    
    project_name: str
    description: Optional[str] = None
    version: Optional[str] = None
    author: Optional[str] = None
    author_email: Optional[str] = None
    license: Optional[LicenseType] = None
    homepage: Optional[str] = None
    repository: Optional[str] = None
    python_version: Optional[str] = None
    project_type: ProjectType = ProjectType.UNKNOWN
    keywords: List[str] = field(default_factory=list)
    classifiers: List[str] = field(default_factory=list)


@dataclass
class DependencyInfo:
    """Information about project dependencies and requirements."""
    
    production: List[str] = field(default_factory=list)
    development: List[str] = field(default_factory=list)
    optional: Dict[str, List[str]] = field(default_factory=dict)
    python_requires: Optional[str] = None
    extras_require: Dict[str, List[str]] = field(default_factory=dict)
    
    def get_all_dependencies(self) -> List[str]:
        """Get all dependencies combined."""
        all_deps = self.production + self.development
        for extra_deps in self.optional.values():
            all_deps.extend(extra_deps)
        for extra_deps in self.extras_require.values():
            all_deps.extend(extra_deps)
        return list(set(all_deps))


@dataclass
class FunctionInfo:
    """Information about a function or method."""
    
    name: str
    signature: str
    docstring: Optional[str] = None
    is_public: bool = True
    is_async: bool = False
    is_property: bool = False
    is_classmethod: bool = False
    is_staticmethod: bool = False
    decorators: List[str] = field(default_factory=list)
    line_number: Optional[int] = None
    file_path: Optional[str] = None
    return_type: Optional[str] = None
    parameters: List[Dict[str, Any]] = field(default_factory=list)
    complexity_score: Optional[int] = None


@dataclass
class ClassInfo:
    """Information about a class."""
    
    name: str
    docstring: Optional[str] = None
    methods: List[FunctionInfo] = field(default_factory=list)
    properties: List[FunctionInfo] = field(default_factory=list)
    inheritance: List[str] = field(default_factory=list)
    decorators: List[str] = field(default_factory=list)
    is_abstract: bool = False
    is_dataclass: bool = False
    is_enum: bool = False
    line_number: Optional[int] = None
    file_path: Optional[str] = None
    attributes: List[Dict[str, Any]] = field(default_factory=list)
    
    def get_public_methods(self) -> List[FunctionInfo]:
        """Get only public methods."""
        return [method for method in self.methods if method.is_public]
    
    def get_public_properties(self) -> List[FunctionInfo]:
        """Get only public properties."""
        return [prop for prop in self.properties if prop.is_public]


@dataclass
class ModuleInfo:
    """Information about a Python module."""
    
    name: str
    file_path: str
    docstring: Optional[str] = None
    classes: List[ClassInfo] = field(default_factory=list)
    functions: List[FunctionInfo] = field(default_factory=list)
    constants: List[Dict[str, Any]] = field(default_factory=list)
    imports: List[str] = field(default_factory=list)
    is_package: bool = False
    is_main: bool = False
    line_count: int = 0


@dataclass
class EntryPoint:
    """Information about project entry points."""
    
    name: str
    module: str
    function: Optional[str] = None
    script_path: Optional[str] = None
    description: Optional[str] = None


@dataclass
class ProjectStructure:
    """Information about the overall project structure."""
    
    root_path: str
    main_package: Optional[str] = None
    src_layout: bool = False
    packages: List[str] = field(default_factory=list)
    modules: List[ModuleInfo] = field(default_factory=list)
    entry_points: List[EntryPoint] = field(default_factory=list)
    config_files: List[str] = field(default_factory=list)
    data_directories: List[str] = field(default_factory=list)
    test_directories: List[str] = field(default_factory=list)
    doc_directories: List[str] = field(default_factory=list)
    total_files: int = 0
    total_lines: int = 0
    
    def get_main_modules(self) -> List[ModuleInfo]:
        """Get modules that are likely entry points."""
        return [module for module in self.modules if module.is_main]


@dataclass
class CodeExample:
    """A code example extracted from the project."""
    
    title: str
    code: str
    description: Optional[str] = None
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    example_type: str = "usage"  # usage, configuration, test, etc.
    language: str = "python"
    is_executable: bool = True
    expected_output: Optional[str] = None


@dataclass
class ConfigurationInfo:
    """Information about project configuration."""
    
    config_files: List[str] = field(default_factory=list)
    environment_variables: List[Dict[str, str]] = field(default_factory=list)
    default_settings: Dict[str, Any] = field(default_factory=dict)
    config_examples: List[CodeExample] = field(default_factory=list)


@dataclass
class TestInfo:
    """Information about project tests."""
    
    test_directories: List[str] = field(default_factory=list)
    test_files: List[str] = field(default_factory=list)
    test_framework: Optional[str] = None
    coverage_files: List[str] = field(default_factory=list)
    total_tests: int = 0


@dataclass
class DocumentationInfo:
    """Information about project documentation."""
    
    readme_file: Optional[str] = None
    changelog_file: Optional[str] = None
    license_file: Optional[str] = None
    doc_directories: List[str] = field(default_factory=list)
    doc_files: List[str] = field(default_factory=list)
    has_sphinx: bool = False
    has_mkdocs: bool = False


@dataclass
class ProjectData:
    """Complete parsed project information."""
    
    metadata: ProjectMetadata
    dependencies: DependencyInfo
    structure: ProjectStructure
    configuration: ConfigurationInfo
    examples: List[CodeExample] = field(default_factory=list)
    tests: Optional[TestInfo] = None
    documentation: Optional[DocumentationInfo] = None
    parsing_errors: List[str] = field(default_factory=list)
    token_count: int = 0
    parsing_timestamp: Optional[str] = None
    
    def get_api_documentation(self) -> Dict[str, Any]:
        """Get formatted API documentation."""
        return {
            "classes": [
                {
                    "name": cls.name,
                    "docstring": cls.docstring,
                    "methods": [
                        {
                            "name": method.name,
                            "signature": method.signature,
                            "docstring": method.docstring,
                            "is_public": method.is_public
                        }
                        for method in cls.get_public_methods()
                    ],
                    "inheritance": cls.inheritance,
                    "file_path": cls.file_path
                }
                for module in self.structure.modules
                for cls in module.classes
                if cls.name and not cls.name.startswith('_')
            ],
            "functions": [
                {
                    "name": func.name,
                    "signature": func.signature,
                    "docstring": func.docstring,
                    "is_public": func.is_public,
                    "file_path": func.file_path
                }
                for module in self.structure.modules
                for func in module.functions
                if func.is_public and not func.name.startswith('_')
            ]
        }
    
    def get_summary_stats(self) -> Dict[str, Any]:
        """Get summary statistics about the project."""
        total_classes = sum(len(module.classes) for module in self.structure.modules)
        total_functions = sum(len(module.functions) for module in self.structure.modules)
        public_classes = sum(
            len([cls for cls in module.classes if not cls.name.startswith('_')])
            for module in self.structure.modules
        )
        public_functions = sum(
            len([func for func in module.functions if func.is_public])
            for module in self.structure.modules
        )
        
        return {
            "total_files": self.structure.total_files,
            "total_lines": self.structure.total_lines,
            "total_classes": total_classes,
            "total_functions": total_functions,
            "public_classes": public_classes,
            "public_functions": public_functions,
            "total_dependencies": len(self.dependencies.get_all_dependencies()),
            "total_examples": len(self.examples),
            "token_count": self.token_count
        }


File: /workspaces/SYSC4918/src/models/schemas.py
"""
JSON schema definitions for project data validation and documentation.

This module provides schema definitions that correspond to the data classes
in project_data.py, enabling validation and documentation of the JSON output.
"""

from typing import Dict, Any

# Schema for function/method information
FUNCTION_SCHEMA = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "signature": {"type": "string"},
        "docstring": {"type": ["string", "null"]},
        "is_public": {"type": "boolean"},
        "is_async": {"type": "boolean"},
        "is_property": {"type": "boolean"},
        "is_classmethod": {"type": "boolean"},
        "is_staticmethod": {"type": "boolean"},
        "decorators": {
            "type": "array",
            "items": {"type": "string"}
        },
        "line_number": {"type": ["integer", "null"]},
        "file_path": {"type": ["string", "null"]},
        "return_type": {"type": ["string", "null"]},
        "parameters": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "type": {"type": ["string", "null"]},
                    "default": {"type": ["string", "null"]},
                    "description": {"type": ["string", "null"]}
                },
                "required": ["name"]
            }
        },
        "complexity_score": {"type": ["integer", "null"]}
    },
    "required": ["name", "signature"]
}

# Schema for class information
CLASS_SCHEMA = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "docstring": {"type": ["string", "null"]},
        "methods": {
            "type": "array",
            "items": FUNCTION_SCHEMA
        },
        "properties": {
            "type": "array",
            "items": FUNCTION_SCHEMA
        },
        "inheritance": {
            "type": "array",
            "items": {"type": "string"}
        },
        "decorators": {
            "type": "array",
            "items": {"type": "string"}
        },
        "is_abstract": {"type": "boolean"},
        "is_dataclass": {"type": "boolean"},
        "is_enum": {"type": "boolean"},
        "line_number": {"type": ["integer", "null"]},
        "file_path": {"type": ["string", "null"]},
        "attributes": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "type": {"type": ["string", "null"]},
                    "description": {"type": ["string", "null"]}
                },
                "required": ["name"]
            }
        }
    },
    "required": ["name"]
}

# Schema for module information
MODULE_SCHEMA = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "file_path": {"type": "string"},
        "docstring": {"type": ["string", "null"]},
        "classes": {
            "type": "array",
            "items": CLASS_SCHEMA
        },
        "functions": {
            "type": "array",
            "items": FUNCTION_SCHEMA
        },
        "constants": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "value": {"type": ["string", "number", "boolean", "null"]},
                    "type": {"type": ["string", "null"]},
                    "description": {"type": ["string", "null"]}
                },
                "required": ["name"]
            }
        },
        "imports": {
            "type": "array",
            "items": {"type": "string"}
        },
        "is_package": {"type": "boolean"},
        "is_main": {"type": "boolean"},
        "line_count": {"type": "integer"}
    },
    "required": ["name", "file_path"]
}

# Schema for project metadata
METADATA_SCHEMA = {
    "type": "object",
    "properties": {
        "project_name": {"type": "string"},
        "description": {"type": ["string", "null"]},
        "version": {"type": ["string", "null"]},
        "author": {"type": ["string", "null"]},
        "author_email": {"type": ["string", "null"]},
        "license": {
            "type": ["string", "null"],
            "enum": ["MIT", "GPL-3.0", "Apache-2.0", "BSD-3-Clause", "Unlicense", "Proprietary", "Unknown", None]
        },
        "homepage": {"type": ["string", "null"]},
        "repository": {"type": ["string", "null"]},
        "python_version": {"type": ["string", "null"]},
        "project_type": {
            "type": "string",
            "enum": ["library", "application", "cli_tool", "web_application", "api", "package", "unknown"]
        },
        "keywords": {
            "type": "array",
            "items": {"type": "string"}
        },
        "classifiers": {
            "type": "array",
            "items": {"type": "string"}
        }
    },
    "required": ["project_name"]
}

# Schema for dependencies
DEPENDENCIES_SCHEMA = {
    "type": "object",
    "properties": {
        "production": {
            "type": "array",
            "items": {"type": "string"}
        },
        "development": {
            "type": "array",
            "items": {"type": "string"}
        },
        "optional": {
            "type": "object",
            "patternProperties": {
                ".*": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            }
        },
        "python_requires": {"type": ["string", "null"]},
        "extras_require": {
            "type": "object",
            "patternProperties": {
                ".*": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            }
        }
    },
    "required": ["production", "development"]
}

# Schema for entry points
ENTRY_POINT_SCHEMA = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "module": {"type": "string"},
        "function": {"type": ["string", "null"]},
        "script_path": {"type": ["string", "null"]},
        "description": {"type": ["string", "null"]}
    },
    "required": ["name", "module"]
}

# Schema for project structure
STRUCTURE_SCHEMA = {
    "type": "object",
    "properties": {
        "root_path": {"type": "string"},
        "main_package": {"type": ["string", "null"]},
        "src_layout": {"type": "boolean"},
        "packages": {
            "type": "array",
            "items": {"type": "string"}
        },
        "modules": {
            "type": "array",
            "items": MODULE_SCHEMA
        },
        "entry_points": {
            "type": "array",
            "items": ENTRY_POINT_SCHEMA
        },
        "config_files": {
            "type": "array",
            "items": {"type": "string"}
        },
        "data_directories": {
            "type": "array",
            "items": {"type": "string"}
        },
        "test_directories": {
            "type": "array",
            "items": {"type": "string"}
        },
        "doc_directories": {
            "type": "array",
            "items": {"type": "string"}
        },
        "total_files": {"type": "integer"},
        "total_lines": {"type": "integer"}
    },
    "required": ["root_path", "total_files", "total_lines"]
}

# Schema for code examples
CODE_EXAMPLE_SCHEMA = {
    "type": "object",
    "properties": {
        "title": {"type": "string"},
        "code": {"type": "string"},
        "description": {"type": ["string", "null"]},
        "file_path": {"type": ["string", "null"]},
        "line_number": {"type": ["integer", "null"]},
        "example_type": {"type": "string"},
        "language": {"type": "string"},
        "is_executable": {"type": "boolean"},
        "expected_output": {"type": ["string", "null"]}
    },
    "required": ["title", "code"]
}

# Schema for configuration information
CONFIGURATION_SCHEMA = {
    "type": "object",
    "properties": {
        "config_files": {
            "type": "array",
            "items": {"type": "string"}
        },
        "environment_variables": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "description": {"type": ["string", "null"]},
                    "default": {"type": ["string", "null"]},
                    "required": {"type": "boolean"}
                },
                "required": ["name"]
            }
        },
        "default_settings": {
            "type": "object",
            "patternProperties": {
                ".*": {}
            }
        },
        "config_examples": {
            "type": "array",
            "items": CODE_EXAMPLE_SCHEMA
        }
    }
}

# Schema for test information
TEST_INFO_SCHEMA = {
    "type": "object",
    "properties": {
        "test_directories": {
            "type": "array",
            "items": {"type": "string"}
        },
        "test_files": {
            "type": "array",
            "items": {"type": "string"}
        },
        "test_framework": {"type": ["string", "null"]},
        "coverage_files": {
            "type": "array",
            "items": {"type": "string"}
        },
        "total_tests": {"type": "integer"}
    }
}

# Schema for documentation information
DOCUMENTATION_INFO_SCHEMA = {
    "type": "object",
    "properties": {
        "readme_file": {"type": ["string", "null"]},
        "changelog_file": {"type": ["string", "null"]},
        "license_file": {"type": ["string", "null"]},
        "doc_directories": {
            "type": "array",
            "items": {"type": "string"}
        },
        "doc_files": {
            "type": "array",
            "items": {"type": "string"}
        },
        "has_sphinx": {"type": "boolean"},
        "has_mkdocs": {"type": "boolean"}
    }
}

# Main schema for the complete project data
PROJECT_DATA_SCHEMA = {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "properties": {
        "metadata": METADATA_SCHEMA,
        "dependencies": DEPENDENCIES_SCHEMA,
        "structure": STRUCTURE_SCHEMA,
        "configuration": CONFIGURATION_SCHEMA,
        "examples": {
            "type": "array",
            "items": CODE_EXAMPLE_SCHEMA
        },
        "tests": {
            "oneOf": [
                {"type": "null"},
                TEST_INFO_SCHEMA
            ]
        },
        "documentation": {
            "oneOf": [
                {"type": "null"},
                DOCUMENTATION_INFO_SCHEMA
            ]
        },
        "parsing_errors": {
            "type": "array",
            "items": {"type": "string"}
        },
        "token_count": {"type": "integer"},
        "parsing_timestamp": {"type": ["string", "null"]}
    },
    "required": ["metadata", "dependencies", "structure", "configuration"]
}


def validate_project_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate project data against the schema.
    
    Args:
        data: The project data to validate
        
    Returns:
        Dict containing validation results
    """
    try:
        import jsonschema
        jsonschema.validate(data, PROJECT_DATA_SCHEMA)
        return {"valid": True, "errors": []}
    except ImportError:
        return {"valid": None, "errors": ["jsonschema library not available"]}
    except jsonschema.ValidationError as e:
        return {"valid": False, "errors": [str(e)]}
    except Exception as e:
        return {"valid": False, "errors": [f"Validation error: {str(e)}"]}


def get_schema_version() -> str:
    """Get the current schema version."""
    return "1.0.0"


def get_schema_documentation() -> Dict[str, Any]:
    """Get human-readable documentation for the schema."""
    return {
        "version": get_schema_version(),
        "description": "JSON schema for Python project parsing data",
        "sections": {
            "metadata": "Basic project information from setup files",
            "dependencies": "Project dependencies and requirements",
            "structure": "Project organization and file structure",
            "configuration": "Configuration files and settings",
            "examples": "Code examples and usage patterns",
            "tests": "Test-related information",
            "documentation": "Documentation files and structure"
        },
        "token_budget": {
            "target_max": 1000000,
            "typical_range": "50000-800000",
            "priority_order": [
                "metadata",
                "dependencies", 
                "structure.entry_points",
                "structure.modules (public APIs)",
                "examples",
                "configuration",
                "tests",
                "documentation"
            ]
        }
    }


# Export the main schema for external use
__all__ = [
    "PROJECT_DATA_SCHEMA",
    "METADATA_SCHEMA",
    "DEPENDENCIES_SCHEMA",
    "STRUCTURE_SCHEMA",
    "CONFIGURATION_SCHEMA",
    "CODE_EXAMPLE_SCHEMA",
    "validate_project_data",
    "get_schema_version",
    "get_schema_documentation"
]


