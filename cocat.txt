File: ./concat.py
import os

base_dir = r'./'
output_file = 'cocat.txt'


# Clear the output file before starting
with open(output_file, 'w', encoding='utf-8') as outfile:
    outfile.write('')

with open(output_file, 'a', encoding='utf-8') as outfile:
    for root, dirs, files in os.walk(base_dir):
        # Skip directories named "test" at any level
        dirs[:] = [d for d in dirs if d.lower() != 'test']
        for file in files:
            # Check for all required file extensions
            if file.endswith(('.py')):
                file_path = os.path.join(root, file)
                outfile.write(f'File: {file_path}\n')
                with open(file_path, 'r', encoding='utf-8') as infile:
                    outfile.write(infile.read())
                outfile.write('\n\n')

print(f'Concatenation complete. Output file: {output_file}')

File: ./__init__.py


File: ./setup.py
# setup.py

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

# Enhanced dependencies for LLM integration
install_requires = [
    # Core LLM integration dependencies
    "tiktoken>=0.5.0",              # Token counting for LLM context management
    "openai>=1.0.0",                # OpenAI API integration
    "google-generativeai>=0.3.0",   # Gemini API integration
    "anthropic>=0.7.0",             # Claude API integration
    
    # Template and output formatting
    "jinja2>=3.0.0",                # Template engine for README generation
    "click>=8.0.0",                 # Better CLI framework
    "rich>=13.0.0",                 # Enhanced CLI output and progress bars
    
    # Data handling and validation
    "pydantic>=2.0.0",              # Data validation and settings management
    "tomli>=2.0.0",                 # TOML parsing for older Python versions
    
    # Additional utilities
    "requests>=2.28.0",             # HTTP requests for API calls
    "python-dotenv>=0.19.0",        # Environment variable management
    "pathspec>=0.10.0",             # Git-style path matching
]

setup(
    name="readme-generator",
    version="1.0.0",
    author="README Generator Team",
    author_email="contact@readme-generator.com",
    description="AI-powered README generator for Python projects",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/readme-generator/readme-generator",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Topic :: Software Development :: Documentation",
        "Topic :: Text Processing :: Markup",
        "Topic :: Utilities",
    ],
    python_requires=">=3.8",
    install_requires=install_requires,
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-cov>=4.0.0",
            "black>=22.0.0",
            "flake8>=5.0.0",
            "mypy>=0.991",
        ],
        "docs": [
            "sphinx>=5.0.0",
            "sphinx-rtd-theme>=1.0.0",
        ],
    },
    entry_points={
        "console_scripts": [
            "readme-gen=main:main",
            "readme-generator=main:main",
        ],
    },
    keywords="readme generator ai llm documentation python",
    project_urls={
        "Bug Reports": "https://github.com/readme-generator/readme-generator/issues",
        "Source": "https://github.com/readme-generator/readme-generator",
        "Documentation": "https://readme-generator.readthedocs.io/",
    },
)


File: ./src/main.py
# src/main.py

import argparse
import sys
import time
from pathlib import Path
from typing import Dict, List
from datetime import datetime

from parsing import CoreParser
from models.project_info import ProjectInfo


def main():
    """Main entry point for the README generator CLI."""
    parser = argparse.ArgumentParser(
        description="Generate README files for Python projects using LLM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py /path/to/project
  python main.py . --verbose
  python main.py /path/to/project --output custom_readme.md
        """
    )
    
    parser.add_argument(
        "project_path",
        help="Path to the Python project root directory"
    )
    
    parser.add_argument(
        "-o", "--output",
        default="README.md",
        help="Output filename (default: README.md)"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Enable verbose output with detailed project structure"
    )
    
    parser.add_argument(
        "--stats",
        action="store_true",
        help="Show detailed parsing statistics"
    )
    
    parser.add_argument(
        "--data-file",
        default="data.txt",
        help="Output file for parsed data (default: data.txt)"
    )
    
    parser.add_argument(
        "--show-comments",
        action="store_true",
        help="Display function and class comments in output"
    )
    
    args = parser.parse_args()
    
    # Validate project path
    project_path = Path(args.project_path).resolve()
    if not project_path.exists():
        print(f"âŒ Error: Project path '{args.project_path}' does not exist.", file=sys.stderr)
        sys.exit(1)
    
    if not project_path.is_dir():
        print(f"âŒ Error: Project path '{args.project_path}' is not a directory.", file=sys.stderr)
        sys.exit(1)
    
    print(f"ðŸ” Analyzing Python project at: {project_path}")
    print("-" * 60)
    
    try:
        # Initialize parser and time the parsing operation
        start_time = time.time()
        core_parser = CoreParser()
        project_info = core_parser.parse_project(str(project_path))
        parsing_time = time.time() - start_time
        
        # Display comprehensive results
        display_parsing_results(project_info, parsing_time, args.verbose, args.stats, args.show_comments)
        
        # Test and validate the parsing results
        validation_results = validate_parsing_results(project_info)
        display_validation_results(validation_results)
        
        # Save all parsed data to data.txt
        save_parsed_data_to_file(project_info, parsing_time, validation_results, args.data_file)
        
        print(f"\nâœ… Parsing completed successfully in {parsing_time:.2f} seconds!")
        print(f"ðŸ“„ Ready to generate README for: {project_info.name or 'Unknown Project'}")
        print(f"ðŸ’¾ Parsed data saved to: {args.data_file}")
        
    except Exception as e:
        print(f"âŒ Error during parsing: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def save_parsed_data_to_file(project_info: ProjectInfo, parsing_time: float, 
                           validation_results: Dict[str, bool], output_file: str):
    """Save comprehensive parsed data to a text file."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    with open(output_file, 'a', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"PROJECT ANALYSIS REPORT - {timestamp}\n")
        f.write("=" * 80 + "\n\n")
        
        # Basic Project Information
        f.write("ðŸ“ BASIC PROJECT INFORMATION\n")
        f.write("-" * 40 + "\n")
        f.write(f"Project Name: {project_info.name or 'Unknown'}\n")
        f.write(f"Root Path: {project_info.root_path}\n")
        f.write(f"Setup File: {project_info.setup_file or 'None found'}\n")
        f.write(f"Requirements File: {project_info.requirements_file or 'None found'}\n")
        f.write(f"Parsing Time: {parsing_time:.2f} seconds\n")
        f.write(f"Parsed Python Files: {len(project_info.parsed_files)}\n\n")
        
        # Main Modules
        f.write(f"ðŸŽ¯ MAIN MODULES ({len(project_info.main_modules)} found)\n")
        f.write("-" * 40 + "\n")
        if project_info.main_modules:
            for i, module in enumerate(project_info.main_modules, 1):
                f.write(f"{i:2d}. {module}\n")
        else:
            f.write("No main modules identified\n")
        f.write("\n")
        
        # Entry Points
        f.write(f"ðŸš€ ENTRY POINTS ({len(project_info.entry_points)} found)\n")
        f.write("-" * 40 + "\n")
        if project_info.entry_points:
            for i, entry_point in enumerate(project_info.entry_points, 1):
                f.write(f"{i:2d}. {entry_point}\n")
        else:
            f.write("No entry points identified\n")
        f.write("\n")
        
        # Dependencies
        f.write(f"ðŸ“¦ DEPENDENCIES ({len(project_info.dependencies)} found)\n")
        f.write("-" * 40 + "\n")
        if project_info.dependencies:
            for i, dep in enumerate(project_info.dependencies, 1):
                f.write(f"{i:2d}. {dep}\n")
        else:
            f.write("No dependencies found\n")
        f.write("\n")
        
        # File Docstrings
        f.write(f"ðŸ“ FILE DOCSTRINGS ({len(project_info.file_docstrings)} found)\n")
        f.write("-" * 40 + "\n")
        if project_info.file_docstrings:
            for file_path, docstring in project_info.file_docstrings.items():
                f.write(f"File: {file_path}\n")
                f.write(f"Docstring: {docstring[:200]}{'...' if len(docstring) > 200 else ''}\n\n")
        else:
            f.write("No file docstrings found\n")
        f.write("\n")
        
        # Function Comments
        f.write(f"ðŸ”§ FUNCTION COMMENTS ({len(project_info.function_comments)} files with function comments)\n")
        f.write("-" * 40 + "\n")
        if project_info.function_comments:
            for file_path, function_comments in project_info.function_comments.items():
                f.write(f"File: {file_path}\n")
                for func_comment in function_comments:
                    f.write(f"  Function: {func_comment.get('function_name', 'Unknown')}\n")
                    f.write(f"  Line: {func_comment.get('line_number', 'Unknown')}\n")
                    
                    if func_comment.get('docstring'):
                        f.write(f"  Docstring: {func_comment['docstring'][:100]}{'...' if len(func_comment['docstring']) > 100 else ''}\n")
                    
                    if func_comment.get('preceding_comments'):
                        f.write(f"  Preceding Comments: {len(func_comment['preceding_comments'])} found\n")
                        for comment in func_comment['preceding_comments']:
                            f.write(f"    - {comment}\n")
                    
                    if func_comment.get('body_comments'):
                        f.write(f"  Body Comments: {len(func_comment['body_comments'])} found\n")
                        for comment in func_comment['body_comments']:
                            f.write(f"    - {comment}\n")
                    
                    f.write("\n")
                f.write("\n")
        else:
            f.write("No function comments found\n")
        f.write("\n")
        
        # Class Comments
        f.write(f"ðŸ—ï¸ CLASS COMMENTS ({len(project_info.class_comments)} files with class comments)\n")
        f.write("-" * 40 + "\n")
        if project_info.class_comments:
            for file_path, class_comments in project_info.class_comments.items():
                f.write(f"File: {file_path}\n")
                for class_comment in class_comments:
                    f.write(f"  Class: {class_comment.get('class_name', 'Unknown')}\n")
                    f.write(f"  Line: {class_comment.get('line_number', 'Unknown')}\n")
                    
                    if class_comment.get('docstring'):
                        f.write(f"  Docstring: {class_comment['docstring'][:100]}{'...' if len(class_comment['docstring']) > 100 else ''}\n")
                    
                    if class_comment.get('preceding_comments'):
                        f.write(f"  Preceding Comments: {len(class_comment['preceding_comments'])} found\n")
                        for comment in class_comment['preceding_comments']:
                            f.write(f"    - {comment}\n")
                    
                    if class_comment.get('body_comments'):
                        f.write(f"  Body Comments: {len(class_comment['body_comments'])} found\n")
                        for comment in class_comment['body_comments']:
                            f.write(f"    - {comment}\n")
                    
                    f.write("\n")
                f.write("\n")
        else:
            f.write("No class comments found\n")
        f.write("\n")
        
        # Project structure and other existing sections...
        # (Continue with existing code for file structure, statistics, validation, etc.)
        
        f.write("=" * 80 + "\n\n")


def display_parsing_results(project_info: ProjectInfo, parsing_time: float, 
                           verbose: bool, show_stats: bool, show_comments: bool):
    """Display comprehensive parsing results."""
    
    print("\n" + "=" * 60)
    print("ðŸ“Š PROJECT ANALYSIS RESULTS")
    print("=" * 60)
    
    # Basic project information
    print(f"ðŸ“ Project Name: {project_info.name or 'â“ Unknown'}")
    print(f"ðŸ“‚ Root Path: {project_info.root_path}")
    print(f"âš™ï¸  Setup File: {project_info.setup_file or 'âŒ None found'}")
    print(f"ðŸ“‹ Requirements File: {project_info.requirements_file or 'âŒ None found'}")
    print(f"â±ï¸  Parsing Time: {parsing_time:.2f} seconds")
    print(f"ðŸ Parsed Python Files: {len(project_info.parsed_files)}")
    
    # Main modules section
    print(f"\nðŸŽ¯ MAIN MODULES ({len(project_info.main_modules)} found):")
    if project_info.main_modules:
        for module in project_info.main_modules:
            print(f"  âœ“ {module}")
    else:
        print("  âŒ No main modules identified")
    
    # Entry points section
    print(f"\nðŸš€ ENTRY POINTS ({len(project_info.entry_points)} found):")
    if project_info.entry_points:
        for entry_point in project_info.entry_points:
            print(f"  âœ“ {entry_point}")
    else:
        print("  âŒ No entry points identified")
    
    # Dependencies section
    print(f"\nðŸ“¦ DEPENDENCIES ({len(project_info.dependencies)} found):")
    if project_info.dependencies:
        for i, dep in enumerate(project_info.dependencies, 1):
            print(f"  {i:2d}. {dep}")
    else:
        print("  âŒ No dependencies found")
    
    # Function Comments section
    if show_comments:
        print(f"\nðŸ”§ FUNCTION COMMENTS ({len(project_info.function_comments)} files):")
        if project_info.function_comments:
            for file_path, function_comments in project_info.function_comments.items():
                print(f"  ðŸ“„ {file_path}:")
                for func_comment in function_comments:
                    print(f"    ðŸ”¸ {func_comment.get('function_name', 'Unknown')} (Line {func_comment.get('line_number', '?')})")
                    
                    if func_comment.get('docstring'):
                        docstring_preview = func_comment['docstring'][:80] + "..." if len(func_comment['docstring']) > 80 else func_comment['docstring']
                        print(f"      ðŸ“ Docstring: {docstring_preview}")
                    
                    if func_comment.get('preceding_comments'):
                        print(f"      ðŸ’¬ {len(func_comment['preceding_comments'])} preceding comments")
                    
                    if func_comment.get('body_comments'):
                        print(f"      ðŸ“„ {len(func_comment['body_comments'])} body comments")
        else:
            print("  âŒ No function comments found")
        
        # Class Comments section
        print(f"\nðŸ—ï¸ CLASS COMMENTS ({len(project_info.class_comments)} files):")
        if project_info.class_comments:
            for file_path, class_comments in project_info.class_comments.items():
                print(f"  ðŸ“„ {file_path}:")
                for class_comment in class_comments:
                    print(f"    ðŸ”¸ {class_comment.get('class_name', 'Unknown')} (Line {class_comment.get('line_number', '?')})")
                    
                    if class_comment.get('docstring'):
                        docstring_preview = class_comment['docstring'][:80] + "..." if len(class_comment['docstring']) > 80 else class_comment['docstring']
                        print(f"      ðŸ“ Docstring: {docstring_preview}")
                    
                    if class_comment.get('preceding_comments'):
                        print(f"      ðŸ’¬ {len(class_comment['preceding_comments'])} preceding comments")
                    
                    if class_comment.get('body_comments'):
                        print(f"      ðŸ“„ {len(class_comment['body_comments'])} body comments")
        else:
            print("  âŒ No class comments found")
    
    # File Docstrings section
    print(f"\nðŸ“ FILE DOCSTRINGS ({len(project_info.file_docstrings)} found):")
    if project_info.file_docstrings:
        for file_path, docstring in project_info.file_docstrings.items():
            docstring_preview = docstring[:100] + "..." if len(docstring) > 100 else docstring
            print(f"  ðŸ“„ {file_path}: {docstring_preview}")
    else:
        print("  âŒ No file docstrings found")


def validate_parsing_results(project_info: ProjectInfo) -> Dict[str, bool]:
    """Validate the parsing results and return validation status."""
    
    validation_results = {
        "has_project_name": project_info.name is not None,
        "has_project_structure": bool(project_info.project_structure),
        "has_python_files": bool(project_info.parsed_files),
        "has_setup_configuration": project_info.setup_file is not None,
        "has_dependencies": bool(project_info.dependencies),
        "has_main_modules": bool(project_info.main_modules),
        "has_entry_points": bool(project_info.entry_points),
        "has_function_comments": bool(project_info.function_comments),
        "has_docstrings": bool(project_info.file_docstrings),
        "valid_root_path": Path(project_info.root_path).exists(),
    }
    
    return validation_results


def display_validation_results(validation_results: Dict[str, bool]):
    """Display validation results."""
    
    print(f"\nðŸ” PARSING VALIDATION:")
    print("-" * 40)
    
    validation_labels = {
        "has_project_name": "Project name detected",
        "has_project_structure": "Project structure mapped",
        "has_python_files": "Python files parsed",
        "has_setup_configuration": "Setup configuration found",
        "has_dependencies": "Dependencies identified",
        "has_main_modules": "Main modules identified",
        "has_entry_points": "Entry points detected",
        "has_function_comments": "Function comments extracted",
        "has_docstrings": "File docstrings extracted",
        "valid_root_path": "Valid root path",
    }
    
    passed = sum(validation_results.values())
    total = len(validation_results)
    
    for key, result in validation_results.items():
        status = "âœ…" if result else "âŒ"
        label = validation_labels.get(key, key)
        print(f"  {status} {label}")
    
    print(f"\nðŸ“Š Validation Score: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ðŸŽ‰ All validations passed! Project parsing is complete.")
    else:
        print("âš ï¸  Some validations failed. Check the project structure.")


if __name__ == "__main__":
    main()


File: ./src/utils/__init__.py


File: ./src/utils/file_utils.py
# src/utils/file_utils.py

import ast
import os
import re
import mimetypes
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any

try:
    import tiktoken
except ImportError:
    tiktoken = None


def safe_read_file(filepath: str) -> Optional[str]:
    """
    Safely reads a file's content with comprehensive error handling.
    
    This function attempts to read a file using multiple encoding strategies
    to handle various file encodings commonly found in Python projects.
    
    Args:
        filepath: The path to the file to read.
    
    Returns:
        The file content as a string if successful, None otherwise.
    """
    if not filepath or not os.path.exists(filepath):
        return None
    
    # Try different encodings in order of preference
    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(filepath, 'r', encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
        except (IOError, OSError) as e:
            # Handle file permission errors or other IO issues
            print(f"Error reading file {filepath}: {e}")
            return None
    
    # If all encodings fail, try reading as binary and decode with error handling
    try:
        with open(filepath, 'rb') as f:
            content = f.read()
            return content.decode('utf-8', errors='replace')
    except Exception as e:
        print(f"Failed to read file {filepath}: {e}")
        return None


def get_relative_path(base_path: str, target_path: str) -> str:
    """
    Gets the relative path from base_path to target_path.
    
    Args:
        base_path: The base directory path.
        target_path: The target file or directory path.
    
    Returns:
        The relative path from base to target, normalized with forward slashes.
    """
    try:
        base = Path(base_path).resolve()
        target = Path(target_path).resolve()
        
        # Calculate relative path
        relative = target.relative_to(base)
        
        # Convert to forward slashes for consistency
        return str(relative).replace(os.sep, '/')
    
    except ValueError:
        # Paths are not relative to each other, return absolute target path
        return str(Path(target_path).resolve())
    except Exception:
        # Fallback to string manipulation
        return os.path.relpath(target_path, base_path).replace(os.sep, '/')


def is_text_file(filepath: str) -> bool:
    """
    Determines if a file is likely to be a text file.
    
    This function uses multiple strategies to determine if a file is text:
    1. Check file extension against known text file types
    2. Use mimetypes library to guess content type
    3. Attempt to read the beginning of the file to check for binary content
    
    Args:
        filepath: The path to the file to check.
    
    Returns:
        True if the file is likely a text file, False otherwise.
    """
    if not filepath or not os.path.exists(filepath):
        return False
    
    # Convert to Path object for easier manipulation
    path = Path(filepath)
    
    # Check if it's a directory
    if path.is_dir():
        return False
    
    # Known text file extensions
    text_extensions = {
        '.py', '.txt', '.md', '.rst', '.cfg', '.ini', '.conf', '.json',
        '.yaml', '.yml', '.xml', '.html', '.css', '.js', '.toml',
        '.requirements', '.in', '.lock', '.log', '.sh', '.bat',
        '.sql', '.csv', '.tsv', '.gitignore', '.dockerignore'
    }
    
    # Check file extension
    if path.suffix.lower() in text_extensions:
        return True
    
    # Check files without extensions that are commonly text
    if not path.suffix and path.name.lower() in {
        'readme', 'license', 'changelog', 'authors', 'contributors',
        'dockerfile', 'makefile', 'pipfile', 'procfile'
    }:
        return True
    
    # Use mimetypes to guess content type
    mime_type, _ = mimetypes.guess_type(filepath)
    if mime_type and mime_type.startswith('text/'):
        return True
    
    # Try to read the beginning of the file to check for binary content
    try:
        with open(filepath, 'rb') as f:
            # Read first 1024 bytes
            chunk = f.read(1024)
            
            # Check for null bytes (common in binary files)
            if b'\x00' in chunk:
                return False
            
            # Try to decode as UTF-8
            try:
                chunk.decode('utf-8')
                return True
            except UnicodeDecodeError:
                # Try other common encodings
                for encoding in ['latin-1', 'cp1252']:
                    try:
                        chunk.decode(encoding)
                        return True
                    except UnicodeDecodeError:
                        continue
                
                return False
    
    except Exception:
        return False


def ensure_directory_exists(directory_path: str) -> bool:
    """
    Ensures that a directory exists, creating it if necessary.
    
    Args:
        directory_path: The path to the directory to create.
    
    Returns:
        True if the directory exists or was created successfully, False otherwise.
    """
    try:
        Path(directory_path).mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        print(f"Error creating directory {directory_path}: {e}")
        return False


def get_file_size(filepath: str) -> int:
    """
    Gets the size of a file in bytes.
    
    Args:
        filepath: The path to the file.
    
    Returns:
        The file size in bytes, or 0 if the file doesn't exist or can't be accessed.
    """
    try:
        return os.path.getsize(filepath)
    except Exception:
        return 0


def is_python_package(directory_path: str) -> bool:
    """
    Determines if a directory is a Python package (contains __init__.py).
    
    Args:
        directory_path: The path to the directory to check.
    
    Returns:
        True if the directory contains an __init__.py file, False otherwise.
    """
    if not os.path.isdir(directory_path):
        return False
    
    init_file = os.path.join(directory_path, '__init__.py')
    return os.path.exists(init_file)


def get_file_extension(filepath: str) -> str:
    """
    Gets the file extension from a filepath.
    
    Args:
        filepath: The path to the file.
    
    Returns:
        The file extension (including the dot), or empty string if no extension.
    """
    return Path(filepath).suffix.lower()


def normalize_path(path: str) -> str:
    """
    Normalizes a file path by resolving relative components and converting separators.
    
    Args:
        path: The path to normalize.
    
    Returns:
        The normalized path as a string.
    """
    return str(Path(path).resolve())


# New Enhanced Functions for LLM Integration

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """
    Count tokens for specific LLM model using tiktoken.
    
    Args:
        text: The text to count tokens for
        model: The LLM model to use for tokenization
        
    Returns:
        Number of tokens in the text
    """
    if not text:
        return 0
    
    if tiktoken is None:
        # Fallback estimation: roughly 1.3 tokens per word
        return int(len(text.split()) * 1.3)
    
    try:
        # Map common model names to tiktoken encodings
        model_mapping = {
            "gpt-4": "cl100k_base",
            "gpt-4o": "o200k_base",
            "gpt-4o-mini": "o200k_base",
            "gpt-3.5-turbo": "cl100k_base",
            "gemini-2.5-flash": "cl100k_base",  # Approximate
            "claude-sonnet": "cl100k_base",     # Approximate
        }
        
        # Try to get encoding for the specific model
        if model in model_mapping:
            encoding = tiktoken.get_encoding(model_mapping[model])
        else:
            # Try direct model name
            encoding = tiktoken.encoding_for_model(model)
        
        return len(encoding.encode(text))
    
    except Exception:
        # Fallback estimation
        return int(len(text.split()) * 1.3)


def optimize_content_for_llm(content: str, max_tokens: int, model: str = "gpt-4") -> str:
    """
    Optimize content to fit within token limits using intelligent truncation.
    
    Args:
        content: The content to optimize
        max_tokens: Maximum token limit
        model: LLM model for token counting
        
    Returns:
        Optimized content that fits within token limits
    """
    current_tokens = count_tokens(content, model)
    
    if current_tokens <= max_tokens:
        return content
    
    # Calculate reduction needed
    reduction_ratio = max_tokens / current_tokens
    
    # Apply various optimization strategies
    optimized_content = content
    
    # 1. Remove excessive whitespace
    optimized_content = re.sub(r'\n\s*\n\s*\n', '\n\n', optimized_content)
    optimized_content = re.sub(r' +', ' ', optimized_content)
    
    # 2. Remove comments (but preserve docstrings)
    optimized_content = _remove_comments(optimized_content)
    
    # 3. Compress long functions if still too long
    if count_tokens(optimized_content, model) > max_tokens:
        optimized_content = _compress_functions(optimized_content, reduction_ratio)
    
    # 4. Truncate by priority if still too long
    if count_tokens(optimized_content, model) > max_tokens:
        optimized_content = _truncate_by_priority(optimized_content, max_tokens, model)
    
    return optimized_content


def _remove_comments(content: str) -> str:
    """Remove comments while preserving docstrings."""
    lines = content.split('\n')
    result_lines = []
    in_docstring = False
    docstring_quote = None
    
    for line in lines:
        stripped = line.strip()
        
        # Check for docstring start/end
        if not in_docstring:
            if stripped.startswith('"""') or stripped.startswith("'''"):
                in_docstring = True
                docstring_quote = stripped[:3]
                result_lines.append(line)
                if stripped.count(docstring_quote) >= 2:
                    in_docstring = False
                continue
        else:
            result_lines.append(line)
            if docstring_quote in stripped:
                in_docstring = False
            continue
        
        # Remove single-line comments
        if stripped.startswith('#'):
            continue
        
        # Remove inline comments
        if '#' in line:
            # Simple heuristic: remove everything after # if not in string
            comment_pos = line.find('#')
            quote_count = line[:comment_pos].count('"') + line[:comment_pos].count("'")
            if quote_count % 2 == 0:  # Even number of quotes before #
                line = line[:comment_pos].rstrip()
        
        result_lines.append(line)
    
    return '\n'.join(result_lines)


def _compress_functions(content: str, reduction_ratio: float) -> str:
    """Compress long functions by summarizing their content."""
    try:
        tree = ast.parse(content)
        lines = content.split('\n')
        result_lines = lines.copy()
        
        # Find long functions to compress
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if hasattr(node, 'end_lineno') and node.end_lineno:
                    func_length = node.end_lineno - node.lineno
                    if func_length > 10:  # Compress functions longer than 10 lines
                        # Replace function body with summary
                        docstring = ast.get_docstring(node) or "Function implementation"
                        summary = f'    """{docstring}"""'
                        summary += f'\n    # Implementation ({func_length} lines) - truncated for brevity'
                        summary += f'\n    pass'
                        
                        # Replace lines
                        for i in range(node.lineno, min(node.end_lineno, len(result_lines))):
                            if i == node.lineno:
                                # Keep function signature
                                continue
                            elif i == node.lineno + 1:
                                result_lines[i] = summary
                            else:
                                result_lines[i] = ""
        
        return '\n'.join(line for line in result_lines if line is not None)
    
    except Exception:
        # If AST parsing fails, just return original content
        return content


def _truncate_by_priority(content: str, max_tokens: int, model: str) -> str:
    """Truncate content by priority, keeping most important parts."""
    lines = content.split('\n')
    
    # Categorize lines by priority
    high_priority = []
    medium_priority = []
    low_priority = []
    
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        # High priority: imports, class/function definitions, docstrings
        if (stripped.startswith(('import ', 'from ', 'class ', 'def ', 'async def ')) or
            stripped.startswith(('"""', "'''")) or
            stripped.startswith('#') and 'TODO' in stripped):
            high_priority.append((i, line))
        
        # Medium priority: non-empty lines with code
        elif stripped and not stripped.startswith('#'):
            medium_priority.append((i, line))
        
        # Low priority: comments, empty lines
        else:
            low_priority.append((i, line))
    
    # Build result by priority
    result_lines = [''] * len(lines)
    
    # Add high priority lines
    for i, line in high_priority:
        result_lines[i] = line
    
    # Add medium priority lines if we have token budget
    temp_content = '\n'.join(result_lines)
    for i, line in medium_priority:
        temp_content_with_line = temp_content + '\n' + line
        if count_tokens(temp_content_with_line, model) <= max_tokens:
            result_lines[i] = line
            temp_content = temp_content_with_line
        else:
            break
    
    # Add low priority lines if we still have budget
    temp_content = '\n'.join(result_lines)
    for i, line in low_priority:
        temp_content_with_line = temp_content + '\n' + line
        if count_tokens(temp_content_with_line, model) <= max_tokens:
            result_lines[i] = line
            temp_content = temp_content_with_line
        else:
            break
    
    return '\n'.join(line for line in result_lines if line)


def extract_code_examples(filepath: str) -> List[str]:
    """
    Extract code examples from Python files.
    
    Args:
        filepath: Path to the Python file
        
    Returns:
        List of code examples found in the file
    """
    examples = []
    
    try:
        content = safe_read_file(filepath)
        if not content:
            return examples
        
        # Parse the file as AST
        tree = ast.parse(content)
        
        # Extract if __name__ == "__main__" blocks
        main_examples = _extract_main_blocks(content, tree)
        examples.extend(main_examples)
        
        # Extract docstring examples
        docstring_examples = _extract_docstring_examples(tree)
        examples.extend(docstring_examples)
        
        # Extract test examples
        test_examples = _extract_test_examples(tree)
        examples.extend(test_examples)
        
    except Exception as e:
        print(f"Error extracting examples from {filepath}: {e}")
    
    return examples


def _extract_main_blocks(content: str, tree: ast.AST) -> List[str]:
    """Extract code from if __name__ == "__main__" blocks."""
    examples = []
    lines = content.split('\n')
    
    for node in ast.walk(tree):
        if isinstance(node, ast.If):
            # Check if this is a __name__ == "__main__" condition
            if _is_main_guard(node.test):
                # Extract the code block
                start_line = node.lineno - 1
                end_line = getattr(node, 'end_lineno', len(lines)) - 1
                
                block_lines = lines[start_line:end_line + 1]
                
                # Clean up the block (remove the if statement itself)
                example_lines = []
                for line in block_lines[1:]:  # Skip the if statement
                    if line.strip():
                        example_lines.append(line)
                
                if example_lines:
                    examples.append('\n'.join(example_lines))
    
    return examples


def _is_main_guard(test_node: ast.AST) -> bool:
    """Check if a test node is a __name__ == "__main__" condition."""
    if isinstance(test_node, ast.Compare):
        if (isinstance(test_node.left, ast.Name) and 
            test_node.left.id == '__name__' and
            len(test_node.ops) == 1 and
            isinstance(test_node.ops[0], ast.Eq) and
            len(test_node.comparators) == 1):
            
            comparator = test_node.comparators[0]
            if isinstance(comparator, ast.Constant):
                return comparator.value == '__main__'
            elif isinstance(comparator, ast.Str):  # Python < 3.8
                return comparator.s == '__main__'
    
    return False


def _extract_docstring_examples(tree: ast.AST) -> List[str]:
    """Extract code examples from docstrings."""
    examples = []
    
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            docstring = ast.get_docstring(node)
            if docstring:
                # Look for code blocks in docstring
                code_blocks = _parse_docstring_examples(docstring)
                examples.extend(code_blocks)
    
    return examples


def _parse_docstring_examples(docstring: str) -> List[str]:
    """Parse code examples from docstring text."""
    examples = []
    
    # Look for code blocks after "Example:" or "Examples:"
    example_pattern = r'(?:Examples?|Usage):\s*\n(.*?)(?:\n\s*\n|\n\s*[A-Z]|\Z)'
    matches = re.findall(example_pattern, docstring, re.DOTALL | re.IGNORECASE)
    
    for match in matches:
        # Clean up the example
        example = match.strip()
        if example:
            examples.append(example)
    
    # Look for code blocks marked with >>> (doctest style)
    doctest_pattern = r'>>> (.*?)(?:\n(?!>>>|\s*\.\.\.|\s*$)|\Z)'
    doctest_matches = re.findall(doctest_pattern, docstring, re.MULTILINE | re.DOTALL)
    
    for match in doctest_matches:
        if match.strip():
            examples.append(match.strip())
    
    return examples


def _extract_test_examples(tree: ast.AST) -> List[str]:
    """Extract examples from test functions."""
    examples = []
    
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if node.name.startswith('test_'):
                # This is a test function
                docstring = ast.get_docstring(node)
                if docstring:
                    examples.append(f"# Test: {node.name}\n{docstring}")
    
    return examples


def create_template_content(template_vars: Dict[str, Any]) -> str:
    """
    Create content from template variables for README generation.
    
    Args:
        template_vars: Dictionary of template variables
        
    Returns:
        Formatted template content
    """
    template = """
        # {project_name}

        {description}

        ## Installation

        {installation_instructions}

        ## Usage

        {usage_examples}

        ## Project Structure

        {project_structure}


        ## Dependencies

        {dependencies}

        ## Contributing

        {contributing_info}

        ## License

        {license_info}
        """
    
    return template.format(**template_vars)


def estimate_content_tokens(content_dict: Dict[str, str], model: str = "gpt-4") -> Dict[str, int]:
    """
    Estimate token counts for different content sections.
    
    Args:
        content_dict: Dictionary of content sections
        model: LLM model for token counting
        
    Returns:
        Dictionary with token counts for each section
    """
    token_counts = {}
    
    for section, content in content_dict.items():
        if content:
            token_counts[section] = count_tokens(str(content), model)
        else:
            token_counts[section] = 0
    
    return token_counts


def chunk_content_by_tokens(content: str, max_tokens: int, model: str = "gpt-4", 
                           overlap: int = 100) -> List[str]:
    """
    Split content into chunks based on token limits.
    
    Args:
        content: Content to chunk
        max_tokens: Maximum tokens per chunk
        model: LLM model for token counting
        overlap: Token overlap between chunks
        
    Returns:
        List of content chunks
    """
    if count_tokens(content, model) <= max_tokens:
        return [content]
    
    chunks = []
    lines = content.split('\n')
    current_chunk = []
    current_tokens = 0
    
    for line in lines:
        line_tokens = count_tokens(line, model)
        
        if current_tokens + line_tokens > max_tokens and current_chunk:
            # Save current chunk
            chunks.append('\n'.join(current_chunk))
            
            # Start new chunk with overlap
            if overlap > 0:
                overlap_lines = []
                overlap_tokens = 0
                for prev_line in reversed(current_chunk):
                    line_tokens_check = count_tokens(prev_line, model)
                    if overlap_tokens + line_tokens_check <= overlap:
                        overlap_lines.insert(0, prev_line)
                        overlap_tokens += line_tokens_check
                    else:
                        break
                current_chunk = overlap_lines
                current_tokens = overlap_tokens
            else:
                current_chunk = []
                current_tokens = 0
        
        current_chunk.append(line)
        current_tokens += line_tokens
    
    # Add final chunk
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks


def get_content_summary(content: str, max_length: int = 500) -> str:
    """
    Generate a brief summary of content.
    
    Args:
        content: Content to summarize
        max_length: Maximum summary length
        
    Returns:
        Content summary
    """
    if len(content) <= max_length:
        return content
    
    # Extract first paragraph or docstring
    lines = content.split('\n')
    summary_lines = []
    
    for line in lines:
        stripped = line.strip()
        if stripped and not stripped.startswith('#'):
            summary_lines.append(line)
            if len('\n'.join(summary_lines)) > max_length:
                break
    
    summary = '\n'.join(summary_lines)
    if len(summary) > max_length:
        summary = summary[:max_length] + "..."
    
    return summary


File: ./src/parsing/ast_parser.py
# src/parsing/ast_parser.py

import ast
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Tuple


class ASTParser:
    """
    Enhanced AST parser that extracts structural information and comments
    from Python source files.
    """
    
    def __init__(self):
        self.current_file = None
        self.source_lines = []
    
    def parse_python_file(self, filepath: str) -> Dict[str, Any]:
        """
        Parses a Python file and extracts comprehensive structural information
        including function comments, docstrings, and inline comments.
        
        Args:
            filepath: Path to the Python file to parse.
        
        Returns:
            A dictionary containing parsed information including docstrings,
            imports, functions, classes, comments, and other metadata.
        """
        self.current_file = filepath
        result = {
            'filepath': filepath,
            'module_docstring': None,
            'imports': [],
            'functions': [],
            'classes': [],
            'constants': [],
            'function_comments': [],
            'class_comments': [],
            'errors': []
        }
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Store source lines for comment extraction
            self.source_lines = content.split('\n')
            
            tree = ast.parse(content, filename=filepath)
            
            # Extract module-level docstring
            result['module_docstring'] = ast.get_docstring(tree)
            
            # Extract imports
            result['imports'] = self.get_imports(tree)
            
            # Extract functions and classes with enhanced comment parsing
            functions_classes = self.extract_functions_classes_with_comments(tree)
            result['functions'] = functions_classes['functions']
            result['classes'] = functions_classes['classes']
            result['function_comments'] = functions_classes['function_comments']
            result['class_comments'] = functions_classes['class_comments']
            
            # Extract constants (module-level assignments)
            result['constants'] = self._extract_constants(tree)
            
        except FileNotFoundError:
            result['errors'].append(f"File not found: {filepath}")
        except SyntaxError as e:
            result['errors'].append(f"Syntax error in {filepath}: {e}")
        except UnicodeDecodeError:
            result['errors'].append(f"Encoding error in {filepath}")
        except Exception as e:
            result['errors'].append(f"Unexpected error parsing {filepath}: {e}")
        
        return result
    
    def extract_functions_classes_with_comments(self, ast_node: ast.AST) -> Dict[str, List[Dict[str, Any]]]:
        """
        Enhanced extraction of functions and classes with comprehensive comment parsing.
        
        Args:
            ast_node: The AST node to extract from.
        
        Returns:
            A dictionary with functions, classes, and their associated comments.
        """
        functions = []
        classes = []
        function_comments = []
        class_comments = []
        
        for node in ast.walk(ast_node):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                function_info = self._extract_function_info_with_comments(node)
                functions.append(function_info['function'])
                if function_info['comments']:
                    function_comments.append(function_info['comments'])
            
            elif isinstance(node, ast.ClassDef):
                class_info = self._extract_class_info_with_comments(node)
                classes.append(class_info['class'])
                if class_info['comments']:
                    class_comments.append(class_info['comments'])
        
        return {
            'functions': functions,
            'classes': classes,
            'function_comments': function_comments,
            'class_comments': class_comments
        }
    
    def _extract_function_info_with_comments(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> Dict[str, Any]:
        """
        Extracts detailed information from a function definition including all comments.
        
        Args:
            node: The function definition AST node.
        
        Returns:
            A dictionary containing function information and comments.
        """
        function_info = {
            'name': node.name,
            'type': 'async_function' if isinstance(node, ast.AsyncFunctionDef) else 'function',
            'docstring': ast.get_docstring(node),
            'arguments': self._extract_function_args(node),
            'decorators': [self._get_decorator_name(decorator) for decorator in node.decorator_list],
            'returns': self._get_return_annotation(node),
            'line_number': node.lineno,
            'end_line_number': getattr(node, 'end_lineno', node.lineno),
            'is_private': node.name.startswith('_'),
            'is_dunder': node.name.startswith('__') and node.name.endswith('__')
        }
        
        # Extract comments associated with this function
        comments_info = self._extract_function_comments(node)
        
        return {
            'function': function_info,
            'comments': comments_info
        }
    
    def _extract_class_info_with_comments(self, node: ast.ClassDef) -> Dict[str, Any]:
        """
        Extracts detailed information from a class definition including all comments.
        
        Args:
            node: The class definition AST node.
        
        Returns:
            A dictionary containing class information and comments.
        """
        methods = []
        attributes = []
        
        for child in node.body:
            if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):
                method_info = self._extract_function_info_with_comments(child)
                method_data = method_info['function']
                method_data['is_method'] = True
                method_data['is_static'] = any(d.id == 'staticmethod' if isinstance(d, ast.Name) else False 
                                             for d in child.decorator_list)
                method_data['is_class_method'] = any(d.id == 'classmethod' if isinstance(d, ast.Name) else False 
                                                   for d in child.decorator_list)
                methods.append(method_data)
            
            elif isinstance(child, ast.Assign):
                # Extract class attributes
                for target in child.targets:
                    if isinstance(target, ast.Name):
                        attributes.append({
                            'name': target.id,
                            'line_number': child.lineno,
                            'is_private': target.id.startswith('_')
                        })
        
        class_info = {
            'name': node.name,
            'type': 'class',
            'docstring': ast.get_docstring(node),
            'bases': [self._get_base_class_name(base) for base in node.bases],
            'decorators': [self._get_decorator_name(decorator) for decorator in node.decorator_list],
            'methods': methods,
            'attributes': attributes,
            'line_number': node.lineno,
            'end_line_number': getattr(node, 'end_lineno', node.lineno),
            'is_private': node.name.startswith('_')
        }
        
        # Extract comments associated with this class
        comments_info = self._extract_class_comments(node)
        
        return {
            'class': class_info,
            'comments': comments_info
        }
    
    def _extract_function_comments(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> Dict[str, Any]:
        """
        Extracts all types of comments associated with a function.
        
        Args:
            node: The function definition AST node.
        
        Returns:
            A dictionary containing various types of comments.
        """
        comments = {
            'function_name': node.name,
            'file_path': self.current_file,
            'line_number': node.lineno,
            'docstring': ast.get_docstring(node),
            'preceding_comments': [],
            'inline_comments': [],
            'body_comments': []
        }
        
        # Extract comments immediately before the function definition
        comments['preceding_comments'] = self._get_preceding_comments(node.lineno)
        
        # Extract inline comments in function signature
        if hasattr(node, 'end_lineno') and node.end_lineno:
            comments['inline_comments'] = self._get_inline_comments(node.lineno, node.end_lineno)
        
        # Extract comments within function body
        comments['body_comments'] = self._get_function_body_comments(node)
        
        return comments
    
    def _extract_class_comments(self, node: ast.ClassDef) -> Dict[str, Any]:
        """
        Extracts all types of comments associated with a class.
        
        Args:
            node: The class definition AST node.
        
        Returns:
            A dictionary containing various types of comments.
        """
        comments = {
            'class_name': node.name,
            'file_path': self.current_file,
            'line_number': node.lineno,
            'docstring': ast.get_docstring(node),
            'preceding_comments': [],
            'inline_comments': [],
            'body_comments': []
        }
        
        # Extract comments immediately before the class definition
        comments['preceding_comments'] = self._get_preceding_comments(node.lineno)
        
        # Extract inline comments in class signature
        if hasattr(node, 'end_lineno') and node.end_lineno:
            comments['inline_comments'] = self._get_inline_comments(node.lineno, node.end_lineno)
        
        # Extract comments within class body (excluding method comments)
        comments['body_comments'] = self._get_class_body_comments(node)
        
        return comments
    
    def _get_preceding_comments(self, line_number: int, max_lines: int = 10) -> List[str]:
        """
        Gets comments immediately preceding a function or class definition.
        
        Args:
            line_number: The line number of the function/class definition.
            max_lines: Maximum number of lines to look back.
        
        Returns:
            A list of comment lines found before the definition.
        """
        comments = []
        start_line = max(0, line_number - max_lines - 1)
        end_line = line_number - 1
        
        # Look backwards from the function/class definition
        for i in range(end_line - 1, start_line - 1, -1):
            if i < len(self.source_lines):
                line = self.source_lines[i].strip()
                if line.startswith('#'):
                    comments.insert(0, line)
                elif line and not line.isspace():
                    # Stop at first non-comment, non-empty line
                    break
        
        return comments
    
    def _get_inline_comments(self, start_line: int, end_line: int) -> List[str]:
        """
        Gets inline comments within a range of lines.
        
        Args:
            start_line: Starting line number.
            end_line: Ending line number.
        
        Returns:
            A list of inline comments found in the range.
        """
        comments = []
        
        for line_num in range(start_line - 1, min(end_line, len(self.source_lines))):
            line = self.source_lines[line_num]
            
            # Look for inline comments (# after code)
            if '#' in line:
                # Simple heuristic: find # that's not in a string
                comment_pos = line.find('#')
                # Check if it's not inside quotes (basic check)
                before_comment = line[:comment_pos]
                quote_count = before_comment.count('"') + before_comment.count("'")
                
                if quote_count % 2 == 0:  # Even number of quotes before #
                    comment = line[comment_pos:].strip()
                    if comment:
                        comments.append(f"Line {line_num + 1}: {comment}")
        
        return comments
    
    def _get_function_body_comments(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> List[str]:
        """
        Gets comments within a function body.
        
        Args:
            node: The function definition AST node.
        
        Returns:
            A list of comments found in the function body.
        """
        comments = []
        
        if not hasattr(node, 'end_lineno') or not node.end_lineno:
            return comments
        
        start_line = node.lineno
        end_line = node.end_lineno
        
        for line_num in range(start_line, min(end_line + 1, len(self.source_lines))):
            line = self.source_lines[line_num].strip()
            
            # Look for comment-only lines within function body
            if line.startswith('#'):
                comments.append(f"Line {line_num + 1}: {line}")
        
        return comments
    
    def _get_class_body_comments(self, node: ast.ClassDef) -> List[str]:
        """
        Gets comments within a class body (excluding method comments).
        
        Args:
            node: The class definition AST node.
        
        Returns:
            A list of comments found in the class body.
        """
        comments = []
        
        if not hasattr(node, 'end_lineno') or not node.end_lineno:
            return comments
        
        start_line = node.lineno
        end_line = node.end_lineno
        
        # Get line numbers of methods to exclude their comments
        method_lines = set()
        for child in node.body:
            if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):
                method_start = child.lineno
                method_end = getattr(child, 'end_lineno', child.lineno)
                method_lines.update(range(method_start, method_end + 1))
        
        for line_num in range(start_line, min(end_line + 1, len(self.source_lines))):
            if line_num not in method_lines:  # Skip method lines
                line = self.source_lines[line_num].strip()
                
                # Look for comment-only lines within class body
                if line.startswith('#'):
                    comments.append(f"Line {line_num + 1}: {line}")
        
        return comments

    # Keep existing helper methods
    def get_imports(self, ast_node: ast.AST) -> List[Dict[str, Any]]:
        """Extract import statements from an AST node."""
        imports = []
        
        for node in ast.walk(ast_node):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append({
                        'type': 'import',
                        'module': alias.name,
                        'alias': alias.asname,
                        'level': 0
                    })
            
            elif isinstance(node, ast.ImportFrom):
                module_name = node.module or ''
                level = node.level
                
                for alias in node.names:
                    imports.append({
                        'type': 'from_import',
                        'module': module_name,
                        'name': alias.name,
                        'alias': alias.asname,
                        'level': level
                    })
        
        return imports

    def _extract_function_args(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> List[Dict[str, Any]]:
        """Extract function argument information."""
        args = []
        
        # Regular arguments
        for arg in node.args.args:
            args.append({
                'name': arg.arg,
                'type': 'regular',
                'annotation': self._get_annotation(arg.annotation),
                'default': None
            })
        
        # Handle defaults
        defaults = node.args.defaults
        if defaults:
            for i, default in enumerate(defaults):
                arg_index = len(node.args.args) - len(defaults) + i
                if arg_index < len(args):
                    args[arg_index]['default'] = self._get_default_value(default)
        
        # *args
        if node.args.vararg:
            args.append({
                'name': node.args.vararg.arg,
                'type': 'vararg',
                'annotation': self._get_annotation(node.args.vararg.annotation),
                'default': None
            })
        
        # **kwargs
        if node.args.kwarg:
            args.append({
                'name': node.args.kwarg.arg,
                'type': 'kwarg',
                'annotation': self._get_annotation(node.args.kwarg.annotation),
                'default': None
            })
        
        return args

    def _extract_constants(self, ast_node: ast.AST) -> List[Dict[str, Any]]:
        """Extract module-level constants."""
        constants = []
        
        for node in ast.walk(ast_node):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        if target.id.isupper():
                            constants.append({
                                'name': target.id,
                                'value': self._get_constant_value(node.value),
                                'line_number': node.lineno
                            })
        
        return constants

    def _get_decorator_name(self, decorator: ast.AST) -> str:
        """Get the name of a decorator."""
        if isinstance(decorator, ast.Name):
            return decorator.id
        elif isinstance(decorator, ast.Attribute):
            return f"{self._get_attribute_name(decorator.value)}.{decorator.attr}"
        return str(decorator)

    def _get_base_class_name(self, base: ast.AST) -> str:
        """Get the name of a base class."""
        if isinstance(base, ast.Name):
            return base.id
        elif isinstance(base, ast.Attribute):
            return f"{self._get_attribute_name(base.value)}.{base.attr}"
        return str(base)

    def _get_attribute_name(self, node: ast.AST) -> str:
        """Get the name of an attribute node."""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_attribute_name(node.value)}.{node.attr}"
        return str(node)

    def _get_annotation(self, annotation: Optional[ast.AST]) -> Optional[str]:
        """Get string representation of type annotation."""
        if annotation is None:
            return None
        
        if isinstance(annotation, ast.Name):
            return annotation.id
        elif isinstance(annotation, ast.Attribute):
            return f"{self._get_attribute_name(annotation.value)}.{annotation.attr}"
        elif isinstance(annotation, ast.Constant):
            return str(annotation.value)
        
        return str(annotation)

    def _get_return_annotation(self, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> Optional[str]:
        """Get return type annotation."""
        return self._get_annotation(node.returns)

    def _get_default_value(self, node: ast.AST) -> str:
        """Get string representation of default value."""
        if isinstance(node, ast.Constant):
            return repr(node.value)
        elif isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_attribute_name(node.value)}.{node.attr}"
        return str(node)

    def _get_constant_value(self, node: ast.AST) -> str:
        """Get string representation of constant value."""
        if isinstance(node, ast.Constant):
            return repr(node.value)
        elif isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, (ast.List, ast.Tuple, ast.Set)):
            return str(node)
        return str(node)


# Keep existing standalone functions for backward compatibility
def extract_docstrings(ast_node: ast.AST) -> List[str]:
    """Extract all docstrings from an AST node."""
    docstrings = []
    
    for node in ast.walk(ast_node):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)):
            docstring = ast.get_docstring(node)
            if docstring:
                docstrings.append(docstring)
    
    return docstrings


File: ./src/parsing/tempCodeRunnerFile.py
file_scanner

File: ./src/parsing/project_detector.py
# src/parsing/project_detector.py

import ast
import os
import re
from pathlib import Path
from typing import Dict, List, Optional


class ProjectDetector:
    """
    Detects key project files and determines project characteristics.
    
    This class is responsible for identifying configuration files, main modules,
    entry points, and determining whether the project is a package or script.
    """
    
    # Common setup/configuration file names
    SETUP_FILES = ["setup.py", "pyproject.toml", "setup.cfg"]
    
    # Common requirements file names
    REQUIREMENTS_FILES = [
        "requirements.txt", 
        "requirements-dev.txt", 
        "requirements-test.txt",
        "dev-requirements.txt",
        "test-requirements.txt",
        "requirements.in"
    ]
    
    # Common main module patterns
    MAIN_MODULE_PATTERNS = [
        "main.py",
        "__main__.py", 
        "app.py",
        "run.py",
        "cli.py",
        "manage.py",  # Django projects
        "server.py",  # Flask/FastAPI projects
        "wsgi.py",    # WSGI applications
        "asgi.py"     # ASGI applications
    ]


def find_setup_files(root_path: str) -> List[str]:
    """
    Finds setup and configuration files in the project root.
    
    Args:
        root_path: The root directory of the project to scan.
    
    Returns:
        A list of absolute paths to found setup files, ordered by priority.
    """
    setup_files = []
    root = Path(root_path)
    
    if not root.exists() or not root.is_dir():
        return setup_files
    
    # Check in order of priority
    for setup_file in ProjectDetector.SETUP_FILES:
        file_path = root / setup_file
        if file_path.exists() and file_path.is_file():
            setup_files.append(str(file_path))
    
    return setup_files


def find_requirements_files(root_path: str) -> List[str]:
    """
    Finds requirements files in the project root.
    
    Args:
        root_path: The root directory of the project to scan.
    
    Returns:
        A list of absolute paths to found requirements files.
    """
    requirements_files = []
    root = Path(root_path)
    
    if not root.exists() or not root.is_dir():
        return requirements_files
    
    for req_file in ProjectDetector.REQUIREMENTS_FILES:
        file_path = root / req_file
        if file_path.exists() and file_path.is_file():
            requirements_files.append(str(file_path))
    
    return requirements_files


def identify_main_modules(file_structure: Dict[str, List[str]]) -> List[str]:
    """
    Identifies main modules from the project file structure.
    Enhanced to handle both flat and src-layout projects.
    
    Args:
        file_structure: Dictionary mapping directory paths to lists of files.
    
    Returns:
        A list of identified main module file paths.
    """
    main_modules = []
    
    # Check root directory first
    root_files = file_structure.get("./", [])
    for file_name in root_files:
        if file_name in ProjectDetector.MAIN_MODULE_PATTERNS:
            main_modules.append(f"./{file_name}")
    
    # Check src/ directory (common Python project layout)
    src_files = file_structure.get("src/", [])
    for file_name in src_files:
        if file_name in ProjectDetector.MAIN_MODULE_PATTERNS:
            main_modules.append(f"src/{file_name}")
    
    # Look for __main__.py in subdirectories (package entry points)
    for dir_path, files in file_structure.items():
        if "__main__.py" in files:
            main_modules.append(f"{dir_path}__main__.py")
    
    # Look for files with "main" in the name in root and src directories
    for directory in ["./", "src/"]:
        dir_files = file_structure.get(directory, [])
        for file_name in dir_files:
            if (file_name.endswith(".py") and 
                "main" in file_name.lower() and 
                file_name not in ProjectDetector.MAIN_MODULE_PATTERNS):
                main_modules.append(f"{directory}{file_name}")
    
    return main_modules


def detect_entry_points(root_path: str) -> List[str]:
    """
    Detects potential entry points for the project.
    Enhanced to handle both flat and src-layout projects.
    
    This function combines main module detection with setup.py analysis
    to identify how the project can be executed.
    
    Args:
        root_path: The root directory of the project.
    
    Returns:
        A list of detected entry points.
    """
    entry_points = []
    root = Path(root_path)
    
    # Check for executable Python files in root
    for pattern in ProjectDetector.MAIN_MODULE_PATTERNS:
        main_file = root / pattern
        if main_file.exists():
            entry_points.append(f"python {pattern}")
    
    # Check src/ directory for main files
    src_dir = root / "src"
    if src_dir.exists():
        for pattern in ProjectDetector.MAIN_MODULE_PATTERNS:
            src_main_file = src_dir / pattern
            if src_main_file.exists():
                entry_points.append(f"python src/{pattern}")
    
    # Check if project can be run as a module (has __main__.py)
    main_py = root / "__main__.py"
    if main_py.exists():
        entry_points.append(f"python -m {root.name}")
    
    # Check for packages with __main__.py in src/
    if src_dir.exists():
        for item in src_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                package_main = item / "__main__.py"
                if package_main.exists():
                    entry_points.append(f"python -m src.{item.name}")
    
    # Check for setup.py entry points
    setup_py = root / "setup.py"
    if setup_py.exists():
        console_scripts = _extract_console_scripts_from_setup(str(setup_py))
        entry_points.extend(console_scripts)
    
    # Check for pyproject.toml entry points
    pyproject_toml = root / "pyproject.toml"
    if pyproject_toml.exists():
        toml_scripts = _extract_console_scripts_from_pyproject(str(pyproject_toml))
        entry_points.extend(toml_scripts)
    
    return entry_points


def determine_project_type(root_path: str, file_structure: Dict[str, List[str]]) -> str:
    """
    Determines if the project is a package or a script.
    
    Args:
        root_path: The root directory of the project.
        file_structure: Dictionary mapping directory paths to lists of files.
    
    Returns:
        Either "package" or "script" based on project structure.
    """
    root = Path(root_path)
    
    # Check for setup files (strong indicator of a package)
    setup_files = find_setup_files(root_path)
    if setup_files:
        return "package"
    
    # Check for __init__.py files (indicates package structure)
    for dir_path, files in file_structure.items():
        if "__init__.py" in files and dir_path != "./":
            return "package"
    
    # Check for src/ directory with packages
    src_files = file_structure.get("src/", [])
    if src_files:
        return "package"
    
    # Check for multiple Python modules in subdirectories
    python_dirs = 0
    for dir_path, files in file_structure.items():
        if dir_path != "./" and any(f.endswith(".py") for f in files):
            python_dirs += 1
    
    if python_dirs >= 2:
        return "package"
    
    # Default to script if no package indicators found
    return "script"


def infer_project_domain(file_structure: Dict[str, List[str]], 
                        dependencies: List[str]) -> str:
    """
    Infer project domain from structure and dependencies.
    
    Args:
        file_structure: Dictionary mapping directory paths to lists of files
        dependencies: List of project dependencies
        
    Returns:
        Inferred project domain category
    """
    # Data science indicators
    data_science_deps = ["pandas", "numpy", "scikit-learn", "matplotlib", "seaborn", 
                        "scipy", "jupyter", "plotly", "bokeh", "statsmodels"]
    if any(dep in dependencies for dep in data_science_deps):
        return "data_science"
    
    # Machine learning indicators (more specific than data science)
    ml_deps = ["tensorflow", "pytorch", "keras", "torch", "transformers", "sklearn", 
              "xgboost", "lightgbm", "catboost", "mlflow"]
    if any(dep in dependencies for dep in ml_deps):
        return "machine_learning"
    
    # Web development indicators
    web_deps = ["flask", "django", "fastapi", "streamlit", "bottle", "tornado", 
               "pyramid", "cherrypy", "starlette", "quart"]
    if any(dep in dependencies for dep in web_deps):
        return "web_development"
    
    # CLI tool indicators
    cli_deps = ["click", "argparse", "typer", "fire", "docopt", "cliff", "cement"]
    if any(dep in dependencies for dep in cli_deps):
        return "cli_tool"
    
    # DevOps/Infrastructure indicators
    devops_deps = ["ansible", "fabric", "paramiko", "boto3", "kubernetes", "docker", 
                  "terraform", "salt", "puppet"]
    if any(dep in dependencies for dep in devops_deps):
        return "devops"
    
    # Testing framework indicators
    testing_deps = ["pytest", "unittest", "nose", "behave", "robotframework", 
                   "hypothesis", "mock", "testfixtures"]
    if any(dep in dependencies for dep in testing_deps):
        return "testing"
    
    # Game development indicators
    game_deps = ["pygame", "panda3d", "arcade", "pyglet", "kivy", "pysimplegui"]
    if any(dep in dependencies for dep in game_deps):
        return "game_development"
    
    # Scientific computing indicators
    scientific_deps = ["sympy", "astropy", "biopython", "networkx", "igraph", 
                      "dask", "numba", "cython"]
    if any(dep in dependencies for dep in scientific_deps):
        return "scientific_computing"
    
    # Check file structure for domain indicators
    all_files = []
    for files in file_structure.values():
        all_files.extend([f.lower() for f in files])
    
    # Look for domain-specific file patterns
    if any("test" in f for f in all_files):
        return "testing"
    
    if any(f.endswith(".ipynb") for f in all_files):
        return "data_science"
    
    if any(f in ["dockerfile", "docker-compose.yml"] for f in all_files):
        return "devops"
    
    if any(f.startswith("manage") for f in all_files):
        return "web_development"
    
    # Check directory structure
    directories = list(file_structure.keys())
    if any("api" in d.lower() for d in directories):
        return "web_development"
    
    if any("models" in d.lower() for d in directories):
        return "machine_learning"
    
    return "general"


def detect_deployment_patterns(root_path: str) -> List[str]:
    """
    Detect deployment and containerization patterns.
    
    Args:
        root_path: Root directory path of the project
        
    Returns:
        List of detected deployment patterns
    """
    deployment_patterns = []
    root = Path(root_path)
    
    # Docker detection
    docker_files = ["Dockerfile", "docker-compose.yml", "docker-compose.yaml", ".dockerignore"]
    if any((root / docker_file).exists() for docker_file in docker_files):
        deployment_patterns.append("docker")
    
    # Kubernetes detection
    k8s_dirs = ["k8s", "kubernetes", "manifests", "charts"]
    k8s_files = ["*.yaml", "*.yml"]
    
    if any((root / k8s_dir).exists() for k8s_dir in k8s_dirs):
        deployment_patterns.append("kubernetes")
    
    # Check for YAML files that might be Kubernetes manifests
    yaml_files = list(root.glob("*.yaml")) + list(root.glob("*.yml"))
    for yaml_file in yaml_files:
        try:
            content = yaml_file.read_text(encoding='utf-8')
            if any(keyword in content.lower() for keyword in ["apiversion", "kind:", "metadata:"]):
                deployment_patterns.append("kubernetes")
                break
        except (IOError, UnicodeDecodeError):
            continue
    
    # Heroku detection
    heroku_files = ["Procfile", "runtime.txt", "app.json"]
    if any((root / heroku_file).exists() for heroku_file in heroku_files):
        deployment_patterns.append("heroku")
    
    # AWS detection
    aws_files = [".ebextensions", "serverless.yml", "template.yaml", "sam.yaml"]
    if any((root / aws_file).exists() for aws_file in aws_files):
        deployment_patterns.append("aws")
    
    # Google Cloud detection
    gcp_files = ["app.yaml", "cron.yaml", "dispatch.yaml", "cloudbuild.yaml"]
    if any((root / gcp_file).exists() for gcp_file in gcp_files):
        deployment_patterns.append("gcp")
    
    # Azure detection
    azure_files = ["azure-pipelines.yml", "azure-pipelines.yaml"]
    if any((root / azure_file).exists() for azure_file in azure_files):
        deployment_patterns.append("azure")
    
    # GitHub Actions detection
    github_actions_dir = root / ".github" / "workflows"
    if github_actions_dir.exists() and any(github_actions_dir.iterdir()):
        deployment_patterns.append("github_actions")
    
    # GitLab CI detection
    gitlab_files = [".gitlab-ci.yml", ".gitlab-ci.yaml"]
    if any((root / gitlab_file).exists() for gitlab_file in gitlab_files):
        deployment_patterns.append("gitlab_ci")
    
    # Travis CI detection
    travis_files = [".travis.yml", ".travis.yaml"]
    if any((root / travis_file).exists() for travis_file in travis_files):
        deployment_patterns.append("travis_ci")
    
    # Jenkins detection
    jenkins_files = ["Jenkinsfile", "jenkins.yml"]
    if any((root / jenkins_file).exists() for jenkins_file in jenkins_files):
        deployment_patterns.append("jenkins")
    
    # Terraform detection
    terraform_files = list(root.glob("*.tf")) + list(root.glob("*.tfvars"))
    if terraform_files:
        deployment_patterns.append("terraform")
    
    # Ansible detection
    ansible_files = ["ansible.cfg", "playbook.yml", "inventory"]
    ansible_dirs = ["roles", "playbooks"]
    if (any((root / ansible_file).exists() for ansible_file in ansible_files) or
        any((root / ansible_dir).exists() for ansible_dir in ansible_dirs)):
        deployment_patterns.append("ansible")
    
    return list(set(deployment_patterns))  # Remove duplicates


def detect_framework_patterns(file_structure: Dict[str, List[str]], 
                            dependencies: List[str]) -> List[str]:
    """
    Detect web frameworks and other major frameworks used in the project.
    
    Args:
        file_structure: Dictionary mapping directory paths to lists of files
        dependencies: List of project dependencies
        
    Returns:
        List of detected frameworks
    """
    frameworks = []
    
    # Framework detection based on dependencies
    framework_deps = {
        "django": ["django"],
        "flask": ["flask"],
        "fastapi": ["fastapi"],
        "streamlit": ["streamlit"],
        "tornado": ["tornado"],
        "bottle": ["bottle"],
        "pyramid": ["pyramid"],
        "cherrypy": ["cherrypy"],
        "starlette": ["starlette"],
        "quart": ["quart"],
        "aiohttp": ["aiohttp"],
        "sanic": ["sanic"],
        "falcon": ["falcon"],
        "celery": ["celery"],
        "sqlalchemy": ["sqlalchemy"],
        "pytest": ["pytest"],
        "unittest": ["unittest"],
        "scrapy": ["scrapy"],
    }
    
    for framework, deps in framework_deps.items():
        if any(dep in dependencies for dep in deps):
            frameworks.append(framework)
    
    # Framework detection based on file patterns
    all_files = []
    for files in file_structure.values():
        all_files.extend([f.lower() for f in files])
    
    # Django specific files
    django_files = ["manage.py", "settings.py", "wsgi.py", "asgi.py", "urls.py"]
    if any(f in all_files for f in django_files):
        frameworks.append("django")
    
    # Flask specific patterns
    flask_patterns = ["app.py", "wsgi.py", "flask_app.py"]
    if any(f in all_files for f in flask_patterns):
        frameworks.append("flask")
    
    # FastAPI specific patterns
    fastapi_patterns = ["main.py", "api.py"]
    if any(f in all_files for f in fastapi_patterns):
        frameworks.append("fastapi")
    
    # Streamlit specific patterns
    if any("streamlit" in f for f in all_files):
        frameworks.append("streamlit")
    
    return list(set(frameworks))  # Remove duplicates


def _extract_console_scripts_from_setup(setup_file: str) -> List[str]:
    """
    Extract console scripts from setup.py entry_points.
    
    Args:
        setup_file: Path to the setup.py file.
    
    Returns:
        A list of console script entry points.
    """
    scripts = []
    
    try:
        with open(setup_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Parse the setup.py file as AST
        tree = ast.parse(content)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                if (hasattr(node.func, 'id') and node.func.id == 'setup') or \
                   (hasattr(node.func, 'attr') and node.func.attr == 'setup'):
                    
                    # Look for entry_points keyword argument
                    for keyword in node.keywords:
                        if keyword.arg == 'entry_points':
                            scripts.extend(_parse_entry_points_from_ast(keyword.value))
    
    except (FileNotFoundError, SyntaxError, UnicodeDecodeError):
        # Fallback to regex parsing if AST fails
        try:
            with open(setup_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Look for console_scripts patterns
            console_scripts_pattern = r'console_scripts.*?=.*?\[(.*?)\]'
            matches = re.findall(console_scripts_pattern, content, re.DOTALL)
            
            for match in matches:
                # Extract script names
                script_entries = re.findall(r'["\']([^"\'=]+)=', match)
                for script_name in script_entries:
                    scripts.append(f"{script_name} (console script)")
        
        except Exception:
            pass
    
    return scripts


def _extract_console_scripts_from_pyproject(pyproject_file: str) -> List[str]:
    """
    Extract console scripts from pyproject.toml.
    
    Args:
        pyproject_file: Path to the pyproject.toml file.
    
    Returns:
        A list of console script entry points.
    """
    scripts = []
    
    try:
        # Try to import tomllib (Python 3.11+) or tomli
        try:
            import tomllib
        except ImportError:
            try:
                import tomli as tomllib
            except ImportError:
                return scripts
        
        with open(pyproject_file, 'rb') as f:
            data = tomllib.load(f)
        
        # Check for project.scripts
        project_scripts = data.get("project", {}).get("scripts", {})
        for script_name in project_scripts.keys():
            scripts.append(f"{script_name} (console script)")
        
        # Check for tool.poetry.scripts
        poetry_scripts = data.get("tool", {}).get("poetry", {}).get("scripts", {})
        for script_name in poetry_scripts.keys():
            scripts.append(f"{script_name} (poetry script)")
    
    except Exception:
        pass
    
    return scripts


def _parse_entry_points_from_ast(node: ast.AST) -> List[str]:
    """
    Parse entry points from an AST node.
    
    Args:
        node: AST node containing entry points data.
    
    Returns:
        A list of extracted console scripts.
    """
    scripts = []
    
    if isinstance(node, ast.Dict):
        for key, value in zip(node.keys, node.values):
            if isinstance(key, ast.Constant) and key.value == "console_scripts":
                if isinstance(value, ast.List):
                    for item in value.elts:
                        if isinstance(item, ast.Constant) and isinstance(item.value, str):
                            script_name = item.value.split('=')[0].strip()
                            scripts.append(f"{script_name} (console script)")
    
    return scripts


def get_project_metadata_summary(root_path: str) -> Dict[str, any]:
    """
    Get a comprehensive summary of project metadata.
    
    Args:
        root_path: The root directory of the project.
    
    Returns:
        A dictionary containing project metadata summary.
    """
    from .file_scanner import scan_directory
    
    file_structure = scan_directory(root_path)
    
    # Get dependencies (simplified)
    dependencies = []
    requirements_files = find_requirements_files(root_path)
    if requirements_files:
        from .metadata_extractor import parse_dependencies
        dependencies = parse_dependencies(requirements_files[0])
    
    return {
        "project_type": determine_project_type(root_path, file_structure),
        "setup_files": find_setup_files(root_path),
        "requirements_files": requirements_files,
        "main_modules": identify_main_modules(file_structure),
        "entry_points": detect_entry_points(root_path),
        "project_domain": infer_project_domain(file_structure, dependencies),
        "deployment_patterns": detect_deployment_patterns(root_path),
        "frameworks": detect_framework_patterns(file_structure, dependencies),
        "has_src_layout": "src/" in file_structure,
        "has_tests": any("test" in dir_path.lower() for dir_path in file_structure.keys()),
        "python_files_count": sum(
            len([f for f in files if f.endswith(".py")]) 
            for files in file_structure.values()
        )
    }


File: ./src/parsing/__init__.py
# src/parsing/__init__.py

import os
import sys
from pathlib import Path
from typing import Optional

# Add the src directory to Python path
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

# Use absolute imports instead of relative imports
from parsing.file_scanner import scan_directory
from parsing.project_detector import (
    ProjectDetector, find_setup_files, find_requirements_files,
    identify_main_modules, detect_entry_points, determine_project_type
)
from parsing.metadata_extractor import (
    extract_project_name, parse_dependencies, extract_setup_metadata,
    parse_pyproject_toml, extract_dependencies_from_pyproject
)
from parsing.ast_parser import ASTParser
from models.project_info import ProjectInfo

class CoreParser:
    """
    Main orchestrator for parsing Python projects and extracting metadata.
    
    This class coordinates all parsing operations to analyze a Python project
    and extract comprehensive information about its structure, dependencies,
    and characteristics including function comments.
    """
    
    def __init__(self):
        self.detector = ProjectDetector()
        self.ast_parser = ASTParser()
    
    def parse_project(self, root_path: str) -> ProjectInfo:
        """
        Orchestrates all parsing operations to analyze a Python project.
        
        Args:
            root_path: Path to the root directory of the Python project.
        
        Returns:
            ProjectInfo object containing all extracted information.
        """
        # Validate root path
        if not os.path.exists(root_path) or not os.path.isdir(root_path):
            raise ValueError(f"Invalid project path: {root_path}")
        
        # Initialize ProjectInfo with root path
        project_info = ProjectInfo(root_path=os.path.abspath(root_path))
        
        # Step 1: Scan directory structure
        print(f"Scanning directory structure for {root_path}...")
        project_info.project_structure = scan_directory(root_path)
        
        # Step 2: Detect key project files
        print("Detecting key project files...")
        setup_files = find_setup_files(root_path)
        requirements_files = find_requirements_files(root_path)
        
        project_info.setup_file = setup_files[0] if setup_files else None
        project_info.requirements_file = requirements_files[0] if requirements_files else None
        
        # Step 3: Extract project metadata
        print("Extracting project metadata...")
        project_info.name = self._extract_project_name(project_info)
        project_info.dependencies = self._extract_dependencies(project_info)
        
        # Step 4: Identify main modules and entry points
        print("Identifying main modules and entry points...")
        project_info.main_modules = identify_main_modules(project_info.project_structure)
        project_info.entry_points = detect_entry_points(root_path)
        
        # Step 5: Parse Python files for comments and additional context
        print("Parsing Python files and extracting comments...")
        self._parse_python_files_with_comments(project_info)
        
        print(f"Successfully parsed project: {project_info.name or 'Unknown'}")
        return project_info
    
    def _extract_project_name(self, project_info: ProjectInfo) -> Optional[str]:
        """Extract project name from available sources."""
        if project_info.setup_file:
            name = extract_project_name(project_info.setup_file)
            if name:
                return name
        
        # Fallback to directory name
        return Path(project_info.root_path).name
    
    def _extract_dependencies(self, project_info: ProjectInfo) -> list[str]:
        """Extract dependencies from all available sources."""
        dependencies = []
        
        # From requirements.txt
        if project_info.requirements_file:
            req_deps = parse_dependencies(project_info.requirements_file)
            dependencies.extend(req_deps)
        
        # From setup.py
        if project_info.setup_file and project_info.setup_file.endswith('setup.py'):
            setup_metadata = extract_setup_metadata(project_info.setup_file)
            setup_deps = setup_metadata.get('dependencies', [])
            dependencies.extend(setup_deps)
        
        # From pyproject.toml
        if project_info.setup_file and project_info.setup_file.endswith('pyproject.toml'):
            toml_data = parse_pyproject_toml(project_info.setup_file)
            toml_deps = extract_dependencies_from_pyproject(toml_data)
            dependencies.extend(toml_deps)
        
        # Remove duplicates while preserving order
        return list(dict.fromkeys(dependencies))
    
    def _parse_python_files_with_comments(self, project_info: ProjectInfo):
        """Parse Python files and extract function comments."""
        # Get all Python files from the project structure
        python_files = []
        for directory, files in project_info.project_structure.items():
            for file in files:
                if file.endswith('.py'):
                    full_path = os.path.join(project_info.root_path, directory.lstrip('./'), file)
                    full_path = os.path.normpath(full_path)
                    python_files.append(full_path)
        
        # Parse each Python file
        for file_path in python_files:
            if os.path.exists(file_path):
                try:
                    relative_path = os.path.relpath(file_path, project_info.root_path)
                    print(f"  Parsing {relative_path}...")
                    
                    parsed_data = self.ast_parser.parse_python_file(file_path)
                    
                    # Store file docstring
                    if parsed_data.get('module_docstring'):
                        project_info.file_docstrings[relative_path] = parsed_data['module_docstring']
                    
                    # Store function comments
                    if parsed_data.get('function_comments'):
                        project_info.function_comments[relative_path] = parsed_data['function_comments']
                    
                    # Store class comments
                    if parsed_data.get('class_comments'):
                        project_info.class_comments[relative_path] = parsed_data['class_comments']
                    
                    # Track successfully parsed files
                    project_info.parsed_files.append(relative_path)
                    
                except Exception as e:
                    print(f"  Warning: Could not parse {file_path}: {e}")


File: ./src/parsing/file_scanner.py
# src/parsing/file_scanner.py

import os
from pathlib import Path
from typing import Dict, List, Set, Tuple


# Enhanced ignore patterns for better project scanning
IGNORE_DIRS: Set[str] = {
    "__pycache__",
    ".git",
    ".idea",
    ".vscode",
    ".pytest_cache",
    ".mypy_cache",
    ".tox",
    "venv",
    ".venv",
    "env",
    ".env",
    "build",
    "dist",
    "node_modules",
    ".coverage",
    "htmlcov",
    ".egg-info",
    "wheels",
}

IGNORE_PATTERNS: Set[str] = {
    "*.egg-info",
    "*.pyc",
    "*.pyo",
    "*.pyd",
    "__pycache__",
}

IGNORE_FILES: Set[str] = {
    ".DS_Store",
    "Thumbs.db",
    ".gitkeep",
    ".gitignore",
    ".pytest_cache",
}

# Framework detection patterns
FRAMEWORK_INDICATORS = {
    "flask": {
        "files": ["app.py", "wsgi.py", "flask_app.py"],
        "imports": ["flask", "Flask"],
        "patterns": ["@app.route", "Flask(__name__)"]
    },
    "django": {
        "files": ["manage.py", "settings.py", "urls.py", "wsgi.py", "asgi.py"],
        "imports": ["django", "Django"],
        "patterns": ["DJANGO_SETTINGS_MODULE", "django.setup()"]
    },
    "fastapi": {
        "files": ["main.py", "app.py", "api.py"],
        "imports": ["fastapi", "FastAPI"],
        "patterns": ["FastAPI()", "@app.get", "@app.post"]
    },
    "streamlit": {
        "files": ["streamlit_app.py", "app.py"],
        "imports": ["streamlit"],
        "patterns": ["st.", "streamlit."]
    },
    "pytest": {
        "files": ["conftest.py", "pytest.ini"],
        "imports": ["pytest"],
        "patterns": ["def test_", "class Test"]
    },
    "sphinx": {
        "files": ["conf.py", "make.bat", "Makefile"],
        "directories": ["_build", "_static", "_templates"],
        "patterns": ["sphinx"]
    }
}


def should_ignore_path(path: Path) -> bool:
    """
    Enhanced path filtering with better pattern matching.
    
    Args:
        path: The pathlib.Path object to check
        
    Returns:
        True if the path should be ignored, False otherwise
    """
    if path.name in IGNORE_FILES:
        return True
    
    # Check for ignored directory names
    if any(part in IGNORE_DIRS for part in path.parts):
        return True
    
    # Check for ignored patterns
    if any(path.match(pattern) for pattern in IGNORE_PATTERNS):
        return True
    
    # Check for temporary files
    if path.name.startswith('.') and path.name not in {'.gitignore', '.env', '.flake8'}:
        return True
    
    # Check for backup files
    if path.name.endswith(('.bak', '.tmp', '.temp', '~')):
        return True
    
    return False


def is_python_file(filepath: str) -> bool:
    """
    Enhanced Python file detection.
    
    Args:
        filepath: Path to check
        
    Returns:
        True if it's a Python file, False otherwise
    """
    path = Path(filepath)
    
    # Standard Python files
    if path.suffix.lower() == '.py':
        return True
    
    # Python files without extension (scripts)
    if not path.suffix and path.is_file():
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                if first_line.startswith('#!') and 'python' in first_line.lower():
                    return True
        except Exception:
            pass
    
    return False


def calculate_file_importance(filepath: str, file_structure: Dict[str, List[str]]) -> float:
    """
    Calculate importance score for LLM context prioritization.
    
    Args:
        filepath: Path to the file
        file_structure: Complete project structure
        
    Returns:
        Importance score (0.0 to 10.0)
    """
    score = 0.0
    path = Path(filepath)
    filename = path.name.lower()
    
    # Main module patterns get highest priority
    main_patterns = ["main.py", "app.py", "__init__.py", "__main__.py", "run.py", "cli.py"]
    if any(pattern in filename for pattern in main_patterns):
        score += 10.0
    
    # Setup and configuration files
    setup_patterns = ["setup.py", "pyproject.toml", "requirements.txt", "pipfile", "poetry.lock"]
    if any(pattern in filename for pattern in setup_patterns):
        score += 9.0
    
    # Example files are valuable for usage demonstration
    example_keywords = ["example", "demo", "sample", "tutorial", "guide"]
    if any(keyword in filename for keyword in example_keywords):
        score += 8.0
    
    # API and web framework entry points
    web_patterns = ["wsgi.py", "asgi.py", "server.py", "api.py"]
    if any(pattern in filename for pattern in web_patterns):
        score += 7.5
    
    # Django specific files
    django_patterns = ["models.py", "views.py", "urls.py", "settings.py", "manage.py"]
    if any(pattern in filename for pattern in django_patterns):
        score += 7.0
    
    # Configuration files
    config_patterns = ["config.py", "settings.py", "conf.py", ".env", "constants.py"]
    if any(pattern in filename for pattern in config_patterns):
        score += 6.5
    
    # Test files (important for understanding usage)
    if "test" in filename or filename.startswith("test_"):
        score += 6.0
    
    # Documentation files
    doc_patterns = ["readme", "changelog", "license", "contributing", "authors"]
    if any(pattern in filename for pattern in doc_patterns):
        score += 5.5
    
    # Utility and helper files
    util_patterns = ["utils.py", "helpers.py", "common.py", "base.py", "core.py"]
    if any(pattern in filename for pattern in util_patterns):
        score += 5.0
    
    # Files in root directory get bonus
    if str(path.parent) in [".", "./"]:
        score += 2.0
    
    # Files in src/ directory get bonus
    if "src" in path.parts:
        score += 1.5
    
    # Python files get base score
    if is_python_file(filepath):
        score += 1.0
    
    # Penalty for deeply nested files
    if len(path.parts) > 4:
        score -= 1.0
    
    # Penalty for very long filenames (likely auto-generated)
    if len(filename) > 50:
        score -= 0.5
    
    return max(0.0, min(10.0, score))


def detect_framework_files(file_structure: Dict[str, List[str]]) -> List[str]:
    """
    Identify framework-specific files and patterns.
    
    Args:
        file_structure: Dictionary mapping directories to file lists
        
    Returns:
        List of detected frameworks
    """
    detected_frameworks = []
    all_files = []
    
    # Flatten file structure for analysis
    for directory, files in file_structure.items():
        for file in files:
            all_files.append(os.path.join(directory, file).replace("\\", "/"))
    
    all_files_str = " ".join(all_files).lower()
    
    for framework, indicators in FRAMEWORK_INDICATORS.items():
        framework_score = 0
        
        # Check for specific files
        for file_pattern in indicators.get("files", []):
            if any(file_pattern.lower() in file.lower() for file in all_files):
                framework_score += 3
        
        # Check for directories
        for dir_pattern in indicators.get("directories", []):
            if any(dir_pattern.lower() in directory.lower() for directory in file_structure.keys()):
                framework_score += 2
        
        # Basic pattern matching in filenames
        for pattern in indicators.get("patterns", []):
            if pattern.lower() in all_files_str:
                framework_score += 1
        
        # If score is high enough, consider framework detected
        if framework_score >= 2:
            detected_frameworks.append(framework)
    
    return detected_frameworks


def identify_example_files(file_structure: Dict[str, List[str]]) -> List[str]:
    """
    Identify example and demo files in the project.
    
    Args:
        file_structure: Dictionary mapping directories to file lists
        
    Returns:
        List of example file paths
    """
    example_files = []
    example_keywords = ["example", "demo", "sample", "tutorial", "guide", "test"]
    
    for directory, files in file_structure.items():
        for file in files:
            filename_lower = file.lower()
            
            # Check if filename contains example keywords
            if any(keyword in filename_lower for keyword in example_keywords):
                example_files.append(os.path.join(directory, file))
            
            # Check if directory name suggests examples
            directory_lower = directory.lower()
            if any(keyword in directory_lower for keyword in ["example", "demo", "sample"]):
                if is_python_file(file):
                    example_files.append(os.path.join(directory, file))
    
    return example_files


def categorize_files(file_structure: Dict[str, List[str]]) -> Dict[str, List[str]]:
    """
    Categorize files by their purpose and importance.
    
    Args:
        file_structure: Dictionary mapping directories to file lists
        
    Returns:
        Dictionary with categorized file lists
    """
    categories = {
        "core_modules": [],
        "utility_modules": [],
        "test_files": [],
        "example_files": [],
        "config_files": [],
        "documentation": [],
        "web_framework": [],
        "data_science": [],
    }
    
    for directory, files in file_structure.items():
        for file in files:
            filepath = os.path.join(directory, file)
            filename_lower = file.lower()
            
            # Core modules
            if any(pattern in filename_lower for pattern in 
                   ["main.py", "app.py", "__init__.py", "core.py", "base.py"]):
                categories["core_modules"].append(filepath)
            
            # Utility modules
            elif any(pattern in filename_lower for pattern in 
                     ["utils.py", "helpers.py", "common.py", "tools.py"]):
                categories["utility_modules"].append(filepath)
            
            # Test files
            elif ("test" in filename_lower or filename_lower.startswith("test_") or
                  "test" in directory.lower()):
                categories["test_files"].append(filepath)
            
            # Example files
            elif any(keyword in filename_lower for keyword in 
                     ["example", "demo", "sample", "tutorial"]):
                categories["example_files"].append(filepath)
            
            # Configuration files
            elif any(pattern in filename_lower for pattern in 
                     ["config", "settings", "conf", ".env", "constants"]):
                categories["config_files"].append(filepath)
            
            # Documentation
            elif any(pattern in filename_lower for pattern in 
                     ["readme", "doc", "changelog", "license", "contributing"]):
                categories["documentation"].append(filepath)
            
            # Web framework files
            elif any(pattern in filename_lower for pattern in 
                     ["views.py", "models.py", "urls.py", "wsgi.py", "asgi.py", "api.py"]):
                categories["web_framework"].append(filepath)
            
            # Data science files
            elif any(pattern in filename_lower for pattern in 
                     ["analysis", "model", "train", "predict", "data"]) and file.endswith(".py"):
                categories["data_science"].append(filepath)
    
    return categories


def scan_directory(root_path: str) -> Dict[str, List[str]]:
    """
    Enhanced directory scanning with better structure mapping.
    
    Args:
        root_path: Root directory path to scan
        
    Returns:
        Dictionary mapping directory paths to file lists
    """
    project_structure: Dict[str, List[str]] = {}
    
    try:
        root = Path(root_path).resolve()
    except (FileNotFoundError, OSError):
        print(f"Error: Cannot access directory '{root_path}'")
        return {}
    
    if not root.exists() or not root.is_dir():
        print(f"Error: '{root_path}' is not a valid directory")
        return {}
    
    for dirpath, dirnames, filenames in os.walk(root, topdown=True):
        current_dir_path = Path(dirpath)
        
        # Prune ignored directories early for efficiency
        dirnames[:] = [d for d in dirnames if not should_ignore_path(current_dir_path / d)]
        
        # Skip processing if current directory should be ignored
        if should_ignore_path(current_dir_path):
            continue
        
        # Filter and sort files
        filtered_files = []
        for filename in filenames:
            file_path = current_dir_path / filename
            if not should_ignore_path(file_path):
                filtered_files.append(filename)
        
        # Only include directories that have files
        if filtered_files:
            relative_dir_path = os.path.relpath(dirpath, root)
            
            # Normalize path representation
            if relative_dir_path == ".":
                key = "./"
            else:
                key = str(Path(relative_dir_path).as_posix()) + "/"
            
            # Sort files for consistent output
            project_structure[key] = sorted(filtered_files)
    
    return project_structure


def get_file_statistics(file_structure: Dict[str, List[str]]) -> Dict[str, any]:
    """
    Generate statistics about the project file structure.
    
    Args:
        file_structure: Dictionary mapping directories to file lists
        
    Returns:
        Dictionary with project statistics
    """
    stats = {
        "total_directories": len(file_structure),
        "total_files": sum(len(files) for files in file_structure.values()),
        "python_files": 0,
        "test_files": 0,
        "config_files": 0,
        "documentation_files": 0,
        "max_directory_depth": 0,
        "largest_directory": "",
        "largest_directory_file_count": 0,
    }
    
    for directory, files in file_structure.items():
        # Count Python files
        python_files_in_dir = sum(1 for f in files if is_python_file(f))
        stats["python_files"] += python_files_in_dir
        
        # Count test files
        test_files_in_dir = sum(1 for f in files if "test" in f.lower())
        stats["test_files"] += test_files_in_dir
        
        # Count config files
        config_files_in_dir = sum(1 for f in files if any(
            pattern in f.lower() for pattern in ["config", "settings", ".env", "requirements"]
        ))
        stats["config_files"] += config_files_in_dir
        
        # Count documentation files
        doc_files_in_dir = sum(1 for f in files if any(
            pattern in f.lower() for pattern in ["readme", "doc", "changelog", "license"]
        ))
        stats["documentation_files"] += doc_files_in_dir
        
        # Track directory depth
        depth = directory.count("/")
        stats["max_directory_depth"] = max(stats["max_directory_depth"], depth)
        
        # Track largest directory
        if len(files) > stats["largest_directory_file_count"]:
            stats["largest_directory"] = directory
            stats["largest_directory_file_count"] = len(files)
    
    return stats


def prioritize_files_for_llm(file_structure: Dict[str, List[str]], 
                            max_files: int = 50) -> List[Tuple[str, float]]:
    """
    Prioritize files for LLM processing based on importance scores.
    
    Args:
        file_structure: Dictionary mapping directories to file lists
        max_files: Maximum number of files to return
        
    Returns:
        List of (filepath, importance_score) tuples, sorted by importance
    """
    file_scores = []
    
    for directory, files in file_structure.items():
        for file in files:
            filepath = os.path.join(directory, file)
            importance_score = calculate_file_importance(filepath, file_structure)
            
            # Only include files with reasonable importance
            if importance_score > 0.5:
                file_scores.append((filepath, importance_score))
    
    # Sort by importance score (descending) and limit results
    file_scores.sort(key=lambda x: x[1], reverse=True)
    return file_scores[:max_files]


File: ./src/parsing/metadata_extractor.py
# src/parsing/metadata_extractor.py

import ast
import configparser
import re
import subprocess
from pathlib import Path
from typing import Any, Dict, List, Optional

try:
    import tomllib  # Python 3.11+
except ImportError:
    try:
        import tomli as tomllib  # Fallback for older Python versions
    except ImportError:
        tomllib = None


def extract_project_name(setup_file: str) -> Optional[str]:
    """
    Extracts the project name from setup.py or pyproject.toml files.
    
    Args:
        setup_file: Path to the setup file.
    
    Returns:
        The project name if found, None otherwise.
    """
    file_path = Path(setup_file)
    
    if not file_path.exists():
        return None
    
    if file_path.name == "pyproject.toml":
        toml_data = parse_pyproject_toml(setup_file)
        return toml_data.get("project", {}).get("name") or toml_data.get("tool", {}).get("poetry", {}).get("name")
    
    elif file_path.name == "setup.py":
        metadata = extract_setup_metadata(setup_file)
        return metadata.get("name")
    
    elif file_path.name == "setup.cfg":
        return _extract_name_from_setup_cfg(setup_file)
    
    return None


def parse_dependencies(requirements_file: str) -> List[str]:
    """
    Parses dependencies from a requirements.txt file.
    
    Args:
        requirements_file: Path to the requirements file.
    
    Returns:
        A list of dependency strings.
    """
    dependencies = []
    
    try:
        with open(requirements_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Skip git URLs and local file paths
                if line.startswith(('git+', 'http', 'https', '-e', '.')):
                    continue
                
                # Extract package name (remove version specifiers)
                package_name = re.split(r'[><=!~]', line)[0].strip()
                if package_name:
                    dependencies.append(package_name)
    
    except (FileNotFoundError, IOError):
        pass  # File doesn't exist or can't be read
    
    return dependencies


def extract_setup_metadata(setup_file: str) -> Dict[str, Any]:
    """
    Extracts metadata from a setup.py file using AST parsing.
    
    Args:
        setup_file: Path to the setup.py file.
    
    Returns:
        A dictionary containing extracted metadata.
    """
    metadata = {}
    
    try:
        with open(setup_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        tree = ast.parse(content)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                if (hasattr(node.func, 'id') and node.func.id == 'setup') or \
                   (hasattr(node.func, 'attr') and node.func.attr == 'setup'):
                    
                    # Extract keyword arguments from setup() call
                    for keyword in node.keywords:
                        if keyword.arg in ['name', 'version', 'description', 'author', 'author_email', 'license']:
                            if isinstance(keyword.value, ast.Constant):
                                metadata[keyword.arg] = keyword.value.value
                            elif isinstance(keyword.value, ast.Str):  # For older Python versions
                                metadata[keyword.arg] = keyword.value.s
                    
                    # Extract install_requires for dependencies
                    for keyword in node.keywords:
                        if keyword.arg == 'install_requires':
                            deps = _extract_list_from_ast(keyword.value)
                            if deps:
                                metadata['dependencies'] = deps
    
    except (FileNotFoundError, SyntaxError, IOError):
        pass  # File doesn't exist, has syntax errors, or can't be read
    
    return metadata


def parse_pyproject_toml(toml_file: str) -> Dict[str, Any]:
    """
    Parses a pyproject.toml file and extracts relevant metadata.
    
    Args:
        toml_file: Path to the pyproject.toml file.
    
    Returns:
        A dictionary containing the parsed TOML data.
    """
    if tomllib is None:
        return {}
    
    try:
        with open(toml_file, 'rb') as f:
            return tomllib.load(f)
    except (FileNotFoundError, tomllib.TOMLDecodeError, IOError):
        return {}


def extract_license_info(root_path: str) -> Optional[str]:
    """
    Extract license information from various sources.
    
    Args:
        root_path: Root directory path of the project
        
    Returns:
        License type/name if detected, None otherwise
    """
    license_files = ["LICENSE", "LICENSE.txt", "LICENSE.md", "COPYING", "COPYING.txt", "LICENSE.rst"]
    
    for license_file in license_files:
        file_path = Path(root_path) / license_file
        if file_path.exists():
            try:
                content = file_path.read_text(encoding='utf-8')[:500]  # First 500 chars
                license_type = detect_license_type(content)
                if license_type:
                    return license_type
            except (IOError, UnicodeDecodeError):
                continue
    
    # Check setup.py for license info
    setup_file = Path(root_path) / "setup.py"
    if setup_file.exists():
        metadata = extract_setup_metadata(str(setup_file))
        license_info = metadata.get('license')
        if license_info:
            return license_info
    
    # Check pyproject.toml for license info
    pyproject_file = Path(root_path) / "pyproject.toml"
    if pyproject_file.exists():
        toml_data = parse_pyproject_toml(str(pyproject_file))
        license_info = (toml_data.get("project", {}).get("license", {}).get("text") or
                       toml_data.get("tool", {}).get("poetry", {}).get("license"))
        if license_info:
            return license_info
    
    return None


def detect_license_type(content: str) -> Optional[str]:
    """
    Detect license type from license file content.
    
    Args:
        content: License file content
        
    Returns:
        License type if detected, None otherwise
    """
    content_lower = content.lower()
    
    # Common license patterns
    license_patterns = {
        "MIT License": ["mit license", "permission is hereby granted", "mit"],
        "Apache License 2.0": ["apache license", "version 2.0", "apache software foundation"],
        "GNU General Public License v3.0": ["gnu general public license", "version 3", "gpl-3"],
        "GNU General Public License v2.0": ["gnu general public license", "version 2", "gpl-2"],
        "BSD 3-Clause License": ["bsd 3-clause", "redistribution and use in source and binary"],
        "BSD 2-Clause License": ["bsd 2-clause", "redistribution and use in source and binary"],
        "GNU Lesser General Public License": ["gnu lesser general public license", "lgpl"],
        "Mozilla Public License 2.0": ["mozilla public license", "version 2.0", "mpl-2.0"],
        "ISC License": ["isc license", "permission to use, copy, modify"],
        "Unlicense": ["unlicense", "this is free and unencumbered software"]
    }
    
    for license_name, patterns in license_patterns.items():
        if any(pattern in content_lower for pattern in patterns):
            return license_name
    
    return None


def extract_version_info(root_path: str, setup_file: Optional[str] = None) -> Optional[str]:
    """
    Extract version information from multiple sources.
    
    Args:
        root_path: Root directory path of the project
        setup_file: Optional path to setup file
        
    Returns:
        Version string if found, None otherwise
    """
    # Check __init__.py for __version__
    version = _extract_version_from_init(root_path)
    if version:
        return version
    
    # Check setup.py/pyproject.toml
    if setup_file:
        version = _extract_version_from_setup(setup_file)
        if version:
            return version
    
    # Check VERSION file
    version = _extract_version_from_version_file(root_path)
    if version:
        return version
    
    # Check git tags
    version = _extract_version_from_git_tags(root_path)
    if version:
        return version
    
    return None


def _extract_version_from_init(root_path: str) -> Optional[str]:
    """Extract version from __init__.py files."""
    init_paths = [
        Path(root_path) / "__init__.py",
        Path(root_path) / "src" / "__init__.py",
    ]
    
    # Also check package directories
    for item in Path(root_path).iterdir():
        if item.is_dir() and not item.name.startswith('.'):
            init_paths.append(item / "__init__.py")
    
    # Check src/ subdirectories
    src_dir = Path(root_path) / "src"
    if src_dir.exists():
        for item in src_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                init_paths.append(item / "__init__.py")
    
    version_patterns = [
        r"__version__\s*=\s*['\"]([^'\"]+)['\"]",
        r"VERSION\s*=\s*['\"]([^'\"]+)['\"]",
        r"version\s*=\s*['\"]([^'\"]+)['\"]"
    ]
    
    for init_path in init_paths:
        if init_path.exists():
            try:
                content = init_path.read_text(encoding='utf-8')
                for pattern in version_patterns:
                    match = re.search(pattern, content, re.MULTILINE)
                    if match:
                        return match.group(1)
            except (IOError, UnicodeDecodeError):
                continue
    
    return None


def _extract_version_from_setup(setup_file: str) -> Optional[str]:
    """Extract version from setup.py or pyproject.toml."""
    file_path = Path(setup_file)
    
    if file_path.name == "setup.py":
        metadata = extract_setup_metadata(setup_file)
        return metadata.get("version")
    
    elif file_path.name == "pyproject.toml":
        toml_data = parse_pyproject_toml(setup_file)
        return (toml_data.get("project", {}).get("version") or
                toml_data.get("tool", {}).get("poetry", {}).get("version"))
    
    return None


def _extract_version_from_version_file(root_path: str) -> Optional[str]:
    """Extract version from VERSION file."""
    version_files = ["VERSION", "VERSION.txt", "version.txt"]
    
    for version_file in version_files:
        file_path = Path(root_path) / version_file
        if file_path.exists():
            try:
                content = file_path.read_text(encoding='utf-8').strip()
                # Simple version validation
                if re.match(r'^\d+\.\d+(\.\d+)?', content):
                    return content
            except (IOError, UnicodeDecodeError):
                continue
    
    return None


def _extract_version_from_git_tags(root_path: str) -> Optional[str]:
    """Extract version from git tags."""
    try:
        # Get the latest git tag
        result = subprocess.run(
            ['git', 'describe', '--tags', '--abbrev=0'],
            cwd=root_path,
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            tag = result.stdout.strip()
            # Clean up common tag prefixes
            if tag.startswith('v'):
                tag = tag[1:]
            if re.match(r'^\d+\.\d+(\.\d+)?', tag):
                return tag
    
    except (subprocess.TimeoutExpired, subprocess.SubprocessError, FileNotFoundError):
        pass
    
    return None


def parse_configuration_files(root_path: str) -> Dict[str, Any]:
    """
    Parse additional configuration files.
    
    Args:
        root_path: Root directory path of the project
        
    Returns:
        Dictionary containing parsed configuration data
    """
    config_info = {}
    
    # Parse INI-style configuration files
    config_files = [
        "tox.ini", "pytest.ini", ".flake8", "mypy.ini", "setup.cfg",
        ".coveragerc", ".pylintrc", "pyproject.toml"
    ]
    
    for config_file in config_files:
        file_path = Path(root_path) / config_file
        if file_path.exists():
            if config_file == "pyproject.toml":
                config_info[config_file] = parse_pyproject_toml(str(file_path))
            else:
                parsed_config = parse_ini_file(str(file_path))
                if parsed_config:
                    config_info[config_file] = parsed_config
    
    # Parse JSON configuration files
    json_config_files = [".eslintrc.json", "tsconfig.json", "package.json"]
    
    for json_file in json_config_files:
        file_path = Path(root_path) / json_file
        if file_path.exists():
            try:
                import json
                with open(file_path, 'r', encoding='utf-8') as f:
                    config_info[json_file] = json.load(f)
            except (json.JSONDecodeError, IOError):
                pass
    
    # Parse YAML configuration files
    yaml_config_files = [".github/workflows", ".travis.yml", ".circleci/config.yml", "docker-compose.yml"]
    
    for yaml_file in yaml_config_files:
        file_path = Path(root_path) / yaml_file
        if file_path.exists():
            config_info[yaml_file] = {"exists": True, "type": "yaml"}
    
    return config_info


def parse_ini_file(file_path: str) -> Dict[str, Any]:
    """
    Parse INI-style configuration file.
    
    Args:
        file_path: Path to the INI file
        
    Returns:
        Dictionary containing parsed INI data
    """
    try:
        config = configparser.ConfigParser()
        config.read(file_path, encoding='utf-8')
        
        result = {}
        for section_name in config.sections():
            result[section_name] = dict(config.items(section_name))
        
        return result
    
    except (configparser.Error, IOError, UnicodeDecodeError):
        return {}


def extract_dependencies_from_pyproject(toml_data: Dict[str, Any]) -> List[str]:
    """
    Extracts dependencies from parsed pyproject.toml data.
    
    Args:
        toml_data: Parsed TOML data from pyproject.toml.
    
    Returns:
        A list of dependency names.
    """
    dependencies = []
    
    # Standard Python packaging format
    project_deps = toml_data.get("project", {}).get("dependencies", [])
    dependencies.extend([dep.split()[0] for dep in project_deps])
    
    # Poetry format
    poetry_deps = toml_data.get("tool", {}).get("poetry", {}).get("dependencies", {})
    for dep_name, dep_spec in poetry_deps.items():
        if dep_name != "python":  # Skip Python version constraint
            dependencies.append(dep_name)
    
    return dependencies


def _extract_name_from_setup_cfg(setup_cfg_file: str) -> Optional[str]:
    """
    Extracts the project name from a setup.cfg file.
    
    Args:
        setup_cfg_file: Path to the setup.cfg file.
    
    Returns:
        The project name if found, None otherwise.
    """
    try:
        config = configparser.ConfigParser()
        config.read(setup_cfg_file)
        
        if config.has_section('metadata') and config.has_option('metadata', 'name'):
            return config.get('metadata', 'name')
    
    except (FileNotFoundError, configparser.Error):
        pass
    
    return None


def _extract_list_from_ast(node: ast.AST) -> List[str]:
    """
    Extracts a list of strings from an AST node.
    
    Args:
        node: The AST node to extract from.
    
    Returns:
        A list of strings extracted from the node.
    """
    if isinstance(node, ast.List):
        items = []
        for element in node.elts:
            if isinstance(element, ast.Constant) and isinstance(element.value, str):
                items.append(element.value)
            elif isinstance(element, ast.Str):  # For older Python versions
                items.append(element.s)
        return items
    
    return []


def extract_project_metadata_comprehensive(root_path: str) -> Dict[str, Any]:
    """
    Extract comprehensive project metadata from all available sources.
    
    Args:
        root_path: Root directory path of the project
        
    Returns:
        Dictionary containing all extracted metadata
    """
    metadata = {
        "name": None,
        "version": None,
        "description": None,
        "author": None,
        "license": None,
        "dependencies": [],
        "configuration": {},
        "has_tests": False,
        "has_docs": False,
    }
    
    # Extract from setup files
    setup_files = [
        Path(root_path) / "setup.py",
        Path(root_path) / "pyproject.toml",
        Path(root_path) / "setup.cfg"
    ]
    
    for setup_file in setup_files:
        if setup_file.exists():
            if setup_file.name == "setup.py":
                setup_metadata = extract_setup_metadata(str(setup_file))
                metadata.update({k: v for k, v in setup_metadata.items() if v})
            elif setup_file.name == "pyproject.toml":
                toml_data = parse_pyproject_toml(str(setup_file))
                project_data = toml_data.get("project", {})
                metadata["name"] = metadata["name"] or project_data.get("name")
                metadata["version"] = metadata["version"] or project_data.get("version")
                metadata["description"] = metadata["description"] or project_data.get("description")
                metadata["license"] = metadata["license"] or project_data.get("license", {}).get("text")
            break
    
    # Extract version from multiple sources
    metadata["version"] = metadata["version"] or extract_version_info(root_path)
    
    # Extract license information
    metadata["license"] = metadata["license"] or extract_license_info(root_path)
    
    # Extract dependencies
    requirements_file = Path(root_path) / "requirements.txt"
    if requirements_file.exists():
        metadata["dependencies"] = parse_dependencies(str(requirements_file))
    
    # Parse configuration files
    metadata["configuration"] = parse_configuration_files(root_path)
    
    # Check for tests and documentation
    metadata["has_tests"] = any(
        path.exists() for path in [
            Path(root_path) / "tests",
            Path(root_path) / "test",
            Path(root_path) / "pytest.ini",
            Path(root_path) / "tox.ini"
        ]
    )
    
    metadata["has_docs"] = any(
        path.exists() for path in [
            Path(root_path) / "docs",
            Path(root_path) / "doc",
            Path(root_path) / "README.md",
            Path(root_path) / "README.rst"
        ]
    )
    
    return metadata


File: ./src/models/__init__.py


File: ./src/models/project_info.py
# src/models/project_info.py

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
import re


@dataclass
class ProjectInfo:
    """
    Enhanced data class to store comprehensive information about a parsed Python project.
    
    This object holds all metadata, structural details, and LLM-optimized content
    extracted by the parsing engine for README generation using models like Gemini 2.5 Flash.
    """
    # Core project information
    root_path: str
    name: Optional[str] = None
    dependencies: List[str] = field(default_factory=list)
    entry_points: List[str] = field(default_factory=list)
    main_modules: List[str] = field(default_factory=list)
    project_structure: Dict[str, List[str]] = field(default_factory=dict)
    setup_file: Optional[str] = None
    requirements_file: Optional[str] = None
    
    # Enhanced metadata for README generation
    version: Optional[str] = None
    author: Optional[str] = None
    description: Optional[str] = None
    license_info: Optional[str] = None
    
    # LLM-specific content extraction
    docstrings: List[str] = field(default_factory=list)
    function_signatures: List[Dict[str, Any]] = field(default_factory=list)
    class_hierarchy: Dict[str, List[str]] = field(default_factory=dict)
    usage_examples: List[str] = field(default_factory=list)
    
    # Content categorization for prioritization
    core_modules: List[str] = field(default_factory=list)
    utility_modules: List[str] = field(default_factory=list)
    test_files: List[str] = field(default_factory=list)
    example_files: List[str] = field(default_factory=list)
    config_files: List[str] = field(default_factory=list)
    
    # Framework and project type detection
    detected_frameworks: List[str] = field(default_factory=list)
    project_domain: Optional[str] = None
    deployment_patterns: List[str] = field(default_factory=list)
    
    # Token management and optimization (critical for 1M context window)
    estimated_tokens: int = 0
    priority_score: float = 0.0
    file_importance_scores: Dict[str, float] = field(default_factory=dict)
    
    # Advanced project analysis
    import_dependencies: Dict[str, List[str]] = field(default_factory=dict)
    api_endpoints: List[Dict[str, Any]] = field(default_factory=list)
    design_patterns: List[str] = field(default_factory=list)
    configuration_options: Dict[str, Any] = field(default_factory=dict)
    
    # Function comment extraction (new enhancement)
    function_comments: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)
    file_docstrings: Dict[str, str] = field(default_factory=dict)
    class_comments: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)
    parsed_files: List[str] = field(default_factory=list)
    
    def calculate_token_estimate(self, model: str = "gemini-2.5-flash") -> int:
        """
        Calculate estimated token count for LLM context based on project content.
        Optimized for Gemini 2.5 Flash's 1M token context window.
        
        Args:
            model: The target LLM model for token estimation
            
        Returns:
            Estimated token count for the project content
        """
        try:
            import tiktoken
            # Map model names to appropriate tokenizers
            model_mapping = {
                "gemini-2.5-flash": "cl100k_base",  # Approximate tokenizer
                "gpt-4": "cl100k_base",
                "gpt-4o": "o200k_base",
                "claude-sonnet": "cl100k_base",
            }
            
            encoding_name = model_mapping.get(model, "cl100k_base")
            encoding = tiktoken.get_encoding(encoding_name)
        except (ImportError, KeyError):
            # Fallback estimation: roughly 1.3 tokens per word
            return self._fallback_token_estimate()
        
        total_tokens = 0
        
        # Count tokens from various content sources
        content_sources = [
            "\n".join(self.docstrings),
            "\n".join(self.usage_examples),
            str(self.function_signatures),
            str(self.class_hierarchy),
            str(self.project_structure),
            self.description or "",
            str(self.function_comments),
            str(self.file_docstrings),
        ]
        
        for content in content_sources:
            if content:
                total_tokens += len(encoding.encode(content))
        
        self.estimated_tokens = total_tokens
        return total_tokens
    
    def _fallback_token_estimate(self) -> int:
        """Fallback token estimation when tiktoken is not available."""
        total_chars = 0
        
        # Estimate based on character count
        text_sources = [
            "\n".join(self.docstrings),
            "\n".join(self.usage_examples),
            str(self.function_signatures),
            str(self.class_hierarchy),
            self.description or "",
            str(self.function_comments),
            str(self.file_docstrings),
        ]
        
        for text in text_sources:
            total_chars += len(text)
        
        # Rough estimation: 4 characters per token
        estimated_tokens = total_chars // 4
        self.estimated_tokens = estimated_tokens
        return estimated_tokens
    
    def calculate_priority_score(self) -> float:
        """
        Calculate overall priority score for content optimization.
        
        Returns:
            Priority score (0.0 to 10.0) indicating content importance
        """
        score = 0.0
        
        # Project completeness indicators
        if self.name:
            score += 1.0
        if self.description:
            score += 1.0
        if self.version:
            score += 0.5
        if self.author:
            score += 0.5
        
        # Documentation quality
        if self.docstrings:
            score += min(2.0, len(self.docstrings) * 0.2)
        
        # Function comments quality
        if self.function_comments:
            score += min(1.0, len(self.function_comments) * 0.1)
        
        # Usage examples availability
        if self.usage_examples:
            score += min(2.0, len(self.usage_examples) * 0.5)
        
        # Project structure complexity
        if self.main_modules:
            score += min(1.0, len(self.main_modules) * 0.3)
        
        # Framework detection bonus
        if self.detected_frameworks:
            score += 1.0
        
        # API endpoints (for web projects)
        if self.api_endpoints:
            score += 1.0
        
        self.priority_score = min(10.0, score)
        return self.priority_score
    
    def get_high_priority_content(self, max_tokens: int = 1000000) -> Dict[str, Any]:
        """
        Extract high-priority content optimized for LLM context window.
        Default max_tokens set to 1M for Gemini 2.5 Flash.
        
        Args:
            max_tokens: Maximum number of tokens to include
            
        Returns:
            Dictionary of optimized content for LLM processing
        """
        # Calculate current token estimate
        current_tokens = self.calculate_token_estimate()
        
        high_priority_content = {
            "project_metadata": {
                "name": self.name,
                "description": self.description,
                "version": self.version,
                "author": self.author,
                "license": self.license_info,
            },
            "project_structure": self._get_summarized_structure(),
            "dependencies": self.dependencies[:50],  # Increased for larger context
            "main_modules": self.main_modules,
            "entry_points": self.entry_points,
            "frameworks": self.detected_frameworks,
            "domain": self.project_domain,
            "deployment_patterns": self.deployment_patterns,
        }
        
        # Add content based on available token budget
        remaining_tokens = max_tokens - self._estimate_metadata_tokens()
        
        if remaining_tokens > 50000:
            high_priority_content["docstrings"] = self._get_top_docstrings(20000)
        
        if remaining_tokens > 30000:
            high_priority_content["function_comments"] = self._get_top_function_comments(15000)
        
        if remaining_tokens > 20000:
            high_priority_content["usage_examples"] = self.usage_examples[:10]
        
        if remaining_tokens > 15000:
            high_priority_content["function_signatures"] = self.function_signatures[:50]
        
        if remaining_tokens > 10000:
            high_priority_content["api_endpoints"] = self.api_endpoints[:20]
        
        if remaining_tokens > 5000:
            high_priority_content["design_patterns"] = self.design_patterns
        
        return high_priority_content
    
    def _get_summarized_structure(self) -> Dict[str, int]:
        """Get summarized project structure with file counts."""
        summarized = {}
        for directory, files in self.project_structure.items():
            python_files = [f for f in files if f.endswith('.py')]
            if python_files:
                summarized[directory] = len(python_files)
        return summarized
    
    def _get_top_docstrings(self, max_chars: int) -> List[str]:
        """Get top docstrings up to character limit."""
        selected_docstrings = []
        char_count = 0
        
        for docstring in self.docstrings:
            if char_count + len(docstring) <= max_chars:
                selected_docstrings.append(docstring)
                char_count += len(docstring)
            else:
                break
        
        return selected_docstrings
    
    def _get_top_function_comments(self, max_chars: int) -> Dict[str, List[Dict[str, Any]]]:
        """Get top function comments up to character limit."""
        selected_comments = {}
        char_count = 0
        
        for file_path, comments in self.function_comments.items():
            file_content = str(comments)
            if char_count + len(file_content) <= max_chars:
                selected_comments[file_path] = comments
                char_count += len(file_content)
            else:
                break
        
        return selected_comments
    
    def _estimate_metadata_tokens(self) -> int:
        """Estimate tokens used by basic metadata."""
        metadata_text = f"{self.name} {self.description} {self.version} {self.author}"
        return len(metadata_text.split()) * 1.3  # Rough estimation
    
    def add_docstring(self, docstring: str, source_file: str = ""):
        """Add a docstring with optional source file tracking."""
        if docstring and docstring.strip():
            # Avoid duplicates
            if docstring not in self.docstrings:
                self.docstrings.append(docstring)
    
    def add_usage_example(self, example: str, source_file: str = ""):
        """Add a usage example with optional source file tracking."""
        if example and example.strip():
            if example not in self.usage_examples:
                self.usage_examples.append(example)
    
    def add_function_signature(self, signature: Dict[str, Any]):
        """Add a function signature to the collection."""
        if signature not in self.function_signatures:
            self.function_signatures.append(signature)
    
    def set_file_importance_score(self, filepath: str, score: float):
        """Set importance score for a specific file."""
        self.file_importance_scores[filepath] = score
    
    def get_summary_stats(self) -> Dict[str, Any]:
        """Get summary statistics about the project."""
        total_python_files = sum(
            len([f for f in files if f.endswith('.py')])
            for files in self.project_structure.values()
        )
        
        return {
            "total_directories": len(self.project_structure),
            "total_python_files": total_python_files,
            "total_dependencies": len(self.dependencies),
            "total_docstrings": len(self.docstrings),
            "total_usage_examples": len(self.usage_examples),
            "total_function_signatures": len(self.function_signatures),
            "total_function_comments": len(self.function_comments),
            "total_parsed_files": len(self.parsed_files),
            "estimated_tokens": self.estimated_tokens,
            "priority_score": self.priority_score,
            "detected_frameworks": len(self.detected_frameworks),
            "api_endpoints": len(self.api_endpoints),
        }
    
    def is_within_context_window(self, max_tokens: int = 1000000) -> bool:
        """
        Check if project content fits within the specified context window.
        
        Args:
            max_tokens: Maximum token limit (default: 1M for Gemini 2.5 Flash)
            
        Returns:
            True if content fits within limits, False otherwise
        """
        return self.calculate_token_estimate() <= max_tokens
    
    def get_cost_estimate(self, model: str = "gemini-2.5-flash") -> float:
        """
        Estimate API cost based on token count and model pricing.
        
        Args:
            model: LLM model name
            
        Returns:
            Estimated cost in USD
        """
        tokens = self.calculate_token_estimate(model)
        
        # Pricing per 1M tokens (input) based on research
        pricing = {
            "gemini-2.5-flash": 0.30,  # $0.30 per 1M tokens
            "gpt-4o-mini": 0.15,       # $0.15 per 1M tokens
            "claude-sonnet": 3.00,     # $3.00 per 1M tokens
        }
        
        price_per_token = pricing.get(model, 0.30) / 1000000
        return tokens * price_per_token
    
    def optimize_for_cost(self, target_cost: float = 0.01) -> Dict[str, Any]:
        """
        Optimize content to stay within cost budget.
        
        Args:
            target_cost: Maximum cost in USD
            
        Returns:
            Optimized content dictionary
        """
        current_cost = self.get_cost_estimate()
        
        if current_cost <= target_cost:
            return self.get_high_priority_content()
        
        # Calculate reduction ratio needed
        reduction_ratio = target_cost / current_cost
        max_tokens = int(self.estimated_tokens * reduction_ratio)
        
        return self.get_high_priority_content(max_tokens)


